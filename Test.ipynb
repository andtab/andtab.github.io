{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "A3_Tabri_Andrew_1000414875.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNzjT5ks8VtpGun4eQLPnFO",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andtab/andtab.github.io/blob/master/Test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3m2Meqmszd6_"
      },
      "source": [
        "#**A3: TD(n) and Mountain Car**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "feYkKBUhylTU"
      },
      "source": [
        "In this assignment we are going to apply the TD(n) methods to solve the Mountain Car problem.\r\n",
        "<br>\r\n",
        "<br>\r\n",
        "By now, you should know how to load OpenAI gym environments:\r\n",
        "\r\n",
        "`env = gym.make(`<font color = 'mahogany'>`'MountainCar-v0'`</font>`)`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qH8I-t7zTpX"
      },
      "source": [
        "#**Imports**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N4o2JHiwEsyZ"
      },
      "source": [
        "!apt-get install -y xvfb python-opengl > /dev/null 2>&1"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AMTqRocaEqM0"
      },
      "source": [
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vo_Vo2SW7ciu"
      },
      "source": [
        "import gym\r\n",
        "from IPython.display import HTML\r\n",
        "from IPython import display as ipythondisplay\r\n",
        "from pyvirtualdisplay import Display\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "# Additional imports required for this implementation\r\n",
        "import glob\r\n",
        "import base64\r\n",
        "import copy \r\n",
        "import random\r\n",
        "from prettytable import PrettyTable"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEtz-gIcy4-2"
      },
      "source": [
        "#**Exercise 1: Prelims**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHRsenRIy9Aw"
      },
      "source": [
        "These are useful exercises to develop facility with the environment prior to the graded component of the assignment. Explore the above environment as we've done in past assignments and exercises.\r\n",
        "<br>\r\n",
        "Develop a MC controller and assess its performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NuphwEZe1ljT"
      },
      "source": [
        "**Text Response**: Inspecting the source code for the Mountain Car (MC) testing environment, we observe that an episode will terminate when the MC has hit or exceeded the flag position (0.5), with a velocity >= 0 (successful episode), or if an episode takes greater than or equal 200 steps to conclude (failed episode). MC states are represented by a tuple of `(position, velocity)`. The MC can move left, remain neutral, or move right at any given state. The built reward structure for the MC problem is -1 for every state transition."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XL0yaMu3Fr7d"
      },
      "source": [
        "**More notes on position and velocity for this problem...**\r\n",
        "<br>\r\n",
        "Position, $x$, ranges from $[1.2, 0.6]$ (inclusive)...\r\n",
        "<br>\r\n",
        "Velocity, $\\textbf{v}$ ranges from $[-0.7, -0.7]$ (inclusive)...\r\n",
        "\r\n",
        "when $x \\geq 0.5$ and $\\textbf{v} \\geq 0$, we have a successful episode."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "btHJUiinWLeY"
      },
      "source": [
        "##**Action Space**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ID035SM2WPCC"
      },
      "source": [
        "At any state, the mountain car can take one of 3 actions; push left (0), no push (1), push right (2), as presented in the below table."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "ilxhfZUjXYfz",
        "outputId": "d7f3c884-0b5b-4da7-8fb4-df7ba824fe71"
      },
      "source": [
        "# This cell is used to display the table illustrating the relation ship between\r\n",
        "# input and performed logic action\r\n",
        "\r\n",
        "# Instantiate table class\r\n",
        "action_space_table = PrettyTable()\r\n",
        "\r\n",
        "# Provide title to table\r\n",
        "action_space_table.title = 'Input-Action Mapping'\r\n",
        "\r\n",
        "# Provide field names to table\r\n",
        "action_space_table.field_names = ['Input', 'Logic Action']\r\n",
        "\r\n",
        "# Populate table rows\r\n",
        "action_space_table.add_row([0, 'Push Left'])\r\n",
        "action_space_table.add_row([1, 'No Push'])\r\n",
        "action_space_table.add_row([2, 'Push Right'])\r\n",
        "action_space_table.border\r\n",
        "print(action_space_table)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------------------------+\n",
            "|  Input-Action Mapping  |\n",
            "+--------+---------------+\n",
            "| Input  |  Logic Action |\n",
            "+--------+---------------+\n",
            "|   0    |   Push Left   |\n",
            "|   1    |    No Push    |\n",
            "|   2    |   Push Right  |\n",
            "+--------+---------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_NuQs0SIWE_L"
      },
      "source": [
        "##**Randomized Action Selection Model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LK3Nga0LXl_c"
      },
      "source": [
        "Developed code to demonstrate the MC taking random action selection, afterwards displaying the results in an embedded video. The code for implementing the video replay functionality takes guidance from the following notebook, linked <a href = \"https://colab.research.google.com/github/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class_12_01_ai_gym.ipynb#scrollTo=XDKGJ9A3O8fT\">here</a>: "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WRUpMyrEC6Ih"
      },
      "source": [
        "###Display Video Replay of MC Episode"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pio6LyWv6sCN"
      },
      "source": [
        "# Establish video size\r\n",
        "display = Display(visible=0, size=(1400, 900))\r\n",
        "display.start()\r\n",
        "\r\n",
        "# Function for video replay\r\n",
        "def display_video():\r\n",
        "  # Produce a list with all the video files created in the video folder of the\r\n",
        "  # notebook\r\n",
        "  mp4list = glob.glob('video/*.mp4')\r\n",
        "\r\n",
        "  # If video folder contains a video file...\r\n",
        "  if len(mp4list) > 0:\r\n",
        "\r\n",
        "    # Take the first video found in the folder \r\n",
        "    mp4 = mp4list[0]\r\n",
        "\r\n",
        "    # When video is opened with the 'read' method, it is encoded as a base64 \r\n",
        "    # string - we must convert this data into a video of mp4 format\r\n",
        "    video = open(mp4, 'rb').read()\r\n",
        "    encoded = base64.b64encode(video)\r\n",
        "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \r\n",
        "                loop controls style=\"height: 400px;\">\r\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\r\n",
        "             </video>'''.format(encoded.decode('ascii'))))\r\n",
        "  else: \r\n",
        "    print(\"Could not find video\")"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "huEOKl0TDz8n"
      },
      "source": [
        "###Random Action MC Episode Run (Output Video Recording)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C44tCZ6omhN3"
      },
      "source": [
        "As we can see... performance isn't all to great here. Let's see if TD Learning control methods can be implemented to improve this situation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 473
        },
        "id": "WvA3G-GDOK13",
        "outputId": "638cc337-162c-4a34-9e4c-88ab1b93adcd"
      },
      "source": [
        "# Apply the Monitor wrapper to the regular environment specification in order\r\n",
        "# for a replay to be recorded\r\n",
        "env = gym.wrappers.Monitor(gym.make('MountainCar-v0'),'video', force = True, resume = True)\r\n",
        "\r\n",
        "# Run an MC episode with random actions being taken\r\n",
        "env.reset()\r\n",
        "\r\n",
        "for i in range(50000):\r\n",
        "  action = env.action_space.sample()\r\n",
        "  print(\"step i\",i,\"action=\",action)\r\n",
        "  obs, reward, done, info = env.step(action)\r\n",
        "  print(\"obs=\",obs,\"reward=\",reward,\"done=\",done,\"info=\",info)\r\n",
        "  print('obs2=', env.env.state)\r\n",
        "  ipythondisplay.clear_output(wait=True)      \r\n",
        "  \r\n",
        "  if done:\r\n",
        "    break\r\n",
        "    \r\n",
        "\r\n",
        "env.close()\r\n",
        "print(\"Iterations that were run:\", i)\r\n",
        "print(\"step i\",i,\"action=\",action)\r\n",
        "print(\"obs=\",obs,\"reward=\",reward,\"done=\",done,\"info=\",info)\r\n",
        "\r\n",
        "display_video()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iterations that were run: 199\n",
            "step i 199 action= 1\n",
            "obs= [-0.32044734  0.00359201] reward= -1.0 done= True info= {'TimeLimit.truncated': True}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<video alt=\"test\" autoplay \n",
              "                loop controls style=\"height: 400px;\">\n",
              "                <source src=\"data:video/mp4;base64,AAAAIGZ0eXBpc29tAAACAGlzb21pc28yYXZjMW1wNDEAAAAIZnJlZQAAqsltZGF0AAACrgYF//+q3EXpvebZSLeWLNgg2SPu73gyNjQgLSBjb3JlIDE1MiByMjg1NCBlOWE1OTAzIC0gSC4yNjQvTVBFRy00IEFWQyBjb2RlYyAtIENvcHlsZWZ0IDIwMDMtMjAxNyAtIGh0dHA6Ly93d3cudmlkZW9sYW4ub3JnL3gyNjQuaHRtbCAtIG9wdGlvbnM6IGNhYmFjPTEgcmVmPTMgZGVibG9jaz0xOjA6MCBhbmFseXNlPTB4MzoweDExMyBtZT1oZXggc3VibWU9NyBwc3k9MSBwc3lfcmQ9MS4wMDowLjAwIG1peGVkX3JlZj0xIG1lX3JhbmdlPTE2IGNocm9tYV9tZT0xIHRyZWxsaXM9MSA4eDhkY3Q9MSBjcW09MCBkZWFkem9uZT0yMSwxMSBmYXN0X3Bza2lwPTEgY2hyb21hX3FwX29mZnNldD0tMiB0aHJlYWRzPTMgbG9va2FoZWFkX3RocmVhZHM9MSBzbGljZWRfdGhyZWFkcz0wIG5yPTAgZGVjaW1hdGU9MSBpbnRlcmxhY2VkPTAgYmx1cmF5X2NvbXBhdD0wIGNvbnN0cmFpbmVkX2ludHJhPTAgYmZyYW1lcz0zIGJfcHlyYW1pZD0yIGJfYWRhcHQ9MSBiX2JpYXM9MCBkaXJlY3Q9MSB3ZWlnaHRiPTEgb3Blbl9nb3A9MCB3ZWlnaHRwPTIga2V5aW50PTI1MCBrZXlpbnRfbWluPTI1IHNjZW5lY3V0PTQwIGludHJhX3JlZnJlc2g9MCByY19sb29rYWhlYWQ9NDAgcmM9Y3JmIG1idHJlZT0xIGNyZj0yMy4wIHFjb21wPTAuNjAgcXBtaW49MCBxcG1heD02OSBxcHN0ZXA9NCBpcF9yYXRpbz0xLjQwIGFxPTE6MS4wMACAAAAQ6WWIhAA3//728P4FNjuY0JcRzeidMx+/Fbi6NDe9zgAAAwAAAwAACNCLwW1jsC2M+AAAJZAt9C+gGAAN3dq5XHPUJqMQ9STIE4h5i1p0ovAz2D6+0y6GAp6iGk03o7xeeJnNFlP7pyt72mmO8alQPFkcQS/hupH7h4mJTeRhtQk621kLNt8RFPKKRW6wQBeqa/bIUz2QUD/5/AKR9JghRtwUcbJYDG+GJMPfc0Z9+Ky2ZULlDK+ObhIm0waUlSDFaiGatI7Cv9YYX3GwANdOvfJULx5gqPt4GHqavREAVJQU2iOKP5G+WttIUpCgmsc99A8Dcsc4NyqwXB/Wn1HghqJQQTXBhrqdl+eCJeXRZ8bhBdBnr0sWP5YVPan+zifC1CRYcA7fnq256iVjYkyvp6grqbaJSiuDI5DbujXnFly7nle/W7NR/1S2Vof4k+CO61XnSIdtvxgWBwUWA46h0k1oBtMIQvNUXU33AM8G80Ey7IIx18o8b2mKF3zbUTMnFoBoXl6eDPvaN5fHuWZowtenf+WYDl1jSre9SqRT06h/6ZRDv7xTAHJNq6qxYSI0NftlMttSAzKr+nSofw/95u7GewZDF0yP7yLs82zMDY+0kJ1m7wus3HJL/RKeMZXAAAADABOd+97ZsfU/AetTFlL3eGUPI3gGHjOnAH31e4cNjmvsmesMTTM4eiERjIRWvHAU2TClT9QN4TxKLEodabOQADnwf0FnYOL2zQOiHdi3H/i0RT8Z6Q0OufexkjrfnBX8Ml+QdWQhPTOOVmynP2U537Z3NDuWSSw0CZz/THyMceaGg9XQ76ezrOt+G1EjwsVV9mb+q47cAwK/MLgHGq7Gck3N7ogAwAQ5YaQCO+lmuiUbPjz/Yd02QKOIDdL3mSY6VuxGFsKBYO8f0SRO+qjpT+Cm0RfoXnFLlFhjelV7VI7Aj+kvvWhqqdvsZh0oAjDuA/KteGfgQh2a4TwEB+IGv2Gkdqv3xJ9lIxX/3yowxySFuZ0+f1VUxedtXp5HyDwABORx1ZrGd+OIhIS8oWUL2EdmLP7P8IXICbCYmyEDV1ApwDnEiEFKoBVI7Zjll3KcVGMmcEDgBRlXZlJsPyA8JyP6fGtElmhHQ+E10HJ7Thld6rVLs/sbMDcspT4yhVk5V/cdobTn/kQ4ShBbsn1mFiUmUBjTYEwVOB8nRWYZ+fZRorSg+VbVzJwEziuojDaBme+4BQmK6S3hmx04jcjPXejw839OdX/0emPwxp6BBehtubLMbrrq+Kem0x/eRuI7IEeO9nvalmIHwarzjaNA80eco7v8zrqhCDQXtvbIBJZdISniHDToD9Vp42mUm6kHhfrXqeY8e5YvE+3q1aw1I3K58e2Q79VN900UgWF18QdfpQIPLBATdwg9uAABXVn0g9eAwYBDKBh09jReeRz/kgP9MBDPYe8dOVtgfULjZd9klC4VecBUoXp/+/Nxr25nUt3LNWyjZXfLHXhCqmA9rATdu88/aPY7n4+AGAT+ND337i/gEtQ4dRrGlDk61v16gcPogjUgVwKx1S3Zx+KKpV7WAYRfFTmnMBoGh+/rqA36lUBu8wIXFD8KGhlAtdrRoTlTqwsVDMspO1rMsY16Mzy9vKOik5cvigHNWkJYSI2kzf5xeYjU4G1qNAAAKeEh0P7f6H0DmPODvHjYSys6p7vhKP5r9wdkl+uV3Z6xqWa6/n2i3CaCbnaxxBF7rWQRZG7cyiWHlKfnkVk/FPmeB5yz60PqeexBVHUedbqSlUIFFn46vItwWNB5mdT+0Hk5XiCR5N6YJ60eWMgNuZunq3KmlwOgSF9IZFvTE5YmP/E+DqWFW25g+H8JrIC+4qVZU8h8qoFOEntb8Gan5nCwzXh2Cf1XJXipmQgC9UwMeE6iorSLm3mlsdcv1bA+L1Gxc51Egh7PKoDfoCfAMSh51NH/4r+nsBfaKnmjiSoxlIz6n6NnLJ6GZdgledksv2OEQ99NWzpFDN5fzOMEOoKSkP8iP1XiTAEosYw6eQtMq51ui6y+V8oEUBVHNLLXqOHl1oi/oYektmOyxg1Hd5eBPwgMQPWCYVYBQ2me0z2N0q1CmVfXrRQVszshaKzTxwLV0jPYm1Zlc8FsKyCo3pWKDEzPfx1B2sPBA4PZOmgMkq9HGsKIc5Sh8FXCUq8Iyh7v8ectcPz1hINsCBKl8QDSpbpcn5tIQBudtD7yaSsVO8xw5o3W2w8pAMcq/1an/tPs7NuJBW4O7gS7Ell7CnYIIZ9MO0OP1chcSVMbY/7+wVQ3c1Ph0CS5DvhM9iEbuhggBGVX1/BkWed0g1XrynA1m7mAWqZ34Clk+OlFhF4Xr5DM+q+Df2FUpysy1PHwPWh3Bd7Cki3d/K+SIDBJFHYxHM1tdUQ9P8ACdSHBQ42cYV6Ner4oK0o+2bcghkAIyh3zSC/X7GbWlLV95QLuvuZBxFSdTaYvSj0r2uByPQjOIrZHfhskOhfTZLNV5v/I+31vwyL4YwQr4BKE9JWHmxPEU2NFd2AQJNUpReb92cAt6hVXu12udLBXx+UfDnO6NARPTnYfwOZogS2uprKQGAqrqBMz0s6xAOG63e21z3p5AVddb+mqmUmelJz7GrVSz4MX+3DAav+U4IzJ/vfFD4V8bMz2iSD9RfPh2hBJaEn2DVku7rloC52Bf6czw4OegTryGK6hHumSJSD0VEmRRDqwj1W2uEYxgZ5nWiiZAnV/dNw4FBpjDYnGf0eoeCdV8Pf3H62OcAeLd75SYQFu1GDX+cMtaUuLwZMDCOr27Rb95Ax14Ok4yS9rVDbjdn+xXIL3aPCmFwpGiWj9X0IAjxkwPU5Q3DpSKwtnn1hpQAHb7VRd6vLWiquGKNsKhnYAmqdfD848xffseLvyWSPeoFhgZJm7pWnP7RsnlEVszor19i4JaG8SpB6oHAma2sZ+5mZ3BentE3Y5BFC/70Fg4nzcSfH+WEgaTz5RnCIwdZS7qXK484xv2U1QNyiL4lRE8MTm9Z9f+DsLRbSH44Xu7NU7itPXcZR73YXvsqQ/M7WMRp21Afj81jrXodsb9Q04y5Z8/rfzIxuP2ir8htD4I3KKxhmLWsfJjcNtd2OAF+Zu0Xk5asplaF0bDEq1Cl4fZFiqHaNzYqF9z81GWGdijm3fz3qPOQB5NMyT+R2ZsL1jGzD7mbzD3bLkTwoyXEZ8V1q+bZIn7RKYn1tvfVleNiAljTjLegyZQFap55cGC4xC0w3QTTn5yjqNeypmTUB0Nz9/bvk0H1fAwGyw1TC06BFNQedySTkKA+xEaB1s+OtPf8vNkXpX5Fjuhbil/Chk3aYjNnXkLbBhkP9lap4E+ihO27dcATbxoYiZBkNewn7RMoyWyUZnA8NGvJK1xqqCzC46d/zvFAVORiJ3h7T6vpgcDN3ReYHcNm4Rs7EJoFd+h+KYOvM6/67X1RthlbF56AJvIQ+MajL/GbaV3P6jGocoZNP/yk29MXcOXyHSixHZN7O+LQEGV3nun7YTf/97b/vo1VOAa4IsQ0lk5KlWt5o/dSGxhBOYEiIUYvxJ12GcFcx/paQ7paP87MjAZap+P+83+Q4VS3eNSfqbgdpP6K4E/0bqQdF5dJR7BaB6+Cfl/pHAAGnvC5dxSwZQ/USFks/Jy58WdOUDvVew2S7RaYhka0f75w83bkOjcm39m1XJQs3Lwnvyl4LvXNLo3SQqH+kv90wBF82nuYhuT/OHnWDEiTJdr9jPW5EXSuCbhiYI/7ArKem3YqEtcoepyX38xzCcOeWw/b21FwV4w3AglAXiU/HHy3XP8Q30ulmp/dZdr17Sj7Uo5H6AniyLVct9H/FefJjOO9Q7V4rRBuqJhnYYka0DT/128VnaPLbpAyM7UmoBjeWJOV1FC7fnAAEUC8ebGlie2A8ttljQRa9853NtBJo8TzOAGccfryT/RCX+KuNLf1VnJWN/yLjBCBSQqE0y0evfvtRXDUgcSFd0Z1L4atfYXv8p9KnMn8kac0oCRnc5ELxb5fJ8Gc/BLRpnngei+27SViuoO18rmoZfZ5iBqDb5wjfRoEmsHJq9l0M3sknD+wGgoh9Pxwlfn7XlVLp06U1Ov4sN6XwdVlBAx5lEncPMU7+3yyp8e+2w3swMNS/Ht9iQDafRMhcMS3TpXHEq+u6LWPgQQsJ+tKKveh0nE1VAane0pwAAAwDJcOncLZ5JDBOkDSCNoI+BgW2ZvWZni9sOftxm6tQCMFNBOj4JA9tfzIlbtsOk/ElvUSOU+5NmzB1nc+lKOjVhZVJSOUrJN7WZ8wckc1PJWAbjnuonaTtW1DCmGpAWuRljD6TP8B8ODGihyY8WCCKDzIYIswxogXeTTxBarxVGWzjH9yVmx34CERx7n6EkzRJ2RZo6XkDz8r+uDVIcCOYxZl5pF8jbXhMTwxwuMKt72zXUHupmqIOb58Vbjvzdm7ylHRExdDT6Eq3iTLroQjJaA2XkLrM/XHWneCGoUHi+/OGOAuufMgZWUMBItkbPMhwCvBGzg/8FcgFjTzjzj1VDVHgTTJyXa642/Sap1/XcPQBkPfVQk0RREtJr0i2u+vzMZaKgePmHP9iqZ/97qm269rz8D0SvAgIvf8asQd7qYHn44CLy/kcktBhoGEAABZskZR1M0wnIHtoHsr0O9F91GxTqYYKGpPlNf0FJLN36A/5+t3WobkbDY9qoYIHjRtWQDhCBGGeQ+2yvKs3tKvCEmMyIwcBzFK0/0wHsaLbXNvlJIUoCTwRs+Bskv7SV0ssn77RrzSWgjcMkyeseoRbrwdKyZnn/thTBNh0uiIdiZWTHZ0B9lST7P9qu8m5NsYwN0ONC8t1i5Mzd+2oXloKtKahS8z1ChdR28s4YdWm+UmmMlhTYMzD9BeRRfqANUi9CB5h4MwiiLlYGmAmE/mObaXNfv/pwGvbjW3o7iAW1gEy/OVjppJrnE5hJCZMw0FI3SZsOmWPLtUObYZxjB9DpogdCryKZcZbCEl61l8pAYOOQO48dlkWPTngtSNVMjJYUHkokaN4k4w1o4rSSQp6VqNGK2QBigeHYae6XWB/Y8A57RIF7FaUXEFYL2sGTiIaFDoTwAYvWbPYuLbL512xT8u1wZyeKo3SnVQpvB/573joJ7aB6b49wX8Zsunk7hrJAl0+BQMJ8mICgippOQJmN1ctqKI0T/0pC6HX3y4AASGpZu1QOy0zway7FiPT9WZY093tNLmrbCusCGfqx+i86sopuVY9peQW9ae20NEtevp0AY/KmI8wbtaWC8KSnft+gvZWeQLEw4UJD/FLHSsl0HV/S44ne5AEP41Z0gFwtxdN0Jk1A8mkYg2SumIkwKXTAiVEAgaKSysEIjTecU6VAV6+QX0Ajw/irvE/YkT60Hfs5bT6Y73LuikO7SKSl24bQPDZPXXWXQBZJMkVXNH5JkzQy/Ugiw13za3QFOtPghgJClQCOHk4aPL7WCjL1ImWGecastwG9ObVecvvXHzbKJFW7bpSDRktofqRR5gt1gXpNmSK7ORBc/ACTr/uSpNK2+Y4n+8ugAUJdAXqseDEjwl4avIwG9XXvwZMLGDU7pbG1lVDGpVHEsxVnguZLwyz6OhM4kLLqj46/IjMJNfFSrr/8DzUc9ow22azkL2LDcr78DHo/rBNSLft8hAGZ+FToK+tbDAntJPtTO3+u9KetE3j8k/xWPBGuBg8EoNJWQGc8YCsdQ0yTw1VrkaDbsK8WcUl+A0ga6OXJboYWrav4SDdOaEbxL2AErADxgQAAAldBmiJsQ3/+p4QBHZrhIKYA4AFh1xY1HY02mT3s82lZEwQSCwbm4jgVARfhVmdYsuqeXNPtcVNDRd6ms0v9AtMYLh07UyqTvyR1Gtxbx03ptk+1JNrb2ZClwblnHEGDksJNfWoM6BbLqStjPIGHx4Reg4v46WdeX2U5deBVBwYc/Ef2zN9/p3OLxA7Il00C7rGaytDximsK/nCzlqnBE72jf4s7FA1X7TiLQASgBAh2b3/xKpd/+hDCpXgo2d09Pip2kiEBIQi4g4cPSbhLwNONGrjUK67JC/cJh6NumHGAzariINsC4H/FVmjCMJ3LhX5CwbUzTppvLczNoktdPzGmFClZbS3jpTw73epWiljJTVei7I1rbX9242o+roTLl4lNuJuQqXnNzzm+hDmLct1ARsDPNR5EkRJGXNk7AdCgXYTbU0/utzOSMYoIN3i9Ray+K/QCRnmsKVVUbutjmNn9i9FsR3d3dNOiwJqcQf8SC5ZHNnpW19KA8iOXrSIrOOolZ2c3VTcr2r/OCbiSSijZ3CegbERbABwJlefe4K1B79OU6kP1kkNGHn5nBltaPDk6FJib1eNpF6D2XxEuGEsdA6Jdt4+tvXMoUB4UrQE3cbCkOwL3+E/bGQdOR7wrKoVjn1ft50PnYSrtOjWyCs/q2uSd2kmi4jkryF/clz63vpiOVgNo6xsNN+afN6Hhc8HlgOUNvI1Be5fIXr1ysGdHZPQbGvrrfagMF6DxbX0aCcXyXiL6SeeabekPHaxBqDk3eOJSkrPA89S/ui0UoTNG6nmlFoEC4AAAAGwBnkF5Cf8AEViMckY/d/bn9TekaupjSJO6VLwc2qMQskz8oN4Xh8FSchCbgZNt6SQIc3DNPWwGHMLDK332QAdlOIcGjP3zGj7XE62xleI8JRdFk0g7WVNtgzAHDVtWLTN6dRTurQG9baqEMIkAAAEyQZpGPCGTKYQ3//6nhAArXvCf4/3svu/z7RoAHICVFzgCMrkJbFm9+5+fVk2okIYW1G+j6oMeZTaRcwrOv5WRuvFW8SNAiarC4752eN8QU4MiLvUz/hgK3e8xC+z0ZcPyaykLVCZSLZdSAa/Tn901UdhmIJZ+/PLiMCqN906DmteJYtVL2B0Tox+Xqi+LLaZUvwF4w649moNLsz0Tewm8NRnUuwkCmTGwMlRkc9QvyFK9G93ew2y2AEi4dm7Vfcd5MRJiit/xmzeGhQyQmTqBPuXcS5brxG8I7QsqPGT79PV5CZ9WdUld6eXMp2fBt874x5T9pVDBIfTyR5ZXjpQKG7JX46P9ZhHWTihF1jbGL2VtI4neJnUYAV6uJfMehNqFuF54jwUt3c7O7suN6i27+2iwAAAAr0GeZGpTwr8AIrI7k/p97Ve15mY3FmDAeUZ/o0UDkyPDGATxaJJwAX0pUxVJgPsXBtO1dKkvoTDwqcWPIFr/Xp36h3gghreRgpW/BrOSxdCvAczFvAfg1xwgLdgkOJr4R+Ym2CpE+Xl4COs5dG1G7gvPxDvxGc577IONisdfCLA0m9vOC0oMN6FIG9kr9HydrSrA0HBcMEhxw3zBfKr5+WNWv9eviEkiLrNnUwb7ZzkAAABfAZ6DdEJ/AAZvye8i7PJPN8b9DTAllLDvO4AB2gDqkFUNiN+XstsaUZGahE3ms+mTmePq6btkFu9FbzOj839x03+orVQgra8hsX/w8VsI0dZp/xDwUGbPvmqd8xQZsDsAAACMAZ6FakJ/AAKPb+BrpiuEmswgAdOrn0U3h+kSygDhv6pI1FqmntEeBZ24f36I4Tf2m8a5J7RfeN/rwiQ2F+dXSDlUoyUcFEY1ALb0g8plfc8nZpG3OZLBMG7YEXl9/T51AQ+NoLYfTrCD5xaWQ+bFCid3edkfkOi7ABDQV4UKk5FB8T4vCr/7YYoXBLUAAAD2QZqKSahBaJlMCG///qeEAAADAABLVs2fsXPQ9wQADRveahecpfwe9+vi/0bpsSTsDT9PLQdN6bDh//G5um2vOtW+4L97WhYs2Ckw8uK7NbWiT5FSmA7zyLF8CeBxb8W5mwfMZxyKd+X80qNYOVqDE2kkASXYZKUwXDyu5im45Pa5a0xyyCfSTiDHaeq3KRZn00//SUJ+HtWxVswtJHXXwwz0tOLhmQV0qpmXhW1LI87cSvkiGNmgK/vvfcnXrwcgJel5CaYPNVsD77I/LspaqfC8fMWx0MKVY0s6zn5CR+sWpLNooTh2BQv6B6JirlHKRFH8xadhAAAAskGeqEURLCv/AAHxr0RfroGwkwrN7Th37t9AGKACcmGlK/JPPa0HWxuUAI7Xn6sxeyqlOQW6T3AG+1oHq452JGCZ/T9xK7EjDZ91Q8llzq2Vxc4Vp9xewAKeAvlhPkQNWdFgVVCc6Eep8Dd4m7vOFAarNiyT7xwM58Oi6txX7ZDkQmrVF/vqXq9mvbeyaLG/e0vKHfLykdCdOUWsaW+jwXX52ByndDBuANLqh7KeuH+xs4AAAACUAZ7HdEJ/AAKOjGr7sbYH7AE7IKAw6ql/54AAAvkcNld954Sf+ULJXHF9WOFx2PhylZyU8emUosS5hn6dTm7CKaV7patRvJwqRaA9DNh6WP20OwGAf4ogCyiR1ygAleARITUtat5prihQcL2zpUo0GQb6kMdUmzocN2kIYL9i2467vu/OnWZSmngqEbWYbZYtJtIEtAAAAJoBnslqQn8AAo9v4GumK4S1QSKI20ADsCRVMgOLSV7IBLCSS0wVH1fOG5hf2Eeo/2ggWcSS9ZxuY4q0DquWKdyoxKu8C5aMP2kvKZ1qLr3oD+0ZFtXwk8wORSk4HerKbeYX53M11QTzVgS+luCRCYQtj2Rk4UPcFnCEPK7XhkZyoCJTWrIyrBC0qOgwIUMp4FjTkTW0qXg6IdyZAAABJUGazkmoQWyZTAhv//6nhAAAAwAAS1M5ejA07uPr6AAuwfM32rYmXnMGwzjDf32qqE2ujbc47IlGE1+JQZJJPs+eQ6bG8CF4VPgmehc40iS1OTO9AiDry1bzrJ5S73KlRprpPLUX6efsGWmsx9VLIdmjNA0TEYaAbu0Mt5F+ZMkFdb18CpxPEW+mVfZy7SQndK/BF7kjmEGSKcQbnncDov6+/iwTsokArhCpTjU/oICK+J9gr2DIsq0ZFf0OlxBgnRRa00f0CwdOKb4MGxUQsTaxXhsUvvs8aJCXcOIEFY/WAsT4DSg+o/gD/Ae+x+Qj32IfgQGVR/wUmvuHAFpwyDQ25ca+1UDb4UG49HUOFrsn5MAYhA3SbmACLO0FWD8VQjDu1kzQAAAAwEGe7EUVLCv/AAHxr0RfroGwkwrPTcZwP+oYqFEozFhxABD7fVxKqKfiateDREncPe+c6DTO2xbB5HPhO+KbGD1wI3HWjc7rEwoANbepPEU2tS2VHEdYeLm7cumkZS4++RMJwHWLi8lq5ItDK7W5MZ2+JQDF8KVOuSZbQj7/aqt59Kb/Vnyd8YSnCm4vOt3TBmtbLOAZ77q1bgk7PeBdA8OkoLdmImVSG4XtBMUf0xqawM1cmIgyILt4pePOWV8+gAAAAJ8Bnwt0Qn8AAo6MavuxtghwGhJnZra5AALtpe2/eeRMQ/cU54oSsXZWKhAewzdfmH+yfdS9tIOfZ7mY4uWcxuyaJLa49QxV3DK0OLpZUVBTmpiUDlwfvWhwG1FQ8bR88opz1QNTvlXzPJwjqWnTjmkpLaMVsDnJ/K9b1NqR9dF67cc9mi9F94VJnl0vK1naCm4M+ac7XHTzguAA1Qrv8BkAAACDAZ8NakJ/AAKPb+BrpiuE2Ji9BBZbPRuM3SO9u8VB+oMfIAN+8mftHbIWesOCzOg1+Ehwey35Cc7rkC/zmwtNf3tdpEfH/XjGUf1qMXWAXddEX3lv2fKeBESecGFAVImUXK41e3Nipw/i52tVt9KZLOhew+sMr7VmHPbDYqkkf2ihfVEAAAD/QZsSSahBbJlMCG///qeEAAADAABNum9o5vKHow4RMCyQAcu3/4RwrlFBVp4Wo8wgDkMs1/tx8nGFhLlE9+8I4WyHzQgZULSOJNSg5kz7cxIbP0NmegH1Ouf1WQ7gQROvwfkQxy2fQJRm6w0OucHu+FybUoMNJ2FFsDhf+X+Cix1MtUdlUKagYvl7uzlJJ/e8nqCmr38eN4Csij22YZ7iospORNiIOj4lERb5MdXEnRkxzzARqi0hnFOg4Oe55ZofGo9QlQKQsbZ0/5kDK/5Nee5Axomhc+bnI4hguecEd3l/xi/963uKhd9MfT+K50wDSuiosQ/6MttHNyGT1AFBAAAAqkGfMEUVLCv/AAHxr0RfroGwkwrPHpJHRAJpPkrS1cHlassOdFixcl5vXKVqC5zCEd4GwhU/Wq0eE9VthpJKjs8OZYCNtvceZK7N4B/j1TtzuCDobB+MM0AbG04qi0Q1gQ5SVb3WFUq2GUOCI692T9GU1TGyrhybGWgjOTPeWaQ1G740C8Kzde7UaYPeZcjdRIYCBySmVEKfBJEMZ/Hs72MudR13LiPcoc7AAAAAngGfT3RCfwACjoxq+7G2CGz3eEdFQo3JEU4ATTyeucGhWCtS+P04Xxm8U84rdKFpPdYM+2m+nFuWMv9xWiEeP25tXktJKb+EGoZwSZNagfJ6yTw6SdqJCHBnigoVcYKI+vXvAmLUEccACCvG4qplnXxD+U4U6t5Qfh6ajaI72uQnMCREcA21XFoedn2O1u/5ULRV5NEFTSd0gP2WOlHAAAAAmgGfUWpCfwACj2/ga6YrhNVxWOuTW89/QyORPyOV4EnODAAnUedx+EJOJMBZp5/TY2ikq2sjIm9Ue/omZoa8hxyfBi+uMEMVUbrW6yLCd1CcpjgkoM4W7rgIm6cbF1y88p1nQggd0x+z0eZg34I3Q0Y7K+KRpYJyBM6DBmrLMiSCV7LJudX0T6AvPs9tal1/dRcoeN5JWKItRIEAAAE2QZtUSahBbJlMFEw3//6nhAAAAwAAT5TT+syJQ0gBLQ/swm0BGA/EjsXAf4mnGYncaoxpt05+ekBixzDsiPyldOP8EWwA2b+E5jI75qfzp5EQ/vv1sOZMr5e1Q8CeWVboJNVo7wmRdB+wiJM6xCKxh4SX5Na8D5NZKiemaZMXGTqi+JwvBJ3wZ39sMNqCOYr0DQNueiKpA88ZdDss21dfvCbbiWGe28C7dALm+G/UmGoqoBnRjCSzSHsjWYogXT2+qVhRVakh/m8oaKh37GmMxRItrg64omkkk5wqghX25udBqJKYtxHVpzKl13aaUN8GIfzw1k3LYVIlg/ZLzshRBNOIXRnMr7gy733VQfw4F2BswYq0gW1j7U4UfggfiC1WWQK+G/ypKW/kxPa8iU8AH09ui7+HuAAAAKoBn3NqQn8AAo/m/eLlRXFQ2tv+DklhgOqg8yrjPkwAHZFb9XqefRtz9Y+3Er+KuadMelP1ZVAAMe7rjXMnXLsNNMkPikqHBOuhENG/rZ3eX2dAxaqEtxq2R4AbLm/JpQv0lXjdjc5B8EEiCg9TcSoPbo3bIMm1ZVYP4tOM1teIWIwitXLgzcKu8cUUlgxGQrY9hc1F45liW4oq/+MTrq8eelTg6xyO/k+OPAAAARZBm3hJ4QpSZTAhv/6nhAAAAwAAUesjDXpMgBZFRm5Ri2dUmZZx65epMvuXhVDzWl9MWWoP64S8UiB2cIVAHZffrdBblxMeMapBlZZzXKfGkYjOL2u+1ul6200JzmaC96fliLFtPwKp0p400hDCA+jKV7V8Ew2yaqhezZL62koNU56CwYt+XXq0HPoT5aBlehiIOteOEOT/BxCYe0f5wIq0ZkQN0OC6C7XnEGgo6kSRMh7mtF5LvuhHNZP9rYIkSM8DpR1oFyddpQanuSfgVFb7kAsgXb1RfrrqjPScM93ykpzfzXr13Pg3JmCg/fejNceUY/2dMgaMrEPXcThDNxGmw/XIfRhbQRjGaG/s5dQzLEIDFRqVwQAAAL1Bn5ZFNEwr/wAB8Y3YDXrDIGWlOIhhP982UUAEPz5Jwld/ewdsr86VHPJvgPOZTDK11WN956tvOi3xGOORhCdL7Ng/PCAvxpxcAtSYtx6Wt69t4zysGp8s7M6YGpbhRWYnG1+DU8Ac98VWJbIjfkEF+ACspdhfoluRE/yrN55NczlC/YIfKq37HiMOIS4tpYbJm5t72UiiVuKQ1tGYZBYUSlCwSofETMilBsmc68uAf5aSPz0ihuS8+ZsjXcwAAACbAZ+1dEJ/AAKOjGr7sbYI79AfVk/XYU0m0NQ1iuHYHFxJABz836wKwGm4+1lhwGmbk0Bvn1PLiKb6iWNFSNWgCsC2zp/YHJk+vffBUbjStfVDjQqPqUujQndwH4NvFfp6ZS4pxzqwsfCnCmHRlcjFy2bruJQBEKW8hjupk75j4jTL2zrQNFKfz4OSCDPYOqjhwdKBTeZh3trGNmEAAACUAZ+3akJ/AAKPb+BrpiuFHj3m43q4tF0uYNiVh3pU2gnmTSTAAPDruw4BOd7XvuB7hGetuuEARmAlFSzDRrC4GzU6kS+J4qkYSLpOUxwhekdQMItRJGI3HUFVBaB6c+kV6v/AriHaMAnZEqHji/0yA1b2c/PQKDCUPrfIvGYD6516lPKlhkDdrzp2gqsZJYfGGJeZVQAAASJBm7pJqEFomUwU8N/+p4QAAAMAAFIPACg3SAEuUZshLRVEsMnN5YY4EglbeRk5z8rN7/1s/ol+q1tfseKfxB1c0Dxrfh9j1b/ezIL+/AMSU1YUG69aFf1vh5eEuGgwBtZz5Dy3wqJrn/ijNyot+v6Wcb8rKPALnaOfO9cz0dftbx2pU+in0e0KZ0LyN5eQBsZ4LZ4JAyW4CNY1f8wCLUnqCsg1SEbpccpFc7xeFJAqlTlmz7WRqyHHACVJqhx1m9JlgSr4dtG5LDjcSI7H6Ap4fxz5JLgoZgVZcfEVt9osDCLb7pVWTB9H7hhjEFBimrnGvnyYo4N93A48J6UmOmW0Iu4On92AjhgGhmlpe0MaKyy98Qd7AY06TxauQqV2ZZuW4AAAAKQBn9lqQn8AAo/m/eLlRXFQ2t7qwNekGaQdofcYwUjKw4zsXuc1gAcPHggmkGMM8GlrcRv3cOvBWQ4zYs+XN6fJORSteGLzpzeovsr4Mq+n5+4NqHCsT3jGrC8h18YlYVkAqbirsboZR3yQ9Fk7cpCsHOof89WDH5VrlykhYBzXWt1gRvA5w4JUD1HLy25H3wStaYDC3IMdrRkyZ+OU6FFWHzDZ8QAAASZBm95J4QpSZTAhv/6nhAAAAwAAUaE7j1+8aJMskY4SAEIImcr2sbT0iGUmAfojnk+ACfIxvq7RLbI1pdGB7ESc/v820J5DgdwcEveUSwXRFqOhlD9bCtuyiChVDsZK9utNvXuqiRCwHpiCDuUAYktfueYz43Zh9edh8EVaJgDdkaLUiKnVYVeEzRrf4ehCJOdNAVdJKQOLfHS1K67ay2Caz4N/dC/kE126YnnEea76Ky0n3/rLMwCtvhoFYAajcjW9R05if9QasBDeNQowGfMpmF0QZe/uf0m07LDy+ilmk5zXFRqJkMO0gWXdTBrD7JQgN2h1RK2ybmVPWnlTAtLleIwnuZ+GAz+wmdrNZaNCHZS1HM132xuzqdE6AUQjzWVWg4aY/rcAAAC3QZ/8RTRMK/8AAfGN2A16wyBlpTgtuc6L7s+xIANqO5PW+o67PXY2nJr93QZ59oVFueEIpBf3HJFZPOE27teVvcguWPeTvWZYnCN3HlqQtMT4jeQR/E+EWVlkNtAFaKPkXnojQfG93k5Y+yh4TZpS9nual9i2JDVfcxn/lp3IKApljdJQiXgfmLweStGXwT7USu/A1mkJBJek7nMg+1u14CdmZzor387+HMZe/hT8/tXX8nKT9kCxAAAAhgGeG3RCfwACjoxq+7G2COFS3eXSpNAAW1EkOlKq1uNdr3SBVx5C0zJAVzlVydrqEAkbnScEWstH+qpHIhX5UR0HWmOiZ3Za16EoM/hTxeN5Jlp/CegHcKumf3uTlJ91U8dg0+TfCvYDCayern7euKSihwH5M7t+FbhQbSsAOrmP/xxnrWSBAAAAqgGeHWpCfwACj2/ga6YrhRjfBVUeSAAvXC6JXj2OrwSpiPWngjE4EWDWRbQ0PoHwbxoC6QqnDOZ5mJQCdXn/O818o8bcPuliwIZT66E10tuPCU0Exx8in5/GpvVgp7NesrD4UFKIUkMVIXXBt+7iFG+zciCEgDgK1JQOjcTVkK19Znfg3QNsB4s9Re/TInQ+IUpj+K+L683URhtfFmGaJlEcdris9uOxwF0EAAAAykGaAEmoQWiZTBTw3/6nhAAAAwAAUbGMxDyQAbUHaBRYlGp7fosNJd03Hj/o3V8vtOzSHgUGTAk/pYend9PPm2zq7Y84eTVRE8N1H9+d7HifmCQrzZbPaete7pTjITaLsw8RMu/5l24v8rCWTbOOpkYSOFQ5PqqF6Ce5xEYIa56oBBkkcV+JBCH1GTItOqGgaSKIUaD2YWc66CuXqZnx1PTgzX9sG1ARQx5ZJnj6ctdocpFsxCLqFAiAigroYj+U7h6oVUp9nfvAGYAAAACTAZ4/akJ/AAKP5v3i5UVxUNrhfuMLtIOVznTz7Jw5SLmQ3Ze2VCAAIfH2YfVqasS0BzsJCt/LzJQs0tDCFxqceRdmmNiYEjv1pTUsrSfPl3Rscsk4tPFhAXAx4h2PRvi/PN4332scbClxRj+a54vmXpSYd/xcN6N58Fo5K9dR7OjG+dH18yA8P3ku5S5fuayndCAhAAAA9EGaIknhClJlMFLDf/6nhAAAAwAAVE8rtQBbroRl9Bm9AAh+tB1y2rBzIiHMCnBS+UbaP885dNBJ4pQWuFZfCMdRFmL+WQznt7kedjsS+q0Q+FkgwsFC9QvI2DKONPM1az1uaDp0fvaD+6Ma9pSACxpGdRqo731/4n8w7W6DEwBq78HAp97OdxgVR0iDjwg7h0egl9JP9jClWDtZqqL9SCihmz92yZNFiqVG9+T/Di9XFXDgkcxINeQLm8kLNGQT8cs0iEqorEYPOH+nYc/s2pgj20huzDYyqzDb6VVdn5oetOu6EE4GtQfAo1qbfcut2ppmK8AAAAB2AZ5BakJ/AAADAABYvMVpqLQePAAX15bbWk9z7GdkAYP/0jf33sO0tGmjqwbzNvRQusE+9178BhwqIYwrHgj5ONcO7ZPnwQCNMOfuEXGwLephGzV2Qr6LIkCnovct5DyFWSuA50ZgVpCzr2XN5t6N5oWSnFFlcwAAAMhBmkZJ4Q6JlMCG//6nhAAAAwAAVFWH6Wevl0OgQAJ281Mr+o7l9ofzfSag9m3Np5uRXk2FoaN3zJHRXeEv+HpBte2xatANKC+oH6I005+Sw/j2TIyPpRinc8asMPRapghPR5oDGGoPv6SBy/OM/XCbAJwqyk0KINnV5I0d4E7HthNn7h7Egv/N/d0ZJsbEkXzG0bUmGGvkyxxS2g2humhLq2gLiDCXw1FXWm7+A8TTHLSgwfLrbgrOxFirXAN9xBal3F2ilVTeFgAAAIFBnmRFFTwr/wAAAwAAhtWDw//HVx4GoAEIyX1JzhD0RPH2ct8LD4hMQsrqAq/K3Goq0fZ0tvr1pBtfQhEE8vDVf2KFWXHKZLZbmsV06TSCOGlRRMl1xlq5w+VF6E989tjpl7pHOuYT0aGm+ez3hWkgZ37vrOCXWGDZl2AGae5S7rEAAABuAZ6DdEJ/AAADAADizITZwWtpjNeAAc++9NX3yJeXTct3mBNrkvPxt10lhitOGLDHPDF9QYpZQV/Uz8fVaL480tuE/mzrIA8iN4W5bKhXECWNNCS3VmWhV9G+Egu2BAF3HdmpsAkXm2NP68FGst0AAABTAZ6FakJ/AAADAABYooQgAlSRB+KK9FaH2d22oCVHxlprtE2e35aVP5gWebD5ETP0MyliW6ABl/TmKu1duaQDIxbBX9sNXQsBXf5vNGQrbpAYg3EAAADFQZqKSahBaJlMCGf//p4QAAADAAFI5lzvpvp7PgAdoe+pQV5QE/va+Apuci1rBXOHICPykgmyLmE8B0BS0+R71bcKMy/Rmwyoof9k1RWIbWEJ5CqqWMwqkZYQFCMlgfnEbMa2BVD2TOulhpuUNYquy14/F0Q9L06/zOYHNRbVuGFy5tfQykrCOJRMh2FxOdncyj/7PQuvQMI4tymBF5fLAGtxNDI91sWHfrpDE57GDvgw+EO5D44rc98WP6p0Q43ngu3elC0AAAAsQZ6oRREsK/8AAAMAAIcGaUduBNYh7+sckAH+6cCkUU9OA49KJ/ItOf4JcJAAAAA+AZ7HdEJ/AAADAABYtKVeeADmit2VBwcn0RrY+edQ0RXq9eD2w3cshSdlngnUSPK18+JQC940ztbFrsS9XqwAAAB5AZ7JakJ/AAADAABYlBQsZH5gteAoe54AOY1rYSUZ6icKS99x30Rj1gfRq1wtUozGxNeKiU2GcOsTV3FJJDzCfwkQUuT8TGflruBrJ8oj2jfH6jVRgkQyPBDeBymZSsagiGLxGY8twOw6unxSvj8iWQgjEHrwxBLeNwAAANxBmstJqEFsmUwIb//+p4QAAAMAAFILU87+qh6QAZd7zOV7KkKMtpfH0+zul4SK9sCaMuDMRKOol5x8ITYHzbZ2onUSNkDPhEoyTRzSzc6QvMHcXyV3tl0ZKW/HL0aTuReb6EflBEp6Y3IzHgOypNWXiufE5BxTTW9M0YacoZfbFYF0RaJ2LoIruTOb7Au4Kry2fP6pKHUTjn7/95jxv1vPAO5lsNDlAkF+eG1FkOzB/Wv4X3k3snq4HKzl2nzwJNV7ai9syliB0h5ZQaw/RU7XBmNmbS5i7820fBVgAAABDEGa70nhClJlMCGf/p4QAAADAAFI5lzxObZRPTapke3Z/AALeGbqwa396EIRgbUzUqR23OmOdLErmQk12dSnePl83vvJGqMkkCjiDKko94+HQlFhPfFcQqBaKldqS97bRG4ecHSepJNBX0gxaO2LJbPe09z1qzgMiBDIha8Uz55wjQJ0eTWyJxkRLvorcutRneeCi9W1mKUAFWiFXuKQVjQBR8e1NNqsEINqQa2LUCrX/4enOBjp9fsx+HO3Y6Cm9Vn2RDgtcv9W4ZUHX989rQ4oN6rqu4S12Bl968deLZgplammHBnB2cesrofxdbzBxZV+P++pngnUgp5THKpb5Pnxl+m6dY68rF3eHOAAAAC1QZ8NRTRMK/8AAAMAAIb6yOMHByGnoPcXmzZDX9ABBeMNvyTN2qprnf2b9sm+7x83R5D/HeV+5UnEJLPozJuRfLpEoKlsCNZIk0pBQTF2hs5a5Y7k1gxwg+LlimXnqlb8J9c39yEetOPEpL0sM/Jx2np68kDlFXZ/wQ1+lcHLfcvnbn0hrPYL5Qd8cA7zpQQt8KZwV8Ln3fslwV6F0poaqZpBXt2DvxkdSksb9VH7y7/KwW490QAAAI0Bnyx0Qn8AAAMAAFiRS/KfK0ABfXlv0kKRPk7/AmTzoJ440DnVgbOhHhu8/8e4HhgAA9DhFp+mMlH7qohjyoNNP1I05qnwKQAxbB8zhcpflVUjSWFXOP+yHIm+85chEjw17yBntCQcZyU35ft0wyt6TSoJnMOJJZnlrGjVvhXW5Dz9LZoCT9J6T8/Sd/EAAACaAZ8uakJ/AAADAABWaflda8AGxp8M+zn44hxgOHRPyK1OuC27akSI9+SaitNlJhPPash1vBbNM5AgKAyiPp1Xtvet2i1p7V0HuO97kVZaZK6W5b9T+Of5nnksEzwg8YbQc/1uXtQ4swtoi2kyBuKzaeoYiTYF4PE6+tTl3XEMVidBVJ7Jv7xhwKRHCckIAniaiLDNs2MU/wTI4QAAAQlBmzBJqEFomUwIb//+p4QAAAMAAFHADbdTJ9EUh6WgAOy8sbqcqQMDVz0pVd4XYabk2kLaO+JEMvH3Ci8X0sJX5sYShEqfAydT7uhulhsjleDhQZbawYprgyzGqavGE8BLg65EMWUhX5dj2qUu0KJuQPjWXYflhQeCpQUk+6OhziOatN3dOXyCFa0o04WDAU6HMTf0ZGHXes1VWHpYvPIf7AH9j1uXk87jRvxlZycKWiCcGVpZ5KbM9STQmxrOedW5s5757W/Iboyc+G8V6jnGBc6JrtgG9GuZsfS/DjIdxxDHf7zJy/7G57U/58QCEEzAzn1yygNAx4I3+QVo/UdIspm/sv9pnOuAAAAA/0GbUknhClJlMFESw3/+p4QAAAMAAFI94FX5wdrd8taLTkYxbwlABT8Zn9fjZ6kZBHpWvFuJkBzf9nsDVVcVqFdI5Q4CuYiJ95Tw91no6d1Uayvxw+8zl9Lnwryvni6gY8gxfuZ8TsMIGjK0BF11h6F95/xQonIZJqRBvUzD6/VzXiKvQ6iN/QwKJVXTcWHIuWguGTN56Fm1AK8bk9V0UUkBMMbRYxAJxFiofisAaGGvco7Zuh57zflfcJyaFtg8pl89BKa8+AWoL4321ndIOpqxEMi8v48O1rWq/k6BGQjz2pOmvAoJlKjUIp3AD6s7nJWsqYBMCpZTxg3evO5bgAAAAHkBn3FqQn8AAAMAAFZY8j0UxNCr+iDWKH5Fg3IDgJoCrS+xx5y6tXXAAOfa6JVyTFGWM/Lr8et4fiUO6DIBm0zBA/JBDn7w9R6jX3SiCJie6Xew1CdF3sFFbSU7raDrLQbln42L43tcA8Hip0pEhMSDhovs1BMCMSclAAAA1UGbdknhDomUwIb//qeEAAADAABP+Zdo5vKHn24sn0ZM2wy9KXS+fCFK077w8ynkBrVYYcRACoDH//X8BuYpA1xiSrgrCkv2uCqpPlcOXA8SduDILjMKihK39eaIlnnHohgC39/I3jvdRphW4J4k4J7P170rCB1npnDsbs2uDa+J7VknQnXP99UWwNQEkclu6Z+wfZzk6gdV7UrS/DWP0HJyDuFObPe1cHF9Yc6R9jaFSAbBks7YlMUkYQcx06jpGrQMHKZpDO7d8WcF5ah9sHMQfaWYYAAAAKJBn5RFFTwr/wAAAwAAhtWDMmzQyUf2fiyVsierTIAKLDrhMPQyVxhxOGfUJf88S18M5YZhZ8x0SaogqqvM5TX9odhRWnjBimQYNKZXYw14oSTytvkRtTgT9mH+FiNUUhYifHTGRGMmgzi+iqhVNAmnfh4CH8eav8wHkT28GG7nrmlrUdWpshVkzFWXVXeLn88Gu6pACXn8as7pm8pijJtGQQMAAACGAZ+zdEJ/AAADAABT276pqnd5tiErXyw+M6gsr3Vd3inw1z59gAHHePIXdoCZp+XwaY20iNLaVeOLV3JKnjgWV3OcqfGoR7jikmx59VJyVHP+C4lb4GrbnYXwF4gyENnsGVpkwyQvkh6jsnMN0gh/cRU78xVx+YDF270ekSwb1+8HVP1qiW0AAABiAZ+1akJ/AAADAAAfDUKWONH2KgCljjRTWIjvHHA2JxpzDgA5wwB37+Cd9GNwWpVpUlsdNGu3jtWjyAolvRjXC2Z2PsZKWybFJcInkSva1S65ydVtOI6W2vWShyNd4omWToAAAAEiQZu6SahBaJlMCG///qeEAAADAABLSNVp3UwV46i3UX2hE1OATStZr/wLpHeBqLddlRagec+pnruQcvtvvtiwBj9FMyVuyFfQe6RHiVI2Av//GX1+HIdagodHqUsVReEpVsCSAlc7IdAxDQl0ZitY/0kE2xxlCVqFiOCW8Af9gJpOkVOOUCzPKy/Z36oOqAX+y++PyiWacqcTI+LQfaUbqpYu9GabmffTw4o7ScHDX5dJFY4VUXqketfCiRPqaldYXsACWdl30oNQnZa0/S73PtcJtV+MPWf9De9gsRHDw1J6nYHEKHhojW6mIJFYVSRglQVwEEboGxbYWvJxVlxxQn/7ZVNd47OzB+Vp/7+PdZQiB8NJEBEfFP8818Os+8c1EMEAAACTQZ/YRREsK/8AAAMAAIcGaGz7hzIxIc9UGHJ2EDO6eZPYVxYbMogA4s25dOf+Md2ALdieO67i3YGrVjs/VJizJ72nQgT0UtmcHlD2h6XRjuTWmw+JcW7I4JRcpKASUFXa80WpNnBwFGHQON8M4hbmsEMy/F4Gnov4iTCShcbkUhfPDAxC0jrpVB5w7bHOc8jnO92BAAAAmQGf93RCfwAAAwAATG4wf/TQAjiUIiUlg3lSQgZu8Pg8BK6j4TTI6aR0emicuhLSY0Rgxcg8rplHXr3MDI4p0Cs5ZOP2903yEkV17EHKdmdgrIi20qHefbUbdBUuXucpyJXbI6jkcIK6vsS0yAeL2hqWJfVEuyQeQj0xqfxa0+1wqQeYb/H7uC96e9uUQcKlOnu3ZyBevcsCSAAAAIoBn/lqQn8AAAMAAFH8yqlKO4ouEIvZh5ZtLFLF/B+Y6dVHvO/P+AH/IATVxKf+dTCidHXnqKmpma2E1cOa0RpR/DwvFLfhD7lTLtFO0CGQDQg5iLHPC2kfZ82QnVzcdrGCyU7488E5MU5FbdCmI9/PcHE/YkpKQ1Yxnsow7JDW1ZVHjEWgLMx16VsAAAEnQZv+SahBbJlMCG///qeEAAADAABNvkWzwC5aJfj7C0SQAF1XJd1YtI03gp3RjXNpED1aDHiYiV0ljgLaeA4X57q4HpC5tXz3ArBMgrHcAPbiSlixxiol5mcutwhhd+q71mduPNqIjYAhANepdaItuOICTCJ5KverDyUdX8iXSQY/l+aRlJao02amFz8ZGHBoCPRgw1CEPE61JNjQll44XQUzaFpIQmupFIZAl+mrE1Zyvuz5JvDpbvjYpHNjaQig8OdvogMFMjxZugsSNQfkCSRx4LDWn0FEzZ1wAIIlE2Y2gNI+CkAx3mHSi95W4Y0qQ650IsULLOFNW3jJav/kVyJU4QYqIIA7z47gJOA/9lGT0wxijkvrAV3efVbyxESFc+EYS5CvXgAAAKtBnhxFFSwr/wAAAwAAhwZoahJMhzJfKAEJz5K0tMybWVX9eFBVvmBJQPoMYuHroCm8G25kieoy4FfJM5lv22nicrLh8DCX6b1mWHzlrPyNJE2/FK/vTp87AGHoQUQIuFWc2vfec196BlSlP8xHqqqYD4lmRM0Aj+O2BOOXg+7s6lUv9Fimd/ywkoewGaaF6AvbTUavXAb3VwYiIwm0w+H/2fiy/ZcOrZNyOGEAAACBAZ47dEJ/AAADAABR4sH8YQ7MhU0m6AAfoHSCLNXA/gKoQ6rMvohb38NvSvJjrbLt2CqApSkexgyLwzvQI85aWLPFdR4AOfTcsPWon0cfPv97t609Q+OafYEXwHGLuVW0SUaTiJ4IgciAda5YlU6fyhY/fOAB45XxnS6EZlPNzlWhAAAAlgGePWpCfwAAAwAATXMjYz5RBzdtL+cGY/+8fN/uEADj68CRgZ5ZALpF6UGB5Sntcins6MtImmSfL8BJM59gDfqqX+KG57FOnNo0aaYgIA5UezrCmbnrq5rl63MdCJL6ekBlJgFAalNVL3px2aduJxLCoFFragmG7e25O4Eb+R4KQ+u2195Xd6I9swmVhtm9/kkPYYOWaQAAAR1BmiBJqEFsmUwUTDf//qeEAAADAABLS+bgDmXJd1f8OH+rCEzdxIO0Njic6dwR9l4XCtvExSrJmtSXn7FFkwwzxXFiSptx7vcXuwBM3EB1X6LZ/i76IdUmo8WfTLZeQqCGFXU4Ydzh0Oum5bPRKHE5lz45aSrsr2/5sI4fboodtz4g7EH1FYAKXBhIhLzSQoy8kfuvT98vOtrYS7M15qklF22WDYDr0mYbPCs90yXU7erIHOn1UkFrUo4n8i91+cqeH1yZ7QI4dmmEBooU0Qv2O7/ubKR8VXLmoH9jQyTGiipS8jf5JRpxUUANyJzQg+tzk+XuGpydtYIm3Br+ZLBaid15GFTbvgxsuzV9648NhY+FpdCwb+jVsTUHp5wAAAChAZ5fakJ/AAADAABPmShh9YAOaK36SrTysJMkQhZ6023DsX4mXvQbY+tgzttbJbGhayOCZarskrJBqVIv99F4cG+NRynCkOgYy9f/eUtJXhAEC5qvb+dhQjLx/J1eMXIcdfydUpE7BqWetqqAD2Z7EtHMMUC5B1Zyi4+FPpdDUcQ93BWy16A9Wsx3EZC929WOIFsrMpA2jtXHuCc//a7AS4EAAAEFQZpESeEKUmUwIb/+p4QAAAMAAElL5uAFlcu6v0RC1xWWh4X99RtIr1uYmQ7f99Sf/YfwpA3GLDv0WkxbcNVENHOKlLA/V84AuNUC5dmh7NwNnn5uMOO6WdsMpJfKpFXS/8ya6FAOxJ242p/XLN6xcwNQAQuz/j8zI9wQPbEJgvlcZcq4xi0uesq1cTMQHpEqaXxbExKgL1Hf0U2X24takJpV3ZtdXCXFwpZQ3E+7etmwOH0hx40UXw4PPoVJbTmfI3KdrkeHoXqG4tC1fuBeEr5etzllNKkglHBUxmBdQH6n76u4vq4Dzi/afayNhT0kgJW0H9rs8uZd8TQ4rxQjDNwOWBdgAAAAX0GeYkU0TCv/AAADAACG+sejKESwOYHdSYw/eoAQpujWsWX6wuUkrnr0D+S9Qk/FRW4jwenhVXJFOwKpqMsW6MunRkkASLl+HaqBSofU6JG1teX8vh/UTwnF+t7gOqhjAAAAigGegXRCfwAAAwAATXxyxuM0KtcADs/vyyTd1qtlue1thPl1h6Z/k8i6Dk9zjx1ZH/jKSFm8Ktk5hOF6M65wIqzz4XU++9EBFOrbQM2DSszChf63P18/DXuG4XJrawMCcGrs4d7Kilk/WaTpGbYgO5s/j3YF+KjxqjOtjRaSquoZ2mEkxyVMvH4MIAAAADQBnoNqQn8AAAMAAEt3AX2uVzwAcofvHwbLcE7xjuYc5KodQW6UicEZ9APEG85CbGUdtAKxAAAAd0GahUmoQWiZTAhv//6nhAAAAwAAR00HSAEKe8zlZrMH+tk5VgZA9v6GDRBoob1/T5pSquBi9WsKImnyf8z/HjSa3mvoSqv7Lr5nOa3cSh+lapDTXf81Mg1V7U5Ymt1s+kXo9SNG1i8eXTY+t2EP3hZK/iTjunvhAAAA9UGaqUnhClJlMCG//qeEAAADAABJS+bgBA1Wa/NaQVJavyrBFsUHVSErzdGHib1uYudQ3/Qn7W8XrYWz8gLWtGyC7qx8xJAFzpQv/lASpT5IRkaoK9AT2Sx1ZY/BgazUsI9frA09eBtNVy/ubgTX6Ve0OB0o/Pj7BCMTN3ogcRY4BILWRRncYXyN+awg9noUttvY+49oIzvZJq+gP/LY35zysuRCqJsZafe+AjT55J75DipW7vEg5HSZO5h2tga4scZojKyH0HHLwPMygixlTY6NlTV3aHsO2j5BzewtX07l+aAY4VtCYaan70AJtiYMzGlvz1c1AAAAJUGex0U0TCv/AAADAACG+semG2/ZEUxSJuMDTM6wxVu8F1Qx4T8AAAA1AZ7mdEJ/AAADAABKbsD2+ADnCdX12BPE9bbLm6lRx/JU1+vZzc/6k2d8naQoDWiqWOyNuMAAAACHAZ7oakJ/AAADAABNYQopM7P5FgAcdahkA+D19AiMc5I+ylwyOadRAviurwg8xyO1xI2ruDldgwQ+yckDQvu3G3M0xjLUn8NH/m8WqkKXNwXAD8IEFWhCZRZyJM9OmR23hwe+mkDixp9JV0xX5e8hnBshoYuZHtidEuvGrqVMHDCBypMPSo77AAABHkGa7UmoQWiZTAhv//6nhAAAAwAAS0n7iACMjdNIiQMGaMtpPt/db2k567Hc4xVzAGACxsYSJKYTjIAM40snbbKgo3eMi1Di4E07TboLBqgemYptrz+SzaqECVaCDGnLggOagtApjmrVJqkm1aqMb8le8VIVlOhFjp0ype/m3he5CMlH7Wih/rg/Mmr/Zs2etkur2Ta6Br5o5PpuoctdYtArSEloDqHCdpphvs1R2IRTekWpHD4xhKq4gpw86Vs2rbCJ305LFvjTjXDmftnXdBTYzJGv/iPRJSFyLf5DhDVnuQVBZNZExwzNPqhxMW9QmOfommCT7fPNe187YBWDZ98U4b++UCHoAW09FC5s5Izq0u5HT29UsKjJ/qAHBk8AAACWQZ8LRREsK/8AAAMAAIcGaCRy/v1VvayhxpVePgPvKgUb2o3YJC8lEUw/2mADlQCVDiiA9nTm2Ob2qV4gJnGBi+GTeearZvlTnXFLcGH38WR5mGoVZlcIrxfx4kmiyr4EtCVj0DVxgC8lPbrsuYZGkOrPZNLTmDeZitzfxDL0ZZP9tRAgdW1ipA2X3IFaYgmYg2hgfZngAAAAjgGfKnRCfwAAAwAAT7RdiwesAHOHU6whVO3yXXeObSa+0W/cS89yVLC+6WWxmAln6LwBUhdTSW0g2hSuHWevoZgDLC/h6zL8OAcgNbFtn7M+vqPN0NLIxMT2MmgWao20Jf2rFmdYfkaBADCMyIwy406kL6KWrGNc/gSxBb7jSsU8FXnvfaY3VCaWuiFRpAEAAABxAZ8sakJ/AAADAABNO7sFqXgBzPV1+vfAdW0ADtDqeEbSKJOrF1upRdOe1mRjCCE7y5EWhYl6b/0FoAkgMIrRY4/4MH8P36r+Wa0OGjHkk7hcG/GJoYvGAXsB/td14N1OpW+A2D+FHevVvapFNpjzg4kAAAEJQZsxSahBbJlMCG///qeEAAADAABLVoHGIQ6D4+6KcCW0QCZgP0u4ALsoUQWU7Px/N0+IKozJ63+3+q+RHdsy5c9bJHsGxHxbEXImyokzw+7cAjsAIK4L596wvKxEcvWqOB/OGBImlU/atsmMjyH+686IkauiVhnEM3Qb0b5z6qVmhEeZfXbDwnBA87+xun0tD9ZnDEKAg3L3cEC9dO6dhNtVl55zVclWHDwpcoK8hQswaeyi1kmErKxdTxQycvcOAfxKpe109YxOjhK5gFOWO+SaPstSpS6/+r90uHa/qIFK7tUJGqqduxLSQPtJPX5YX7DbNjXRZ2MbQH2UEBVZaBurfYA+VoH5ZwAAAJtBn09FFSwr/wAAAwAAhwZobPhRbTm5w+MEKQnAffpEtbH5aIHs7zwAfithFmPrHo/RgxVae+KAkQpGBoXh15MqhllG7Ci0HEKlCCmu2hHaLrv2Yw2tkd+5EpuGk6VJz8E3KpTVraIgIdTeJMg9l5rrH/DYfXz08IJRhRqX3XqarPJGc3BlZ506FGz4EwzjjsbvGxtb/4/efyJhgQAAAKEBn250Qn8AAAMAAE1Xif91WsAA1B7K8b7qYdYib+Njqg3IphgpgaY+ufEezrnz5QjSVXfj9iR1xqRv5jVYPS5bhU2rTG4cHjrKs5hCiZZhanA07B+ZZjR93UG8j2aaVB9gNbM23T0fFw0QxwD80VagJmi9BKSwk5o9e4Yoy0G+nW/VoGxES5LvZR64isdtPkv65gXRwN324Z6WthRSC3q8wAAAAIYBn3BqQn8AAAMAAFHt1jWYAnksIAboAB+gdIRVyy4CT/x6mtXyo37SPJW7Mb/qKLkZD+Uwy1cKAZFy06ysPU7O8dtkTMqb83S2k6FdxYiSpzCL1AcVtrKZ1LVujJl68QTXko8XUsXg7afhD8A1YfeS3ha5ILQ0JblhMPpOaSM8SP3jFhM+IAAAAPNBm3NJqEFsmUwUTDf//qeEAAADAABLum9hkLzipgApPD+YmIp4VP+DwAM4ozle3myF5Cv9vOSLTsl1EQtgJ5eBJGc4Ck+xXapTJH5UJ7PPJxt4nJEvh6rlh7kYoiXW0g5aw+do12u9VRyqlkDjy6562Iido/co+sNu132trzfm8xHbIPDb4LYTmeolM9u1QSBCOPVfn2Lqk1EFzAP93vx9rLmoKQ4cGKYEcjZ+A3BgD8rb5Cri5gwGc3uwqhc4FfVmII8gfptVNACutOAIgRt0zaxIlRX8EDba1+Y3mx9rr1ik53MJtJzLUJ+m5KZVbgVWTNEAAACDAZ+SakJ/AAADAABR/NQ2J8tsSsE1UXD9d277IC0qLirJ5Lx2cbvZ8l74Iq49sgCabf/QAOvHYstIHbN5DE9iyaAt03mIBSDVxFXSt4l9EhOovo8D6QEn11dXMHa2UVOzB3iLfxFXC323IHqTfNysxmR05M2v/2Ct3JXm6usul93DfIAAAAD7QZuWSeEKUmUwIZ/+nhAAAAMAAS7pvSrujJcasL0wtX4rfeCAAvr1pjuRkMhuVUnwmqDtp+J82rfx25EIwkg4r/5ip6/3+izOc3bhh/u944NaIH+Uz5flJdPjHQ4oR/ykQuZ/dsivpBz6+squMGjbBYBhxDQVZdkviOxM3ns2Y+LTcLawyoj5uUo2tjZkTF/DnYjALEUqla+YvPVoF5f3//DB4Y3Y3h0Thovx8LZMSrpVaHTw/JPHuLnveuu47I4DYgIVq3OefwaFc+MAgqjT0PMLUR0BCN/A31sswTrMjSWve+G2zlOxV+kPpGg79dXHdXty/4MGdbts+34AAACZQZ+0RTRMK/8AAAMAAIb6yCNXY/3xej+JNPo8qADh7R2Gg4AaL39I33Cc8TSNOyDK0VD76ukawsQVOKsEIZKHR3v+/pfazy7L7ItRF26jYUiMrUhBZEO8ho/ZvbDMumFGID8kzaJqbwQVVCT5RBjSrC24BIKfnj9urwYNIdwciyBCrVMMbOXwgZ48Sm+gpPBxpPL0YmrSLtuZAAAAcwGf1WpCfwAAAwAAHxZ0W4qS0NmDpQT5z67LY1Rm6eU7SqQZcPE56HawDIAIZlcjlNSAKeqLLp76f+HdhG2/DBB7oN7EcNWhyd6oP5NNOSRDJjRhpBQIKDyQ7itOrLlKRX6Qhywx/9ij/E1h73VQYd47/TAAAAELQZvYSahBaJlMFPDP/p4QAAADAAE1kOQAnTKvQAIg9WqUd5YULhxxwbKBck2J5f7DG1pNcLZw9xz3KN0IG4Xgu43gUT7dBqAeYX4+9HOjtMFTToSn4OQsFZM0Rpw9YRseP52GPfPM5wmp6TYFcLbm4kUv58AgfsD//Jx9ljds/QpbuQ/6pZflQ9W2dAh2sUiNeFR1zEzpg2/NCGrlCS+BtGIWmLxoiTrexJMj2RZn1DxFhsGXOHrjfksJNp42Lj2eCOfNBssLbm1cOkEsgqthNyr/9o9snga8+YYJf/C+uSgBWIfE0Rp/ix3d6jE9TJHxjxuCHQEsPtxOcCRWZ0LSQEj6UTkgwWDPOxqBAAAAigGf92pCfwAAAwAAU/ZjENH7s7gBNQRp6KczKzow50jjVWFlxSWyN6CnlZNW7YQvRH9rLsnqzlOWAiswzGetu50kp1Ari6ABDI5eunYJxVhkNDV5/aXPiIZ+dBwn5EwGnD23eNKivW5XmQP0cOweAQylNFuZvPplmgsx7GO6ZqyMffBtKvtLDTkywQAAAQxBm/pJ4QpSZTBSw3/+p4QAAAMAAFHAGDe/DZACZih7pex2jgi7j/2rB9aJqHTiT/Sj7yCcn96jOMMFdFmJBGbIyThAPie7cQOxX5TSblZPA3xkITp9dxA8vS3kxLzfUXMWX/xylEFJZ0LoMuBog6/BiEpJBGlvhwBZPoqcMS21FI+yucq9hiFQYfw9YXYDlElFw/yWl0jUcxJ60edfXGZrnaOhG4h3RK6sCH/4DkBtIOt9jfeTZwKbvkQCpU6onqrEdSEGFNdXdgRnHokzqxrH+a6fkS8dYAQfsQk6czECyT1APMhx1qGJF2Vr1WiO8aVMVjWVMLpt2Tfv0aL1hi+rEvlfK0eXQl9CQcPgAAAApwGeGWpCfwAAAwAAVlFCjYEWuaLXmW88ZIAEsegwvJ/10ILY3RO+02cRmbN2XcWxLa+SjE5Ai9upAgx3JUKcv2I8uGfBBW6EEark4RsT57uIRYQD4npcO4R5XMxh4QEel7/n9F0MBF+/3xIZHrdmwvUKUAu8et8CRWOLzfsFn7qzxUudg/lEEXfljBn2LZO8cKj5dqOhFj1PAb1v8sLbvH9Iuurh2U0hAAABGEGaHEnhDomUwUTDf/6nhAAAAwAAUbETTA+0AB2aAzlfbW5ulA5/udyInRcvv8aQg7rFVu2VHZauGnvnUD8eNK/e3HpFwJb6+VPZkUAAAsBLAtjbubqsiuWEfZg/r3blwO+EyARCV7FZW6fhXQ2hAOINa7YzUXhV44RjhUtCurVf66kxQqkUcl0FnU/8JNpdyAWypz46ehzczyxJTr6AAONwTbthi3vhfluDcqY8tWgB4aL1+9egsTgv3QQ7ijjdH1lG0AKogd9WFT5ai2f63FNkHGPfU1tLQI0qqADmp/v9jD2jPpM+m7l0di5XMmp2JiuTdfapredmXz7C6Y2HkyMYH3nW1MwtAe7VWoG8ajpyeUBNPGrdZ+AAAAC7AZ47akJ/AAADAABWahw+QgADkTzUC1x3ToNDutAXhSsmxziSnk1k/9o9Gj8FMAMEASHwsbZGegpapm+kowwSGO1c+s4/LnGpfaWKfwKrjTLcS8FN1qRMeBlDkmI0ZebCDrIhRPtOBRKLmR4BZJS1RHU6GSwuD17chsDz85u1VRdVt6Tp1VF2T0GCU9rUlrGJOKFuI3VrW30cS3GC6dprT48QC8J5xKHFGncbFHt9ugN4HZ82x4/JYzsxkwAAASJBmj5J4Q8mUwU8N//+p4QAAAMAAFQAF2a+NkAN13+IN9WDycpprfi6n4kG9jWhAaqtfsfQMwq8XANISNfyL0bX9964JNQLzPyOmEVXd3puaFA+cC0FszLg1P4ODMWOh7ToE0PdNcnbl2ox75SuHxUmRN215DwgT9UOBsngqf5HkgIvXCGZVcVs9P7Q9waxny1umdygEVRhsh/b0y5lvS0AIazuaHaSj4Jyw83ZgymIwyNB71Ue6t03AD4Dx/5JI2QeUbhiT00Y4JawPvQh9YZFEIF6snglMhllKFZ6xgu+q4NMtOdJ6zrST1zUHSHjQLH5CrHJ7124k1Ne04d3hDf9n8TnCQ4y4bwrICsMXyvC6K0BwQrIIUTlscSRB0ju/67wsQAAAI4Bnl1qQn8AAAMAAFitMSi98NLHTCz6Dc8AHNrf73we/zafLZQqtibnh6Zzv1OOT5BM2YpQwjat1+xJWjT9D25XtZd9n3B9OFTFSyM45DMBdYTETPa1vGJd6z9zFsenEAnz+ziiSM+AUEFL39rJuNATJpbEC8+XpOXoCR1ofsYe6Iit82vbuBMYUwZshLp/AAABYEGaQknhDyZTAhv//qeEAAADAADdMJ3su8JACWTuTCVuJhDKcpjYkbPolkMRZdgMbCrkAKnHUXzmgbTqWfTobVUAF0qRY2vWXZ8ndzwDKo3OCOMdZ0Nce9r1pKwvpZ93KNW4ywKX1fyETJ9o8oRf/XtkZRfnXnt/aXYRBPkhYB1PvkjmBTG6+JrerUm5PyojsNlqgyfLnpr3oS7fwaEVl4ClctHgOnrE+mt37GFxZvrAHboR7MNEiH9QJ6FzROGdoOPNtlVYYdxdaQYiHYXdYaBjbIYJixziVnhvdnh4wZ1b1HV77FuNYU0Lkjz/+W9KbJtYFjSVZEAIRGSi8W3w5BF/1qtAYwoUfGwhHvts6cS7f8g1atKDQm8CxaU1e0pQ94Sb8L562Z9D3yU6W7TBaE0KhIBU3gCiltqk4ixhNZwF6V/jnLl1aFt3rWzQHNEAMN6Bw90SkMLe9rsZ7o3zXsAAAAC1QZ5gRRE8K/8AAAMAALWpocHleKU6SCJtNyzCk8MAHMB0oUfHlfH9MZUwL8WIhUXFYqzxMGGucQpAvvUVU6HmsRfHGmG4myB+jBmcKahnzaGyOH+0V3BWWkMmBZCBZYjcuooO4vAXfuIuR6Xj6I4gUnJnIIzELGvkoBpj4JzCsEae6RWWeT14JZNrpMXN8pQc5BDzt8/TWJo8ERVo9y215nhuseRnhex35rJMuNp4jL0C+oDouwAAAHQBnp90Qn8AAAMAAOhLhe1iM0HSloAC+utlRQjQ6KQwSvmDv/K3xG7Ln4lh+ITs6sxCOWoWRFgLC0PM/YT4+LXHpkxiA3wfETkVeEx4Ee8Y/b28/huk+BWl1f77J+e9fjHKlPIeDDZQcR+pY4mLG7/Ea8T0wAAAAJMBnoFqQn8AAAMAAOfzPkrwAthkH5roKzzsKcnkyhQ9JWCvmMNAZCTYgsj0WLW7OCitD/T8p/N3IhgG/7P4h/ih0nSk0EMBeLgzox26J8Eimg0uIM9BSj/z3bfWnWEAin1Ghi30nKB/X/tojyg6SHJtl1J2LIuZGshvE1rimnTLeUZPBEy0CnSCq3L8cLYYyxPw3MkAAADyQZqESahBaJlMFPDf/qeEAAADAADdONeS+NkALdPt5bbcakZLl5zXM/dc6xnXZqRStrGSd+eHcdXTAGH+2TEGsi72zg1aimN0pWqzncsWGq0IfXVIbSnooeao8v4/UrauyewwMjhGC8RrIpwgjEWi42pCJ/qLCs6/Ed15h/RH2mc6NkICC+WDy4dV0OJrKV6XEA58Ckib+Q8R6s/uO216zjz32/vtOtHyEofjku77do3BWhI3v6+D9J29Zsf2DSZCM6hIdTBaDfmLEOM0uVUTmQuk/V3O9TfOyzkWMVEgZ382A8gAIeIKspuzm6VaFyntEcAAAACGAZ6jakJ/AAADAADoN2ygQACc20AYkHEWaJFEYiuP/Sv1nkSH+lfTidJd8nxN71Aec8yEsHP7baLRNW3x+iYb5kA9juoxKPy+SCpP+UDXHtwVrRCljAVOc++k2iiCr5oYeeW6OqU0osZ0jx6MPX8JCARqqZu1UUpiUarPrnGN8UZcpvBe/kEAAAApQZqmSeEKUmUwUsN//qeEAAADAAALVvH4ANqajF4zkEOmdP+4ffB5AUEAAACLAZ7FakJ/AAADAADnjgfgngA+1TVy0BIyOmLIgwfZ/gQ7EONwxZoZH0W1mCwVn2y1fwCy0BPggIWv2gXKmuepGNrM8r2lTQAe5Hk1ZZObxSwZxEzAe8pSbq0u2mQjgOML2ZY+sdjSEmb8JUtB5BXGeCNdX3tNjhNZiKvlnCbLI7NuKw0k7BFAUCacwQAAAONBmspJ4Q6JlMCGf/6eEAAAAwADY8J0eiZyh9R/6EI6GACXYqWE9OfsyDEVgc2PmmK4+ZGUvQHCcnP9fFVHp8D3/9Iic9KPklXTwn0rWtUd+k4namUUb2+szIIWcuehVQzEiwIK3rSbCKXVPry85LBUH1x5vb2PM/nA09fpS9gZzgWbty27MTSQVDReLtE+lOrZf2ViFAOu54W3jgppRLPRKMCyBqqAAE1MLUgdEclZM3QgEo6oKmhCVn9MVhhZZ5dn48tKSvQ7vt8/y/zUMncux2frPwE3+Bequauc/oIjQPXukQAAALtBnuhFFTwr/wAAAwAAtdLpjWiAFvGblumSqJ5C1GGZCvkFlM5G44NMZKZxbHfjCUYlT1FWm2cVrXXDmtpwDN/engqNV56ODa8/wS6f125sgOVv1SB8rLJSK2acJzs8YzqiLTWXQl5kq6NxyEdid0HGKpjxqcRUIQEK5xH0LzWJ92IepoC3KywO+WvnT9xE03Pka/RH1XvWvhCsZK8wWyOpIjeZzwztifZafOjNIP1IJAgRPc/5xPJwHbgYAAAAjAGfB3RCfwAAAwAA5+M+aEAAnUC7VsKYyimdm3MKrrQSkXyOCxe7Lg0v4ZC+GrppK/Y6vVGaDeR7swqpHOYakG0pWeIJ2oEQePwBK1MJj83RNIjBorxLl6r9Q9MHuONCebCi/kBljg9eTihC5aVphTrSQLX9kOIoSx+BUFt4xBsAwHQ4ivQWsM/JafvwAAAAgwGfCWpCfwAAAwAA6DbigbMNYPn4b8UAHZEromIBTxD7F8fGzjXMKlKoEcJ9m5zfTw/y5nr0afCDlGm2oZ89bnQL7sakUh27cu/xrBsSI3UUDcz9ryKY96QQe1y41DKesXzeFSNipYpsgW26jRSJk8UiD3cQqf3lOj2SV5LSizR0D7vBAAABM0GbDkmoQWiZTAhf//6MsAAAAwABS+ZcciZ3klW0xf/lw+XwAHaE5hdGTjhStjrW1G/FrBLTxRi3XUzrJWeyufqu/zuDJzW40txiXtt2mOAJO5IRZA6dcCO1MaWyIi+N3VRTX67MSS/FZNqz4t4Sz93OWBaBHyFPttyA5sDiP1XVOR3CfEssz4I1NPFKShcIfFAnHS9h4nbqI/SnzuOzp4huGtK9HqSKS9ttWj6JG03LMnFdjEX8Xtha6vAdofVvhI5aa4cFY8Sml9ejww/qGiLR+ggUVXt8DBmETHtvgl43aJjR8iBZ6FENRO+SguCdSAp8zMxabOEF+ZaqOVyUbe6afLD3iNCanjYWhyunCJ/JxPHUfu4L0tEwWzyCpaBK+0MxnfLdZbJ13DZwIYIpOQ1t4WAAAACsQZ8sRREsK/8AAAMAAK9/+yW5iO4RHGPo4ALiErG/L4pq9Y93U+PaOisk9wQolFwyAbOk3PECKZtl+dZCHf8BUNFs2hSuoyB9ZnV+3xZiIz4byAyxHzfB/JwFfMjODzamhgLkBtT1ra8dFM0nzVgfR5TinoZVpUupGGm9s6pA/dbsvDH4e6TiJF4TrRDjKzer98u/k//5ee8ds1z9mEO1rbDgQ5DHkfXttfGYOAAAAH0Bn0t0Qn8AAAMAAOJsyJkfk/bJm7qSn8GUcgp9oPD2QASyTdf+A35JRSm39CIk8zm5lwrFKGzM7rWpn+Jrw0Uh+py0kAcr6rDpaOCSxq+OIrmTTduBXN6C/NzgKYtmwZXn5C0b4rk7NB6VieQrI7DyOkXmySW39gfBlzo4twAAAI4Bn01qQn8AAAMAAOK8Z0Q8LAsdxcGgA5w4AxC2VqThkFubiPxU3xoUNfveK7QVnYM9ALPSGwnN2OCDJ3OlePTyCTi7zN1fYAxxOxy/Cg+bEZT4LTSLjIuBh4iqeKWXtsY8fpVCZIeOaDCpnsGExqJLFK2C0fjMJM3nARzBXVgixV++eUn8qvGZIye+8Ti3AAAA5UGbT0moQWyZTAhn//6eEAAAAwABPay9iV4AEIn1ttS/8HFdWUnDKuK40FxymAkH38Zz2p4XUfIazDIqZSUITnoSVt718V894lg4/nuOmLISbHaM0IRjCD0eAjtsJ+zlG/Yj91kowcUdOSCbRDc5pNbQ36Yvde9f9p+u/4dNKSvroqmzy5g0XH4AVmJ00Hs6QH9I27/9qQwDCqJFaGQrL8wnWtQ/LrULptZmBOHXSeUUJ52HVsLN21wcjAAAZ3JbPXfAKwHmsRo16wK+CP1oPz+wz3y8cuQ0nUj/U2lPWjr6pg8CCpEAAAD/QZtwSeEKUmUwIZ/+nhAAAAMAAT2tColeABDfOE+zpKxM9Mmpd7ptXMJytQBv7ubSMait4+8kBfxsctGXnwjDIhe5j4eRkY64w8JkpkRBxhjrbwRJlUAMDJNIN+2wnEcZ7PCOWKoH1h0Nb9OzV1NqNBv3vMPOirQdQlK4eyAbbFmB0k9bd55eKmTzdQ+9kspSaJgt2Dyaetf+PoBueMp4AnCWfgj5j6rUnjSHx9l57nS2ue1Y04pm3xc4wpmMR6GPqul0fzIm640s6+LYnFYFnvEI6dSNeeQtP1cvGEauqsKlpW1r9lWAvr17BnVYwpJYR4JyzbMIAm5amrzNO5bgAAAA/0GbkUnhDomUwIZ//p4QAAADAAE/5lz10ZczZmACo6XCx1E4cq4FF1iUxhY6GmxEf9sO40F+56qmqpbnfzIw8bxZ18BMV3FBdf8Y6Ns5B6o257oY78tcq5bfWT3X80peG1Cu/FcqKd9/vPCoIugrKBn96+35JDFHF1sca4BwF8mAj3IEbbjdz5WWw5tpCQe0ccjay//4f2KHNqo/HymJPW/FqjDyrs30Xx1tlhg3RYmBXiD9EHewkRCugX5ONx4Ovcf8MIxFlbK9wOr1r+k13QQ+GB/ICx8zOZ2b54gFcFPkO/5KgZKWSZa3rekjM2xsXGa//oWpUusKcn0rT+1z3wAAALRBm7JJ4Q8mUwIZ//6eEAAAAwABNZAvEGwfgPK88eHBq979CoIrSeK7+sa1NiADLvVuRR7coL4lMGH93KdXnxrhgIKAnPISaF75CZIgTfDbFsQM2lLEDxr46jAnTQl1nYgqC66YUbmbuymGJaHfNG1n/atg6fB2AUf7KBvO5wgbX2vFNDGdGvBGbylB3/8z4OAISDTqpZeXEoiENsYtlEPA6ixWpb9ou/BT64VQACe5E64E6+8AAAErQZvUSeEPJlMFETw3//6nhAAAAwAAT/3hLeB1gqEHB0AGwIZpRcaITeSFO5195QrZBgyziK8ZR/Q2kuJp+38wcqLRFqRTulgBF4g+7C2FvaFHybQuyOlw2DJBf83/j4OpGm/7gNd5GnH62W6T2eGNfooPaWTYjHED6NvZVw7vVXjwEp6m0FSjo+Bvnj6HOAfjAa2GS8ZA+VZkydp3Q88MTNyF04/19y68grsuWtEMvQAa+3Zi55RZ8DGVeRzmhdbOYxX8fA5eWRxY1qvlEXiTT5CwDpM23GgdKxWEMCFggDyt7HXe/Ip1AWuUC9WVP34HTr6XLkBADC6geRfID0tWl287WbnJlR/RuFGRSdy3ea96+EGC/DNV6sgQpLL2phgpGTN/JtPz86IGXYIAAACAAZ/zakJ/AAADAABUG8vAERw1O71MaZ9bdwoxO1nIY3pp/VxcT/LoelDiR6dVBjCTdvdDF0TbWEwlBWrF7Wrcp6EFO7ONvvAti9pPcdYx/X5n19LI1xefIYxkRmV8zuhHJmR2vQ8nB+76lvuQu0W+DKwnO8ty/tQhR/2CpKzmzMkAAAEIQZv2SeEPJlMFPDf//qeEAAADAABLuQfA1CICh3Jd1f6AO1N5r9eXMKoc6wSbRxWuTnacqxd9ZcG053FmlLPA241HVZbZPkFBMg0vV2vQuKDjQ8qgiI5XIQOoqQz4nrPFGzfMWD7hWc9RDzGPrVKmp9NSLD/EMjtEJZDcb+PIYimAaCg5J00G3lLt521cPMftLrza0tA2GatSfmCUpxOv6NZKc/XxCxgODI4GUyJ6kAWl5Ml2GAKfhZiVZUNgYF8QfAwe0S9vv5xve55OxplB5390DmiVir5hlrcwk5CcJvWXGByvSQYagSy8Lra4WT7BcH9n9inzmeq52gxotWbldBtyhe8B2XWTAAAAiwGeFWpCfwAAAwAATqVm4cLc+spxiXYg0ccF+95xPQQscbH/KrxxBQRgUGt2hpDFEAIbO2ZB0hbTgI1hn1tL3VC+lfCNsf41VlemVglgAsCHdbBA5QZbx77TevZ0atvgl1+v00XNtCCSNq+dLNaqrdQ8z3/XimXH+io50+dzS6Sts7EHJTvVA8qOePIAAAFfQZoaSeEPJlMCGf/+nhAAAAMAASUQ7FD3iABNXrwVUhBqrb25NJu18TGTQZBn0mx9DcPaxLBjjM2FOd3jjoN+Uu9zDXjCGVJMdB9fQwaD4yk09d1xsImHJDMAwWmah3Iu0phIbZg35/slO8zic86LNqmlI2BDJ6TMkJiZey1ne/MkAPQzcFyH9z8Czim+2Zj4Ci9K4XpA3PC+4b7+6sdXQF8nkNCwuV99Jk1IZFl5SAx1YHcwJmlnGzBAAcmf202S75zZ0bW3mssGiJuAjxiyTMnyrsJbGRul7xFNCSJlnJsLghB7j4rkbtFMsLg14yCCMCBgytGUIk4bVKIx+7bKdlnr5rklnaF1pYGK3BsPjteY5sOabLRrhFJjJN5d8WYWkgNLqzExGXQspY8ZGZPd5fkByjCP3Su/1kYoxEHY6D1Kf3X/8H5ijOIuAllfjEW0+OCKCRM3VpIaoaA/ClGBAAAAuUGeOEURPCv/AAADAACG1YLvlVTgc4v0rSMQzf0ADrgOmuJb6Q0K/VRT8YGaKQrwP0mN9NuKUkG+T9NML2extPJr+mxNLxsNR/97N/6uLD9ijHrvk5GsZPPgIAEP4icFhts25kBaIDjqPVNK7QXb3iahxKVQR65jnAigcvSBa0SRj8Itcg7fRMuZPW4+LDN5wCRPb3Jpp+09lBcmtJ0itcEou3a8Myd1r4kvLQJHXpjGbm0lvIgYynzXAAAAkAGeV3RCfwAAAwAAUOVlpfmQ0OQYSD9bIjTbPaJhtcWIAHZFb9ZOQB/wVw9qxlyyoUSrZ41F5BMIlU878TdIm5Srm0fGc5DWH695430Ehr/e8COK4WWiZSKW+PjMfNr78qdsSrk7COh9o4QPTuA/sth+1JaS9VLJ/iY8wJr3VfjbfF8BG/thhb6pXa68MwzGYAAAAIkBnllqQn8AAAMAAE+ttF281ZRt2ZsafYanAbUTyYvS9ODeBFegAOORhvdWJ2ADV1TxZjtqNxKhqcAIKOH4vnaLfMbhEJJehkTuzCxxSUyfWAF5qy7cX22Yljg32O84qxGgWXOzbt/lX3sgBTRuf/E7XmN9a8P9+F1LW5ARTtPycl78oaLZD/HP0wAAASRBmlxJqEFomUwU8M/+nhAAAAMAASVFIjfFIEAE7evBaSAi6SBP9AO5vWymTSh7PA+nxG4V3LZgeMrs8aXO7Jz8raSxUHzHHmdeUBsTKakz7wBZc8AoP5iFjDuyb9fKquoEw31Lz/T4Oh/yPZe1bmdFMYvJcprjhIwkpNAKM6CulFwas9U5pJzleGzMkdDlBWPCQ8+BpRHwv3P70Tshvep3hSN3Wgo0owJYg8tCLdMgo1xZ4vjKdkSDq3xrTdVSi0OCxckS4U8pLzWdaTpmEhVTYqXXfR9sx7yuODymPNm5Pfe39x5JSKoM1onyUaBLnkE+1lUA9IfZ//QTLBMcsD6+3F+QD7aXNSUfwT3mLC2QcUSTnG3gvCxAUmJsbESt8e/+u9OZAAAAtAGee2pCfwAAAwAAxDzSS9aUm/qmmtAAcUligXre97j9F9lH9JnX8l8IselbgjQeezxHOdFmYSEvaxOdmD/4hgjvgV82rCeNDfzKlZOblnvNZTucfCiDn9ykg9GnnSrjuyMntb44HBBBpeZyDG8OFUtDFS9dLRvwP4yp2m5S6IrUw2H5MmDYi0bPkIvbV01jM1cHx+MPTET3mzbUUcgKBjrax6iUMW0WkI5U9vwgB/vRr9/ZPQAAANhBmn1J4QpSZTAhn/6eEAAAAwAC2cy6SgcXZWl8WACC5/bqxblaTnNNsde1tgh4pqEhS81ySONUDgVfdBxstK0AxZFgZThpGESuY9KgYUNxrKyo0GLiQLvJQZ0bMjbgG/j4CUGORImOLhc/XyCxSsVPsvBRMfmaANavlNe97+0gt8ZEf+WI9SUjwiGepOeJh3iDfHjooWxOfg9iP60o+rE8LFWB7a235QQSWQwKLV+XEYuYjw5LypJYWCv06leRKnNBzc/4xlKm1fDoCP8vVTYD0NF2jVSLChkAAAEhQZqfSeEOiZTBTRMM//6eEAAAAwAC12xVUJ9DCcgBbxuKdn1XIrt3dp1W5rPNdjF/psPzJWVkO+dJGucEdFb0nbB5Rr58Rm+OXa8hGusncZ4xea/HA5IrtkbUXvjxZdA57NU0MD4tQcvKHs4/P5SPRKVBSftmKZdtyzU8VW43+jW0PATmHvYx0bospL4/Jp0bWcMx+DvQH5t7vj7e8O1sUoNbGdJD/e6TFgu+C+JnkQjIB1Maa7ULBg4M/5E14yflyAtmwhj/rvStWojhJQciSMAf2e54Quzh21CwMcAPWcYGIJOw/WaxVNlYw/pos2WCNESv1atOBUtjDQpOhWQAqBX9UI7LuQoFn0aLOVv1+K2QNsGMMGhjOHzz5s/AaJWuQAAAAKwBnr5qQn8AAAMAAMk7/gGAnrACauVVApTypIUGdzy+Bustx3ftnk2WMTZII22vdaVc4rWbGqkQnkBNwve/tpG4DsWQ1xazGjACVmMdGqQ0KWtHqezGxJVWofTalmWgEVD500F1jbdxS8SDBjW59Peh49+MEvJw3why8TZi4wT/a9w4yQ6dPof+UORFvgR8KF1+F6BbBm7XErf0uEXDJo74UcsMa6PN27Ox/vwXAAAA9kGaoEnhDyZTAhv//qeEAAADAAC5xaT+46nSkAG0jO2lmQnG8EXEd4SsNf4tEm1fF/BJlQbsfldQqHfQ2M9PK2s9HyFzpkcDYbHI7+/Jv16uCUNMoUnnt//rGXRJ2PAugcqZWLjPRFhVmKiR/FQ2C38udzCffv6Owj8wbtHUnUwl8PF1iNngUUk5r42UC/PVPkdLBEoAib3BlgUvuqqCeob0IolgrCuh5Mr5+2dsgospXq+++y8QjYao0wSq69GPcgss4oYUZ5gI1cBU8XLF7RADKgXkxMqRfUqud1Ruh9b6H7jjgxX7xvn8CmGVLR4t2k7CG+DNwQAAAN9BmsJJ4Q8mUwURPDf//qeEAAADAAC+33TPlEnkSFktAAiD3mcr+mMLzOKGnmYc7ipe6TpSGHVjJv9UQ9ipp5+R1PAo6v+g3Gr906sdAj6dbdpT7+VX369FnucIJKb+V6icDLH2pRBXDtL6elcj0rYompoGvzNnPJrWDR5SEk8ogvEbfRLrcmMQDQBk7AnNyjIsaB3+J0r0sY9QZu5xCRamQH5tYodTfATuN6DZJkT1JOAMffHrw55DEwAgU926H65tPDjTO73QdL+/liJOV/Fifz7D0wFl/Ox3fhnDxjsIAAAAmQGe4WpCfwAAAwAAyTZUCBtO4AOcOp1g/qfHkKM9oAbZVsUGAytcRHT3ZLkwgr5oMC6BswydnYToQcKPTBmE3q3yIWuiLWUpq8LecBylLlOiCea1F10skNX3v9CX+Eo97I6MXxslWbkGOAiifxJUqsffPxRYdQhDvuKEnAVVjXNA6RStv1m1zhzM9AQ+0qHANzIdaxVX8n6jGwAAAQ9BmuZJ4Q8mUwIb//6nhAAAAwAAvrs4fK391WWgATt2LH/WXLffT4WzbakvY8E9g/UAbiEEL+XgJjhtMCJtQpSutJv3h7Zw479eQkVlHnvnjHQ70ND9/Yee6PQlfR1D1QFa+0nZgdwG5rBg0R3NKB4WxnQFmdo5jEH/2K5Rllabk07y45GaWo5eZSZ2sygNDryplkK6KlnL8aX/1NwBf3EL/XpszhD2fHoi9Hr6wlnUN4saHEAqLyGJORMmpETFMoudxKKwDORhRlxUipg5Aj86X402dmV9hFDfXO7yGs1LygEA4O2B3hwxGXoTwVueKZ0kvGPiFstJej/XMkBgXxAe4sSoFn12NyGtZqfKY+36AAAAh0GfBEURPCv/AAADAAGIdLsqII3UMBc6AAdPo/Zr3Y9ObqCGx2qfO6J5XEvu8hdlQC2tb4kjChctBhT4arI1/vZ0xLTqb1MkSn5mwQxUnnvdB7JTEM0VlWqpcsXBDIwXZIacgEppzTEyiN4VXobk7fIOba1Aj5vfYG4izAJ0Fv1Qb6pnzDnecQAAAHwBnyN0Qn8AAAMAAMlL46Gy1gA5T4euT94Ma3BQhpTaRHaHc0OkdX8WeN32OK7MsWARsOBZVcr0Uzw6qG6hkGnwreq1DysLCVldKDPx+3JB/ZFddBwTpQLF0vF00krjwhA4GfrDg1tLgekj7aM3kDS8NHacliKzy2JeKDSBAAAAawGfJWpCfwAAAwAB8XjQ6JG/jMyAA4eMUoVzV6Ao92P07TydHQPI/mh0VQg6gfaWgpLg2s3gChhogaPONEvAC/iVs9u10NnBO1dFEwcoWzc99Igy8mUDp+dY1aEU3qs9X1xgZ85FLac0MG2dAAAAY0GbKUmoQWiZTAhn//6eEAAAAwACw+I+mIB6b1TV3Z+kFvAASmiYA1lc6QKgopLorz1OAO4VhXFwBPO7EsKd51gkBoM0FjjM0s2UZ7SDnTC2l0XdFJRQ9wSEAykrH2O2XzUvgQAAAKRBn0dFESwr/wAAAwABiLDNTI6AA7Q7krOj/i1BfHe4dfcm8qCZwZJYn63IhPWfOVIrXMqnDMZ4pa/FGU3g8x05v5Y0ZTiHpn5/d3m9IVzwfExImom29kebqf4QOd9zmBo1khvDxiWJmQBdE+OIHa7uIykp+zhRwjZHVXn5o5sAgEWXkrWen5048U/KZ3RiNpOiSFuUhPVoABDz64HrVJON3JV5wAAAAGgBn2hqQn8AAAMAAMPI87aAB2MTyyJyZv8a/CAppz6BBsyr6sQRD8Cu0oqwGXLGO9GuMP/H1CF61JvQ8WjGU4FoPcMysgZuMgAOHr5OteHRNf7LDqT5QKnDkM1q67eXoNhYV8zz6IjhfgAAANdBm2xJqEFsmUwIX//+jLAAAAMAAu3pKlEM7XL0AEQONKLc6XH6j8d5SSI7MyNMK5WTCEPcn/ebuNlSCNxJSqtzmfRp9GuXuH196RIe/ysruXlElH0hjtGuo8WZZ4BsFNT4EKyuS2ieXO8N02s8G6pt5ks6tQpVvEcRsc6lzEXxVkWmGftU1DOs5CP30OTEehltj2glDyqcQls2b+1mxxiIIFzoAt9CpgDOq5hzz6NY1n1waJxQzRXiaeYOYYcVNXDHz5twWit7dcexztIUk7Sc41wnvJ7foQAAAJdBn4pFFSwr/wAAAwABhCv2BeSdY+jpb73cgYswAIYqZD9g8wM4DlIzFsilOyHXbkZUGjXtbhuGDHvSt26ThY4BGiZA8sAYnNjfmeovVk02nfIgc7LP65WVa7nul8T5bs4enh0GPvF3eDo5RuwNZjw+9b5dbDYknLwInOFhwQTXhd1BD0P3CRfNNNzgfs82UkBLqHn8dpLSAAAAfAGfq2pCfwAAAwAAyVzNcwNDz/52eovGYRX5ceqzxAPrlGWsCCzd2/ErYhwAE7dhNIBu9UJqVLZBkw/ILy8PF+ZxXXjVSt/PD6BxZGaIj1bJxKPyS99sB9PCrelfhs+TszBbmu8jndJQ0X8DzUJ5SGZs2zCjl1CKL7kqqsAAAADMQZutSahBbJlMCF///oywAAADAALcp4kvaJh+hFdKv+PRnTWAFuDITzuRrZ2bunjnV6/lrD0o6Lu8IVXwsWN6Eaq0b0kWPtxw233SlisZRl2beY+luJb0Hk1W64pFBbRvgRtenc0GXg2DxZBzYvauF7rL0p8rDGc+gs1vMPblsRC7nxhK/eiz+6PcAUpzqqnw6f+2qih6wK2b+eCgp/v+GIGq8IQd93u9iR9dMAABeUeo5jonFZ/y8kikUcdTwvSC2q8OnS2Gyfrgoi7DAAAA4UGbzknhClJlMCF//oywAAADAALxzLjyuGE3gAIfonAXI8uvcIHmbDP+5eICiNjg5nN5tzmN9g0Ng/921xRCIY1FSVPpo8ta0Ubt18oRTPRWuazTIq3jU0KcQdjRSK/jVpxmrlbs+scNJd9EaBytREPeEPvUIHQhoejKN2yDOg8XZvAeMTExAhIehSF3i4dfE2ehtK8xm9QIfe37OqN6YVxExKnx4On9PxnazgDe3Z08QLVk8xweM+CF33Y0V6QVNLJxrxJsbHQSaGBZqVZcP8+MVVWWQwesoIRXYW+IvnKQIQAAANxBm+9J4Q6JlMCGf/6eEAAAAwAC1d3QacZAAc0bwWkTXSZX///oeNH2AJf9MMqX2cBSvkTrvpBr5XkUt0MsPOGZ6UBKzPcYZjB1PHxSEfd5Ol91LMqF657+Q0WfjXKLOVm3SwGu8KzGkGnsOr3TTHXM2YlyVTMwyyq9/9fVzqkMhSgTGVQpC2u/QlJjWJ1DoHK1QqQKQLtJ850gxjVRp/49j2JbkmB8tn/OTuEjgTUzJX7eUQlh9qzedvDhR0RQ00UuHKp76SAORI8XoGy6h8a07tuJ4wrYAAAnZ0nBAAAA50GaEEnhDyZTAhn//p4QAAADAALWCuEXrIZ1gn6ztAAvmNdLEeLQ3Ds7+sIR3Fu68h728iIZLz8pDP3BYmXHMEasLY281qPy5hkClL2BFPZ/9cP37l3Ajbo16cio8+3EwwhPOrrUgsLBtGf+sAsqQSe7qzNmrhK/IC192LgByyhFLLkjt838fTSgxi9EY5G2nb7cHhVqb3pHNgS3T2Qva3E7uMEFwG1e7311A+OMDuRSbdExPkIWqv4Eu8Pu/tP0y7sz5hYi275UXyU+/8Gcg6xl6BISAESyxCPqSIFZZfVIFcLr1ZGVgAAAAPBBmjFJ4Q8mUwIZ//6eEAAAAwAC2cy6Sf7unur38fX1qACG+cFo3aCSiGym3nGo6WQ5ieK2Sb/w2DatMSVKurBbT5I+R+pvJ103FeUUUfeqDpbkr+7PT+IdaZOqSmx5UWLcun8VQzx/qMNNpnK4ULem8ytmgC0iyCnsgxXLWOGYg/kiPmZqRGHu1s1Kd6JHvBVXuFW08xXtdsXdfW1s0YB3ELapSAmsZdg4og9S2BACImXKkitVV9TVdTD9lQBQn8EdRNJbjp1oum4Cq0i54l601N/QcfB7ahfGDXbQ13YCGmIir60EUWYmqZsPVt5TeYAAAAD/QZpSSeEPJlMCGf/+nhAAAAMAASUXVoADow2Poz0e/7aan+UJZo1kWIeoajYcQsEXXgu6dPa4arT5KIxDzkgQagCuSGPUAIV33A8HztkY1LPw3jfO0DH/De/zqbOhFljYuNzEo5GGdLrgk/NopqIifo8AIajqlfTvY6EOXhimOtH2IjMknrwGspfdQH/0mOy+RWWkS6oE5UPqeyBcVrsyn+c3UmAAwAk54RbFQfvzVT4ER3aFJd+y80Wty8I6chzNp8vCn96xsUAOudmqpqfxwY0RBoRbKww3faLKO0sGcIteiSbdEK40xRZykko63Zzb5GolCFj4T97wA90090qNAAABCUGac0nhDyZTAhv//qeEAAADAABLR2YMcmPlAWACoETOVuiG5buVcFgk2niyZABeMum1t55TPmBVAUSZhJ1h1bIEuoceCeTxObZROd0h2tFSd9eGqd2BHYN8Jfg9HV18Sj+cKnMcFo4EiQQGTNocBLiVAZ1oQzT41zt/VSKgVOi3JGE9DDB/7CgBONF15i25SHOEV5iG2GRxxS/ZS0/4m4Rv9J0/Nhpti0w2r6Pr1L7OJVGNA+lE1kJjRUO1zsL/ZQx+uw8jsFSHFZx3Qh0HygFJ4e16k85NJGM2/rKAUJsYX5j2WPHO1NPqQAg98iXmzQtMxjSNxmF6cket1JgqZsHMbm2mN4xmWbIAAAFQQZqXSeEPJlMCGf/+nhAAAAMAASVH+gemtHNNmACo6PqTe2o9oMkN58HXilXaHnNqiOG+7ucupG8UqflVRHkZcT2B7eUeQoZ4ZOOGCTfswV3CzVeeu1RF5LQjPu+fShIjv9WEsjMLHySw+FaZdOh7l1J4t3qZK9/34kk00JSsMSN1UqWTGJYxE0ag8C3wsqRSNSLw+H1HU8N3j9sv7wqWPcLwdvQERQX+wlCNO8XxO0XkG+10Fx0qs5rUQGtYlYVv0fqI78W7KP57Ys/3+AcCYypmLHjqeWsw4Jc4O1D6PZUrruQaKCjWPK1WMY+uVc0+ddHQaVT80ttzHQKK+/81G98216NhcHp7lYW7lhsI5iUsB7BMOHmWAliJaL9GgTVwPnVbtY2VONiMXPYxLkcK1NoKRMAL2RlDHipLBzI7Mga4ie/gI3Y6GlT0lkbNdFxgAAAA1EGetUURPCv/AAADAACG1YLOg1O8xyoP0AO5nnG3gBDSt55EAUFaIFtU4cnjj+uez9igCJ8xYkeHhjof95jNpe1qOh6E7JUVk+fvNIHFROnGZu3jv1bmMkG3cGr5pcIgQjXZNLZcaHBCRquKBlH8uwdvKj3BH74en9JCIl6LOHst6KPHvYMC0gPYTTeACHVjcNtdJ+mYi9N1Rsezwh5yisTdfZoZ7Yvk4espctEjUGFzXQI52tmnn81cq+iPmEd7IWRxN3Y1ZxzuTV7v7zRhv//i/xWBAAAAqgGe1HRCfwAAAwAAT7OBsNye07W5NnlfcJPgqQAF8zp1hE3sRwXnWTK0X8aDai+O4057EgclZKxkeYqUP1/3iAvbnCFj0IcVc6AvfT/kP8XcXrAIbOHQml4Nbd5PkgUq8kuCDoo2Mf6bkJKBgEyhy0WmLvM99k1ciMr5E1el2SLno0ezNnCiuKNi7qYG4OHGpwa0IbkaGcxWSTXvm8UJwsnT2TCzsLLvhK+wAAAApAGe1mpCfwAAAwAAT4O/FSX2ATsJYC6XAA7EWTP2jtn1SzMaLtNCTJPkhcYWqH19lVUX+9v5/9SUrNXeBilc9su5DLOf2RTEmbAIn0SUv9gupj+s/iRkMCGiLPyWLErOFfS/CtQjv+es0gdfKCq58v+0zqSGxu1wtFQvWE7REWs9kttl/BU7hsGNj4RpXvfMdMZl3d0ybqiSIfMYZ4Q0yjUdgPBhAAAA30Ga2EmoQWiZTAhv//6nhAAAAwAATbpvaYJIStnQc1v+ADLD8GbY3m/XtdwXO5NVXP1ZU3M48mz1rm8FrV8VgFEWcOtYQRFXY37W1XIHkn5Ztch4cVyAPwNlQof92xNxd878kHJXm/3rLgSr5C47lhKV1svhh1p/3IcEUkjTwMDEpPlstmfjEGMXBFbrd05WyRo5Dou6bN8V4jOdu25YB9o9hVpVCWkFlghUQnb6lgpoLYRqkECrjIEKcFqBMDCAG6p45E//P32AYYmojvZlmB1pMOohObDup7oa74nMuBkAAAEXQZr7SeEKUmUwIb/+p4QAAAMAAE+Zq2LP0yAIk1gUNr9zANzrYjcKWGiSO4+DD7cS8P2T+3ZBz84x14EC0ZQMFdmNc8YFk09RoHg/C982cL88WFXjDIwtxvyHqhvislf+IQAAufNtOQeprE4ScKYBX48SZB64dNl2QEpRmWj/2NF9/azKU0y2PMlCLvMF7tYSa0T6OpWbpI77wMBYhg6e9WUFTqW4xNjgp1yTogsqo0rMn0PC2iAIgMRnlP/JfFqry6F7ESH+BtOvJ/l7G5FozpOxMj04NUDmdDwaLTHvKSu8eGS3aaivBeH2DM1yWK8ofze1bbQjI7AKXLzd5zdRcEJU28iRKhd8Ml88VvrCI7oAv+vG6QPAAAAAkkGfGUU0TCv/AAADAACG+shmKsXysmF4GFUcismSylzkOOsBxWpdZWAV5IAGzxTzeWEOftlYEMBib0RFSR0tXzeSscgAZMqTHYhjVOea38aUAPfeDxoxx4WHt1ndXJUMnsv4neRjqWmcFx5GkmmvH17L/uHVV2Ygh7dBg9EU2Fd1k4dNj0JU1IZysaXkuhXJ+ecHAAAAgAGfOmpCfwAAAwAAVC3Vyq/S0fjN7jZI9kaDACaruSknM5e+Uqx0kx1As7wrtOjEIu3WVCjF5ec3C3d4SMX/BAoKRIn72/uynaaDt8Pcmtq98Z2xQcP3SIgIKkrX2OE11jKS+lvpjrB2fT7etTWxaf67c+epPcLWXM6XnDCGlWCcAAABfEGbP0moQWiZTAhn//6eEAAAAwABPmRA8Yv2gAJ28llxcf9rGYWMx9OWxc0uExUc6BRN6H/ZM5Va7l//1WEsUI/pvCXtK0ZI9KxOQ6EHavJHNPschRrGIT27E543NmZDUuiyQ18MebtUWXj6n+mie7wrBgRvUSDCQdqxNRiRsi0Hr5sOeZmQOyI5yqLJdYPaQu2Wi5r0AAP3K8gbaHZVij24DJMC6EGAKly8uA2GN3liMwsAsJGkTCsLJWjZTmLFa6Onme7EYvfnhSiN+saDCAlvr3v+2L2sxDg0XFG91ilp4Fzgv9c6NlJpuItDB5VvFhbL5JcnpgnWMaYdgKdf2HAPc0xDk3yuwA9rJPs/3WVL/fvu+wsxfZXRECi1EbC6k/E0zU005tQas/JUswA7K/Jzni60CtMmplLsab7Oh/d7CibYfyOm7wi30qV/R/zTnS8NvK6PF2ZCUjAEvqj5mj0BazBxHzyfrS3dP9c2C9+H+oA2niMkd20WZiy3AAAAxEGfXUURLCv/AAADAACHBmj5LoRnIxmBZvinAAvl0Gg0gdZkF5h122i7yHaiWInLINBUbNn3OkhBXqz7/268Rdniie6a7ln+raq+6t2Jfqd+qFk2lFymSTPutbhF7+0/+hq/Sx05pcJYD/tMq7cMPw/l0cfTmAgj7CJgwxsQIc6QeKIKn9APvvO2OrOUYlEoHIMeX9JUG97dX6QGnAcG1kqHJxsmhedVNI9RiQc2osuLtFuCTZt5M9SbBDBPuPXQ9f8/rcEAAACJAZ98dEJ/AAADAABVZmhqZ/N9UYcEXes9blFgYFazWAAgA5orfRmX6BUgK6D2vFIh1UgeNR1TjR1L7oCSFP5vzwSolRS/oIFAlj0+g/YpdMFh1vcx2hf6/r5fyExteMCjn77sU4OvNAydiaj4UVab+we41P9BNpqa7TJV+zfdv6scFK966uE2AHwAAACxAZ9+akJ/AAADAABWBMUHWIS9eAEHTD0H8/xrfkRAgTPZnlnheAp+wazqzdHS+r6ddJ9dKvOMZTQRn5cqG69UoPbKlgvOZnOjYFfePRbZYoi6ZFgFEcHZsQTRDx5UWCH/itte7xRt7SChznR95h3bgjA92wIz9FjeAgbToQGwUPuQYzsF8g1uj0ET2TvNxZo7V3RBq3mFUveic2/P7LnzcviIeiv7/y1QqQxIAcB4g0HwAAABIEGbYEmoQWyZTAhv//6nhAAAAwAAUf97D1c5kgA5y4kuKp60k6qtf3lHFE4eEJrIaCozDtyeGpzknEYMZpZl9dmfs/wbu1f8hcQFcxJMpNR0a9cmmLFu8ZKhjR4w1ebOWf9ZDp7CVs7AD6Q+2vc8SI3QffbLtEQnkg2ztfGdF62/ms6JdmuazdZKCAWxStF1jkSbBy3aHqR9sR1QkWLbGn77CuenArHvhu7bkWI35Tt0/iI69c+t4eaHJpQtNeQvK7r68IUuagx4xQrL5CtfRTm5cxNjghdAQJzrBUybBy35VLxq/VXjV2yXYk4ywBRoW4JZiDb1/kaV/8zi5HCn4+60kAcxyjLFwsRbJoySABhSswzYDW0vZQOCEYIjIPLmqQAAAWJBm4RJ4QpSZTAhv/6nhAAAAwAA3LqDVwlsgCs1Nb/MbrvK06403jQZ8B8/pb53n3cZQBs8s3ByVMr+DK1X84OuCLw9RemLc4tmdOmCgr36BbMGBsm93B7r8GWopYw+rqqyxqf1nXg697XNnzLFhQvj8BCbrGDnK7MZ38Luvc6oqizr+rPSIY+G3V46QWxdq6p0E+GuMrTQAdVcStAZFo6FdZ+Kt++cojbTWnjQ6j65et7yR4fgCFiT8vdTTtmA0JiWXTltAd5J91PmHNd1ppsQMyjRcmCHNTKgJYT9LvJslhPsWWqyH/vPFHf+xVDnAdLXADvIJPHbPeFQy3FmymI+aX15jRo7/eAx+LyQSJOHW/6KdpxXXbIORcualnHYS0vFXYfDMmT44VlM7EQPaDm06gMFfyFuiH0d4oMAEfiKrPup2sdFX9ZtGD2Yoyj0FZHlMx+91qzqE/ocb04WSc1l3SAAAAC+QZ+iRTRMK/8AAAMAALP/+sqJssx39XdzjQBe4AIfnyVECH7qIsf8JPcygNQ8U/2rjl7jffGWO8F1rX3XsMVLF9CTeCu3BUkERMW2qTHav661X3RNry8K5yf2VOjy2pIHy+PN6ZMLmlherHbWo9vPFD4WxvJRsWkzTCGK0wB7a89gW8xWBsWs3+YMwSgNFuzF7137XPW7Kq4Kj+5paEf8gUApqgofecmbz/pLmt5eRMF81Bhf8xgcHiTx74svwQAAAJwBn8F0Qn8AAAMAAFiM+9VKaKK/nd/p8hIAHYlRXoXVhefl42BJV6vAwbsSt48UEn/zwu0o3vY+ITWp9b/FaTONGlWZG6MxSHW0vTr9QfJxy0CTuQdklwsQENIJFvom0o5YSTI8EuNjbunaODv0tEzv5H5ZFKHWYvs0Pv5uhEBAZFgxVQlGNeEyVU8kum5kUB7siJiuYwkpXjKmT38AAACbAZ/DakJ/AAADAADlk7jh380SpHkOQAOxiV1dvldfJjrD+42wyTjhm4PTqTEWyo/xyGAj+4lQAFxlAkB3Jl4Ei+DeYJMMDyBxt/k23GuxItvVRVE/0Yw6/MBmX5zb74tSkVe/TeoxaSkXBJ0zEXbgxkA5HHVZUd8a5gUd+Ry1z2aQ5HmfWS4TDTVyMZR5KhGtHzDhywnvEkNHDbEAAAENQZvISahBaJlMCGf//p4QAAADAANeK6P6zHSf7yRJqCazoAgeVlzcaT+ZAtpphdWRiQIEwl9B17m6Z+5La66LFjinXlMxukR6Yb0ya8tlGnCjAv4X/e7s1CH2NJ2KkLPboqwacQnRIomcbBq4dfEIgVd0TcQc/9uq2t6DyTApqXqmGvKatw6PdX3fJi+6OLx8MqeddSYPHx1OIdU00LRkipuyypF9DFpjBZ++RH28s95omlxRvlYd95tEHKrHV6JEOq9iS42aEYIGboAkedafZdC9OOpTX3afEL3p3/bqBYOE/YOBVuRmVJrwqUGlAOzoi7E+rNfDH74/wGT+Z1qCewMmjD9wtc5OBJLS8mEAAACmQZ/mRREsK/8AAAMAALXRmI50QAhTmCQPrmzz3YxZhyTfZo7GaCn29RcbVmcfMhZR5AAecKoHpf49ps+DM3HzQD5noUPsC5Ez7G6j9Q2Zfn+E5VlDzkFfi0l/ocGFCIHR7ehXPk9MAe3n84TA3kQnairZIV161cMDl8xvNND5ioXXd+rYh1r/55LVD4/DDGpWv7DqNaid4/KsBrc7QxF2OE7zHfV2YQAAAIUBngV0Qn8AAAMAAOfizwxAAJz2vXsomIuEjTGbObUXBFtjAqF+7IRSST8vCq4dJ1a4li2q3cTe3PI5iKmMuZHklyEH11PLEmWkdb+4/V4/yQPzZ7hdP15fJH0/LWamcFAp8lGl/JEvIdH0NuYwUxUP7ukYu19mFqhqNUszSRbZo/kVP3ITAAAAfQGeB2pCfwAAAwAA5/T1sk93T8CUkf9QmK5pvVEAIFkeEdktCKELoncMj5NrvgqPesd/98dYnvOhrmpsrUqf6HuydLIZf+ccodTJXaOE8oL1lE7czDcsI/9KMi3feMgj2lHOth50F2qwVEfnWgZsohB30OamTRGmh8Q8SoSwAAAAZkGaCkmoQWyZTBRMN//+p4QAAAMAAFP3w6ABM8L4BuAmkv9tETUHumC+51SE2J/a7Z4ZQTSSD4H/RHfbXVbsggIAdwT4YahZ2feDYajT6/IXszKW5kyKY/GRY0y2Xax97FuEL246cAAAAG4BnilqQn8AAAMAAOeOKTRgAL1zqxUG/MVbduiWNvnQXCaVEPT01lwwiMui20khNYebvr4jKfHYBKQ9KKbwmwy5/u5/hlFndJdskKVKq6t+ALxHtI049Gi/UkXLLsj8iCUHS3OWsrhoaUM6nPzzuQAAAIRBmixJ4QpSZTBSw3/+p4QAAAMAAN0T1+IRWSAE1e8zle+AbY7PTWej4fZdPQk4bK20WoxNpIFJOsiupq+oaJjiiib+OjoNni9RTR8xXHz3EPp5Bz44n0vdh8MJEswhKW2KuU4DZ5wV5FGN52Ab8rh0TndqtCC+n7znjWghfkqgSmG76kAAAAB7AZ5LakJ/AAADAADlMC8KPjAGiZsWNvMwNKogAbKXGvy392bk8IMBdr3d8wHASrXC4gMxH8k0Nr3Dy6RfDc2Z9vR2R8u/gTilFSVoZlvP/7ai+OqKiQcPGYGMjsxuNiUT1eb4uLJwP1a87zbG8pvNtr8qfTqdGnBIgYV4AAABBEGaUEnhDomUwIZ//p4QAAADAANjwnR6JnLsZv0UwCzCpAC27Qmr7oWwKjljz0toPaqpNYagMUjOhXZdCSFFqi8kjZ5wRjiX3RKRVqbTHsxjn178ct17PlvM9kZjhNSzWRxli2oIMOyityuL7h9uKeYQG0sb1byaUzu1JcnFibrnkfXkx9ZsouPUlE0pMw7kQxKggFaLxNQHSD+0NRSnV2VpUy9ltk6QtOACcoOOFW0yPsb83M2+tRkyfq7+TMjXh1bdHP73E0kEKSF/mHyuM6gfnH0haWw+01dDIAgxSEBDyKXYkIM07dpCAl+L5LYw6DIE474ZBLaJGuS/VvfsoRs2v3WBAAAAvkGebkUVPCv/AAADAAC18Hi0jWwAXQNUOyD/s1TWfghSJDDMypRWP5OiznayyH9e3dpQIW7pnXyOSJEL3lRKA3ouYs6pJ3P3cnf0nAbqW+JAqIg//xRWkOMCPfQOCyM5ApxDfle6WXNRvP9heRApmvJ+WV1YgjZbuynDgbKCSgXUFE8VKEYwmKIt/PenKJWIYBWoaqFLaikq5iNnJ6vpMcd+0I3zog+12SZIaXw/4t8ws1XteL3xuL6PV7N/T38AAACSAZ6NdEJ/AAADAADnwsXhR/VAAJ0xiLKD501BeT72WHaR9Iam0pnmHZhtZfd6KAZQ0TQPac7yeHjL0W3m+z5O4cOJfcDXfnHW1XLhPBxcygMHxK6URyRK3J6yBmGH6rz3kbtJDQwwraL+FLPm1miEesyXuVLz90SfILsDupK1guMs7WBpM1cyC9w6jqYf/qKjyhkAAACeAZ6PakJ/AAADAADleQAAT1ElEb6jIH6SlqiOqcQ4RGfLscHLu7gGs9Xp4XdectPiwjBEzUGuBIbHRCAZroSZd2hTKlvD/Wl1z1BW/WhTLmQB6PrJ6zp5611ppi0xrM2tnZ6xWhH+ZnkOzMF2NiOV7WaW8GIf9iioOAp9KEmU35yBRqztXS3Dom1pJyoggutTWJ+v4WoojzCgbspqfu8AAAEEQZqSSahBaJlMFPDP/p4QAAADAANjwnSSkbTcShpEJZqFaAAaN6AdUzGB+bSSCyoD8+iDk6XjkwO7fnSa7UEFOx5RHHwtm98YjN9a6RBVqn6FdEM6Wq5nmlg6otkh+ELRV8Vw2da5X9NZZMiGETnUR7Pvq9radpnbJ2inFf+6Vm/Mu7Ngi5oh5KWbR83Ra/OytZtTmJVjjsitJO30C3Ae0TSLKkAdGs7xsSreFAwdfVG5i8sTXotEklhnz6xIVWlJMCopMMaTCxUos18rsvBNVSNHj/ku0RUJj8VTyyoiub+6X8Ic9QQmW6bpkTQgHx0C6MIRLHzS3ma0pVksw6A70oGud8AAAACMAZ6xakJ/AAADAADoBRqsuyN2/lGqYaAB2RK6Jiil9BiHp9mamhtWYLrlngG32asJd6muvmx/eYJ//ocY3dRlD9aoaH0jothy6NZfEU9NZi/WA4gr7kcl0maym91/F0XO0YS/r6K9bHlTWrVBMtF5jEK8XY0ao1ftftRVTSU0ebBFOXJn9OB8S6Z8H/sAAAD5QZq0SeEKUmUwUsL//oywAAADAAFC3x8F3P0AGxiox6lia8ZiHQwY45VWmvCFt7GmcLl0qdJhZ6WgnT5hNeOm0BKR1iWyVXjnWoBiUTAt+mBsN5fSBM8dZElil63zCrj8LA9iv84+b95PptrSrBC2rvsisouxJGIluQuyJ8+9wBeQaNi4afh/HtkE4EX77YVmjAQYg6TZ5XM/7T9LiluxeZf5VbZkfBfBK1kn0Ewmyp/wappa1XHicsKRmDSdG0AMemX9xZ4RdxNxnLRK+1p3wPnLYYkc1A6JMsWGd53ir2rnheW6S2CHQkPnFNCeEkry5VncfBH4rSKuAAAAngGe02pCfwAAAwAAV6Vko1BeiP/UIAOs+PEOTIP6eyy/E3bOaUXbHVb9ab2esePEShMm4CIEdiiu4Je1lnw/Aalh7AV2eAw8OAb/6AKasLM9ovArbGbazoYZ24cDdY7r61oEtrncfzwyEuJcpVpbU/vc7F/PieuqcCJlUGKylbZdzuNgeh3Szo9KpcquaHkS+/lYBWu2XQwoEc7/lgqYAAAA4kGa1UnhDomUwIZ//p4QAAADAAE/5lz29x4K8v+ABohrOeG5pJY0qOE4bHsS+nt75Oy1Xeenisq35gAZSHk/IcG3zucIU59t+U7yjn5EyCVZb8pAhNO307EkOOfcwJu6VQpCxLAckSznfn4VUTqCg9ZvEJtvX5H7U00c+9AjvKeqJqSy21Grj034/XiE2K3EB04hvo//JBWCNtUwXmOCDI7neNJe2e4sM3yytFxFN7GjgMliHLLtSDXulc0Ep/QOEjUY2HLffUjKCFxyW+ZfNwMZrmAxkKbvms5jZX4/2xldh78AAADUQZr2SeEPJlMCGf/+nhAAAAMAATWQQNyrGODYADRsd+rXoVgJjMMeotFtbEZimQ2a2ppqoeIsotgtKgCgpc/aHqj6+y/r/FvFw18jA92h3p8NMfFdaZ/BLR3yPLZmhvH1O3Tm7ph2WvzIY9aEM4uf8A7oLbO902EX8M1pI1wdA20zGG4MiY5LabK0XzYs0KHPinnxP8jVYgJfcGtyuzBvSkej6mP2nXpGLEV6LCumlMqE1dCj59PN/4LW+rwFYHhel8wD8MP/4wynEN+FZeuv7h81nnkAAAC+QZsXSeEPJlMCGf/+nhAAAAMAATb4sNftDUIDuvTJCvB7gA1X8Fox+yp7wnkI/IT6tOYkya7bzOK8+cPQM4gtnPkybbva/YVkjzmw6NDK3DCMSI22Qx+xZW5NyCCWhCAHmcT81KADeL5EddU8iZJkz6NTxHHFQ4CC/DsX/K9xO8YCpg6AlTNdvc6l9Oc6YOD/i8xrfT8+FSqRY2FU0OLAX8pBGO1oTLoAyXxedCttErwfPmq070TqAKaiC5yvSwAAAMtBmzhJ4Q8mUwIZ//6eEAAAAwAAdDaPu88Lvt8AH121kFHvBKqUTf+HtwbyxYFZs1YhyYgzdy08Cy4qba3G1gLMtDZyCm9xmO9ig9BB8Knp/CBcD4I7bZ+K7D2t+P5hzUnuLg2dLSAedScouFOsEHKMBVf46ChXfRp1cmh+HKbGLpyS4dG3z2DrxRv4M4b0Rh83vqlwmcc02mj/FFvnXdIndoiKmUVCxMakYQ4njxLuVkxWuIuUgAj3mUik2hT9RJvN3Kfjn9f45QbVoQAAAQtBm1pJ4Q8mUwURPDP//p4QAAADAAEm48+BoIgMb14LSTrDKwRLNzBSbs9brVXbejID9mCC7aLDt9camBw6k1fjpn38nj5MTRxYJvbwv/6seOy3999/ey/2wo2PomDjtHikaVmIGqbCNoodeJQBq4zg5ZkSDtums4/vKY2kJ/xDgqPvFaaUBdYJuvmu/pwg1FCJTXoWyj/9qmw54uTZ9Wkas7c1vXaPqETQjbIIQ70ejFln+ayfgEK0DsC4yiaCy98SpAgcstnDPA6e5L1jBs1soz+KVHrNCdHm6txpqftOOOldUzwDPdenjx6kWwxO4dj+hD1EgrUfP91QQ3WP/25NwuQblbVGijykV8AAAACXAZ95akJ/AAADAABUGU2VwuaAEYFKqsFGXlMWkkV44KU1zeoUncmEQ2+nXSzZNr5UIHQ7jDyzJK19p6aus+dqpC6Kc2aXaYrFkvJf9hCbF1didC121fdSEKvGkLm5nWANMbwEjKbRjIn6/TPxhW+ULd0t5ZTrCaYEKHxiJtaK95PrX2/bzfBZyEDhnvtWAZtWzP53wFb7twAAAQ9Bm3xJ4Q8mUwU8M//+nhAAAAMAAS7pvOsfJmICH1jcPAAdaNqyoHKvfSC5tGmqvJtmDOvGNmFfs3f8vPh+jMjbwY7XIN3NShzmkjig4TrQxprzcZtxai5msBjLygJncvMdgti++mPPI7qHgSZ+dWd+++lWOYk7n9SprODWj7s+FdOy5SdmWjYC0iqTmzRdK4QwVP5b6nD2o7Go7flSiwUx0WGS4Lm+7PUCRb9lc3TqWRRMIm4xNsOtYIS1z9JQgZ1wzIi8qWAf2DANgKewrZcf7eN3H2LCd6SPtFKcZd/QNB+l5C4F74uFMfoKiYG605vYQvQ/m8I5/H7coT293TPBlcQ2Bi7f6tA8hYrjqvXgAAAAlwGfm2pCfwAAAwAAUfzLhUZJMHK5hKhEAEQeW/WF1arV2Pcnbt65LBTTZwLPk9h21pAcywayVhdKFreUHiH1W1DZdntcUsZSXMUFhBzAUVQ5QF8vwiiIvtqP54vQjoh4/BUxBi3+AJ5/8kllCl6BKWfAfCgqGBC9FXTWPoyb3Cs9IrV9+VPT3baDNixq3ZkFH4hKDyaAurEAAAFFQZufSeEPJlMCGf/+nhAAAAMAASUUlwhwodEV0AA7Q+4Wi1eowYTx4Hz9bKWd8cyHx0Py1eWHuUHe6dPUiG9RvNeC+eCo7C8LlaJTYbDXqsZjz4ulc6fNMKjoGTMlWBUXk1rWFk+B/QZxHTNdvdnZ+knL8oiOYCvjUkxHNBNlvxYTQ3IqP5teAuHzSj9xhB2waWjnEhpemSOp3kbza6plsaD0mWkiEDmxcTXCMVCPh4MxugJad6mbUn4UALpZrPw+6V3grdfTpLPSgE/vSI//fU+QVqTomcUW2PaHcSf9tOAg0IOvDHxi8rBtB9ypatb2KRYBjJ7CXIJkiCbj9QkMjJ2WSGkS/Mhk+l0WDcVEXFLJSJpUo9QA0Elo6O97PzxpHXdiBaHMRV6McASpDXuCOldMUm2LOGCtY/9Hk5c8AbzMlGBIOQAAAMNBn71FETwr/wAAAwAAhtWC6nFyXwdi0xw8NDFtAA6vOGSd5vAUgzxRitewyKXcn5w50MI4Q5ujKh/ZRKBY0Rxusw2CpzXv3j3ejfINDjiOcacySbc2xrn1sYCBiAM3hGHMUNzQZwIuLhxzhguzA++goWzxsMmK9x/2H0+UmNdumsmZeAzVV2rT//TzLeOA8m+s+2C9vewhZyeuspzc9PJqgHc3lczR8UoSk0OOLgJbayAJA6Msiz5crhHw6//+AuPhKrgAAACkAZ/eakJ/AAADAABPXOLxpxb51bOglAALx08E9C+97iXMA5SoUxHbliguh43PJYO6+krHvCbV6EvSx0V+TJxiQQ9ekG+mRFKdRGWnBPuxEXUUVXHchSk5m8BczHoKlmB/jtqcaTJ6ajr3aeOus3N0ySaBnfJ1Qpy2xD8Rkm1QrBb1eMrn1Ec4rRQSQoLb1xwnPs1+1zDFpSl3Vdcttpxais/p6cAAAAExQZvBSahBaJlMFPDP/p4QAAADAALVXPVwv0ABc+60Kl5t4ubXZwpp3gTeAGab+BuI2o8gem80oa50lrGaFHkLdWO5811SEeP5TRgYraorNyXjotESXVZUf6X8lsI3lsbr8iZ1/dxaNxvsn/psPUfTlf9DYglwb3lDExZaCcmydrkXqe0DEhUs/xMMX+O+2aLaQqiwm64j900Gt0TgteIG7r4iceKIdAMm7RauqQgnO1Cf/P8Su3DWHJReoYreBcgPnalAazO6bqJHF0+elFUTeJCPbdQliHJSCo4aTCAGyrsohXcwA6zYF1NMezmrUi2tPo9AtVtzH74Svh7MfbXZvfv3xftzXA1cXMyKbM/pAtIbqonNeFxfFJa8IyFSHN2ccw8xCIkh/5cVPl2NneXw6acAAACYAZ/gakJ/AAADAADEXx59T5Q59H/VwmLNd+l8sADqdUm9fzrsuE/uh9aVTqpN0fND6fUL7+9ITTs5OAZDGR0fqalKnG/98/ljXjdBCp7j8wi5pECCydYxeRjMbsh2hCZJ5XF2gLcbZ9T+lYcnirK5xzEaFGBn6G5ySiakMZOghZGI+2CUjKK6O5SSBsh8az/funnK4xtXfh4AAAEJQZviSeEKUmUwIZ/+nhAAAAMAAtVY3oh0ABD4cCW95Khru798z5/sURnOOwabMjybSI4pWm3ETjT0aLlm/8xr294esWYp36QfYx/0gY9MsLhSBh14+qlRn02EadsQOEM/F7IAy+QPsKxV80mHfvW87fu2fYglc7bAfdripssvXuLRP3ae6uRflt1WnGTFU3HQezZBQhw+5iREIznqIM6uRumcw9LFpEhxkawcftarQuLEqoOhsGLclLKphjdSCf6l48NncSUtEAwzgtPrI4zRO2lCs1HPG2FnnS9bxeWoqEKQovYljg/dfL+2pQsceHShAFV2939m1eYZqrR1UhyVlMgvm/BxPXP6WQAAAPpBmgNJ4Q6JlMCGf/6eEAAAAwAC52BDoAG19GeZEowBMi2oSlKi9GSPuj6aCE1Poc6SWNRs0AsNjBqapAtVre1q6srAWsQiRgkl/nC33NWZO8PSEj3GJPrjb1uiU11jMrk1vqcrPcFZ4JcVwthpVuwW5P5bcFOR8kQAYcqDTHlkvbAcWxWZKg01UUQ6XCzTiX/ry2J7qYEr1zESogmzuySWAjkth6QXslbXwgwktMJACCfgTF0L7QxJGsXP5ZJc76m220D79+4oJV97TL7Z10C6ki83fw8ynUy8tSYqqy9qpzNERJSd3hLUXGo7T/IQIbJml/S+TSAEVxpmAAAA7UGaJ0nhDyZTAhX//jhAAAADAAtX1MaENJN0XOnP4AOYYjmZeVT3hllaCjQcvi0xhzr0CBvnrzyFeBl2qw7OUg53FaUYsZOCxlLlofHg9G5OZUMDk6/OJzLRyUzbVPrlUixK7Zu1ADbObmEX7Fi3BltjQyXhiHqgI8FOkkTW3+SMyXVBUMF74mrlTAWYwB9RZoYyVhbOY1qxo+q57oh4KCR/4jyUFra6jAv3gXYgrwarD04HdR5xfc5/5MKWn0R1HJh0mEAN6he2oJE0VLOUuEn3sZGtGJPksP1E+Jkmstcd1chP9cjDln6BpJw6QQAAALNBnkVFETwr/wAAAwABknS5tkCFT/iV7KCrtnnFaAAaDkdbporM7HSzdaq284mMkzWwmFYP/EeCJMsYPPxhLPuu5G1kvcerDcCnM05+qX41pNciXZ85PlpWfdQC/0OjtcOb4JIj8l8ZpVGp6fwqnZdNB8LQOkHJaUMP/EPxVJWDPpbYfnMWeoFHMlQ22GmuoBM3SjWoHpRnY5PT6+AUTcx0mD5h/8nppqZGpQnloMNIsh6VGwAAAHABnmR0Qn8AAAMAAMk6sDCKK38zWiBLH810yakkRNygAfiqeIjsjH4lQT6uZYXPn/bYxllVC0vaNPbY8JgATmCmPn6o6gllu//4CBCXjJl1jy1sLKU2x3GsOk4gRFRfd3/HCt9wLZMu8dW9qAb0fXfBAAAAkAGeZmpCfwAAAwAB/HjOvUsllmnkABfM6eEpyvEFrD/EXPtNaptQ242gsyBWj7o4H1fPcKQ4PDLr1Dd0hd+mVvTVYQ5bO+0Pg/wWEwm4cW/2C+8hBpVremh2tEmVWAIxbihbn4dvoHAT7QwbdDzC4v7JIpA//2XLatSgEpWMiioippeg/1llyXMTYXejcTovgQAAAHZBmmhJqEFomUwIT//98QAAAwAAGxFkh0JHABzh1OsOdKWJ/PeZ95Ma4Vjr0ilRCjO/qu8yXV+rmdQ5xl0cwIHv5zD/AKwJQcjE0Ex7tWEof9I8wz64VnjqOpeMpUPqtIC+HegKqVjD/ZWkR2eScDHquSfJ1PrAAAAL721vb3YAAABsbXZoZAAAAAAAAAAAAAAAAAAAA+gAABosAAEAAAEAAAAAAAAAAAAAAAABAAAAAAAAAAAAAAAAAAAAAQAAAAAAAAAAAAAAAAAAQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAIAAAsZdHJhawAAAFx0a2hkAAAAAwAAAAAAAAAAAAAAAQAAAAAAABosAAAAAAAAAAAAAAAAAAAAAAABAAAAAAAAAAAAAAAAAAAAAQAAAAAAAAAAAAAAAAAAQAAAAAJYAAABkAAAAAAAJGVkdHMAAAAcZWxzdAAAAAAAAAABAAAaLAAABAAAAQAAAAAKkW1kaWEAAAAgbWRoZAAAAAAAAAAAAAAAAAAAPAAAAZIAVcQAAAAAAC1oZGxyAAAAAAAAAAB2aWRlAAAAAAAAAAAAAAAAVmlkZW9IYW5kbGVyAAAACjxtaW5mAAAAFHZtaGQAAAABAAAAAAAAAAAAAAAkZGluZgAAABxkcmVmAAAAAAAAAAEAAAAMdXJsIAAAAAEAAAn8c3RibAAAAJhzdHNkAAAAAAAAAAEAAACIYXZjMQAAAAAAAAABAAAAAAAAAAAAAAAAAAAAAAJYAZAASAAAAEgAAAAAAAAAAQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABj//wAAADJhdmNDAWQAHv/hABlnZAAerNlAmDPl4QAAAwABAAADADwPFi2WAQAGaOvjyyLAAAAAGHN0dHMAAAAAAAAAAQAAAMkAAAIAAAAAFHN0c3MAAAAAAAAAAQAAAAEAAAXIY3R0cwAAAAAAAAC3AAAAAQAABAAAAAABAAAGAAAAAAEAAAIAAAAAAQAACgAAAAABAAAEAAAAAAEAAAAAAAAAAQAAAgAAAAABAAAKAAAAAAEAAAQAAAAAAQAAAAAAAAABAAACAAAAAAEAAAoAAAAAAQAABAAAAAABAAAAAAAAAAEAAAIAAAAAAQAACgAAAAABAAAEAAAAAAEAAAAAAAAAAQAAAgAAAAABAAAGAAAAAAEAAAIAAAAAAQAACgAAAAABAAAEAAAAAAEAAAAAAAAAAQAAAgAAAAABAAAGAAAAAAEAAAIAAAAAAQAACgAAAAABAAAEAAAAAAEAAAAAAAAAAQAAAgAAAAABAAAGAAAAAAEAAAIAAAAAAQAABgAAAAABAAACAAAAAAEAAAoAAAAAAQAABAAAAAABAAAAAAAAAAEAAAIAAAAAAQAACgAAAAABAAAEAAAAAAEAAAAAAAAAAQAAAgAAAAABAAAEAAAAAAEAAAoAAAAAAQAABAAAAAABAAAAAAAAAAEAAAIAAAAAAQAABAAAAAABAAAGAAAAAAEAAAIAAAAAAQAACgAAAAABAAAEAAAAAAEAAAAAAAAAAQAAAgAAAAABAAAKAAAAAAEAAAQAAAAAAQAAAAAAAAABAAACAAAAAAEAAAoAAAAAAQAABAAAAAABAAAAAAAAAAEAAAIAAAAAAQAABgAAAAABAAACAAAAAAEAAAoAAAAAAQAABAAAAAABAAAAAAAAAAEAAAIAAAAAAQAABAAAAAABAAAKAAAAAAEAAAQAAAAAAQAAAAAAAAABAAACAAAAAAEAAAoAAAAAAQAABAAAAAABAAAAAAAAAAEAAAIAAAAAAQAACgAAAAABAAAEAAAAAAEAAAAAAAAAAQAAAgAAAAABAAAGAAAAAAEAAAIAAAAAAQAACAAAAAACAAACAAAAAAEAAAYAAAAAAQAAAgAAAAABAAAGAAAAAAEAAAIAAAAAAQAABgAAAAABAAACAAAAAAEAAAYAAAAAAQAAAgAAAAABAAAKAAAAAAEAAAQAAAAAAQAAAAAAAAABAAACAAAAAAEAAAYAAAAAAQAAAgAAAAABAAAGAAAAAAEAAAIAAAAAAQAACgAAAAABAAAEAAAAAAEAAAAAAAAAAQAAAgAAAAABAAAKAAAAAAEAAAQAAAAAAQAAAAAAAAABAAACAAAAAAQAAAQAAAAAAQAABgAAAAABAAACAAAAAAEAAAYAAAAAAQAAAgAAAAABAAAKAAAAAAEAAAQAAAAAAQAAAAAAAAABAAACAAAAAAEAAAYAAAAAAQAAAgAAAAABAAAEAAAAAAEAAAYAAAAAAQAAAgAAAAABAAAEAAAAAAEAAAYAAAAAAQAAAgAAAAABAAAKAAAAAAEAAAQAAAAAAQAAAAAAAAABAAACAAAAAAEAAAgAAAAAAgAAAgAAAAABAAAIAAAAAAIAAAIAAAAABwAABAAAAAABAAAKAAAAAAEAAAQAAAAAAQAAAAAAAAABAAACAAAAAAEAAAQAAAAAAQAACAAAAAACAAACAAAAAAEAAAoAAAAAAQAABAAAAAABAAAAAAAAAAEAAAIAAAAAAQAABAAAAAABAAAKAAAAAAEAAAQAAAAAAQAAAAAAAAABAAACAAAAAAEAAAoAAAAAAQAABAAAAAABAAAAAAAAAAEAAAIAAAAAAQAABgAAAAABAAACAAAAAAEAAAYAAAAAAQAAAgAAAAABAAAKAAAAAAEAAAQAAAAAAQAAAAAAAAABAAACAAAAAAEAAAYAAAAAAQAAAgAAAAABAAAGAAAAAAEAAAIAAAAABAAABAAAAAABAAAGAAAAAAEAAAIAAAAAAQAABgAAAAABAAACAAAAAAEAAAgAAAAAAgAAAgAAAAABAAAGAAAAAAEAAAIAAAAAAgAABAAAAAABAAAKAAAAAAEAAAQAAAAAAQAAAAAAAAABAAACAAAAAAEAAAQAAAAAHHN0c2MAAAAAAAAAAQAAAAEAAADJAAAAAQAAAzhzdHN6AAAAAAAAAAAAAADJAAATnwAAAlsAAABwAAABNgAAALMAAABjAAAAkAAAAPoAAAC2AAAAmAAAAJ4AAAEpAAAAxAAAAKMAAACHAAABAwAAAK4AAACiAAAAngAAAToAAACuAAABGgAAAMEAAACfAAAAmAAAASYAAACoAAABKgAAALsAAACKAAAArgAAAM4AAACXAAAA+AAAAHoAAADMAAAAhQAAAHIAAABXAAAAyQAAADAAAABCAAAAfQAAAOAAAAEQAAAAuQAAAJEAAACeAAABDQAAAQMAAAB9AAAA2QAAAKYAAACKAAAAZgAAASYAAACXAAAAnQAAAI4AAAErAAAArwAAAIUAAACaAAABIQAAAKUAAAEJAAAAYwAAAI4AAAA4AAAAewAAAPkAAAApAAAAOQAAAIsAAAEiAAAAmgAAAJIAAAB1AAABDQAAAJ8AAAClAAAAigAAAPcAAACHAAAA/wAAAJ0AAAB3AAABDwAAAI4AAAEQAAAAqwAAARwAAAC/AAABJgAAAJIAAAFkAAAAuQAAAHgAAACXAAAA9gAAAIoAAAAtAAAAjwAAAOcAAAC/AAAAkAAAAIcAAAE3AAAAsAAAAIEAAACSAAAA6QAAAQMAAAEDAAAAuAAAAS8AAACEAAABDAAAAI8AAAFjAAAAvQAAAJQAAACNAAABKAAAALgAAADcAAABJQAAALAAAAD6AAAA4wAAAJ0AAAETAAAAiwAAAIAAAABvAAAAZwAAAKgAAABsAAAA2wAAAJsAAACAAAAA0AAAAOUAAADgAAAA6wAAAPQAAAEDAAABDQAAAVQAAADYAAAArgAAAKgAAADjAAABGwAAAJYAAACEAAABgAAAAMgAAACNAAAAtQAAASQAAAFmAAAAwgAAAKAAAACfAAABEQAAAKoAAACJAAAAgQAAAGoAAAByAAAAiAAAAH8AAAEIAAAAwgAAAJYAAACiAAABCAAAAJAAAAD9AAAAogAAAOYAAADYAAAAwgAAAM8AAAEPAAAAmwAAARMAAACbAAABSQAAAMcAAACoAAABNQAAAJwAAAENAAAA/gAAAPEAAAC3AAAAdAAAAJQAAAB6AAAAFHN0Y28AAAAAAAAAAQAAADAAAABidWR0YQAAAFptZXRhAAAAAAAAACFoZGxyAAAAAAAAAABtZGlyYXBwbAAAAAAAAAAAAAAAAC1pbHN0AAAAJal0b28AAAAdZGF0YQAAAAEAAAAATGF2ZjU3LjgzLjEwMA==\" type=\"video/mp4\" />\n",
              "             </video>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6mqhGSe3EEHl"
      },
      "source": [
        "###Random Action MC Episode Run (Output Episode Log)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FswdWPw3V_Qm"
      },
      "source": [
        "# This code block will run an episode just like the previous block, although\r\n",
        "# instead of the episode replay being in the form of the video, we will have\r\n",
        "# a text output of the states visited, actions taken, and rewards earned by the\r\n",
        "# MC until termination\r\n",
        "env = gym.make('MountainCar-v0')\r\n",
        "\r\n",
        "env.reset()\r\n",
        "\r\n",
        "for i in range(50000):\r\n",
        "  action = env.action_space.sample()\r\n",
        "  print(\"step i\",i,\"action=\",action)\r\n",
        "  obs, reward, done, info = env.step(action)\r\n",
        "  print(\"obs=\",obs,\"reward=\",reward,\"done=\",done,\"info=\",info)\r\n",
        "  \r\n",
        "  if done:\r\n",
        "    break\r\n",
        "    \r\n",
        "env.close()\r\n",
        "print(\"Iterations that were run:\", i)\r\n",
        "print(\"step i\",i,\"action=\",action)\r\n",
        "print(\"obs=\",obs,\"reward=\",reward,\"done=\",done,\"info=\",info)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5GgdflnoGf5s"
      },
      "source": [
        "#**Helper Functions**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BVIWA9TLorIa"
      },
      "source": [
        "##**State Space Discretization**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sya83ijSoxB4"
      },
      "source": [
        "States in the cartpole environment are defined by the pole's position, linear velocity, angle from the vertical, and angular velocity. These are continuous quantities, which are currently not compatible with our development for policy, action value, and so on. Therefore, we require to discretize the state space. The helper functions below help achieve this aim."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ApHvs2ILrTav"
      },
      "source": [
        "###Position Quantization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "woMDDicddxSL"
      },
      "source": [
        "# Position quantization helper function\r\n",
        "# 11 buckets were constructed for the quantization of \r\n",
        "# position in this implementation\r\n",
        "def position_quantization(input_position):\r\n",
        "\r\n",
        "  if input_position == -1.2:\r\n",
        "    position = 0\r\n",
        "    \r\n",
        "  elif input_position > -1.2 and input_position < -1:\r\n",
        "    position = 1\r\n",
        "\r\n",
        "  elif input_position >= -1 and input_position < -0.8:\r\n",
        "    position = 2\r\n",
        "\r\n",
        "  elif input_position >= -0.8 and input_position < -0.6:\r\n",
        "    position = 3\r\n",
        "\r\n",
        "  elif input_position >= -0.6 and input_position < -0.4:\r\n",
        "    position = 4\r\n",
        "\r\n",
        "  elif input_position >= -0.4 and input_position < -0.2:\r\n",
        "    position = 5\r\n",
        "\r\n",
        "  elif input_position >= -0.2 and input_position < 0:\r\n",
        "    position = 6\r\n",
        "\r\n",
        "  elif input_position >= 0 and input_position < 0.2:\r\n",
        "    position = 7\r\n",
        "\r\n",
        "  elif input_position >= 0.2 and input_position < 0.4:\r\n",
        "    position = 8\r\n",
        "\r\n",
        "  elif input_position >= 0.4 and input_position < 0.5:\r\n",
        "    position = 9\r\n",
        "\r\n",
        "  else:\r\n",
        "    position = 10\r\n",
        "\r\n",
        "  # Return the resulting position index\r\n",
        "  return position"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XGoDjno2rYEt"
      },
      "source": [
        "###Velocity Quantization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6nNUe3dDeHlT"
      },
      "source": [
        "# The logic described for the 'position_quantization' helper function\r\n",
        "# applies similarly here\r\n",
        "# 11 buckets were constructed for the quantization of \r\n",
        "# velocity in this implementation\r\n",
        "def velocity_quantization(input_vel):\r\n",
        "\r\n",
        "  if input_vel == -0.7:\r\n",
        "    velocity = 0\r\n",
        "\r\n",
        "  elif input_vel > -0.7 and input_vel < -0.5:\r\n",
        "    velocity = 1\r\n",
        "\r\n",
        "  elif input_vel >= -0.5 and input_vel < -0.3:\r\n",
        "    velocity = 2\r\n",
        "\r\n",
        "  elif input_vel >= -0.3 and input_vel < -0.1:\r\n",
        "    velocity = 3\r\n",
        "\r\n",
        "  elif input_vel >= -0.1 and input_vel < 0:\r\n",
        "    velocity = 4\r\n",
        "\r\n",
        "  elif input_vel == 0:\r\n",
        "    velocity = 5\r\n",
        "\r\n",
        "  elif input_vel > 0 and input_vel < 0.1:\r\n",
        "    velocity = 6\r\n",
        "\r\n",
        "  elif input_vel >= 0.1 and input_vel < 0.3:\r\n",
        "    velocity = 7\r\n",
        "\r\n",
        "  elif input_vel >= 0.3 and input_vel < 0.5:\r\n",
        "    velocity = 8\r\n",
        "\r\n",
        "  elif input_vel >= 0.5 and input_vel < 0.7:\r\n",
        "    velocity = 9\r\n",
        "\r\n",
        "  else:\r\n",
        "    velocity = 10\r\n",
        "\r\n",
        "  return velocity"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LpXjlyp0rqK0"
      },
      "source": [
        "###State Discretization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1b1_QM7cXzs2"
      },
      "source": [
        "# This function makes use of the previous helper functions in order to take\r\n",
        "# the state of the agent, represented \r\n",
        "# as a tuple (position, velocity), and output its\r\n",
        "# discretized counterpart, based on the buckets developed previously\r\n",
        "def state_discretization(input_state):\r\n",
        "\r\n",
        "  # Unpack input state into its two components\r\n",
        "  position, velocity = input_state\r\n",
        "\r\n",
        "  # Discretize each of the two variables representing the CartPole's state\r\n",
        "  position_discrete = position_quantization(position)\r\n",
        "  velocity_discrete = velocity_quantization(velocity)   \r\n",
        "\r\n",
        "  # Store the discretized version of the CartPole's state into a tuple\r\n",
        "  state_discrete = (position_discrete, velocity_discrete)\r\n",
        "\r\n",
        "  # Output the discretized state tuple\r\n",
        "  return state_discrete"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZodBkswWtYX-"
      },
      "source": [
        "##**Policy Helper Functions**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WdaoEuegtrcl"
      },
      "source": [
        "###Action Selection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "py640gagh8cL"
      },
      "source": [
        "# Action selection for the agent - based on the probability weights for each\r\n",
        "# action in a given state\r\n",
        "def action_selection(policy_input, state_input):\r\n",
        "\r\n",
        "  # From the current state, extract the possible actions and their corresponding\r\n",
        "  # weights\r\n",
        "  action_set = list(policy_input[state_input].keys())\r\n",
        "  action_weights = list(policy_input[state_input].values())\r\n",
        "\r\n",
        "  # Probabilistically an action based on each action's weight\r\n",
        "  selected_action = random.choices(action_set, action_weights, k= 1)[0]\r\n",
        "\r\n",
        "  # Return the selected action \r\n",
        "  return selected_action"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8FE2zcWouykh"
      },
      "source": [
        "###ε-Greedy Policy Generation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PMjgD3D_if__"
      },
      "source": [
        "# Generate a behaviour policy (ε-greedy) given a target policy\r\n",
        "# INPUTS: target policy, and epsilon (expects epsilon > 0)\r\n",
        "def eps_greedy_policy_generator(input_policy, eps = 0.1):\r\n",
        "\r\n",
        "  # Initialize soft_policy dictionary\r\n",
        "  soft_policy = {}\r\n",
        "\r\n",
        "  # Loop through each state\r\n",
        "  for state in input_policy:\r\n",
        "\r\n",
        "      # Determine the amount of actions that can be taken to go from current\r\n",
        "      # state to another state (|𝒜(s)|)\r\n",
        "      act_set_size = len(input_policy[state])\r\n",
        "\r\n",
        "      # Determine the maximum probability of taking an action for a given state\r\n",
        "      optimal_value = max(input_policy[state].values())\r\n",
        "\r\n",
        "      # Determine which actions out of the state's action set |𝒜(s)|\r\n",
        "      # are optimal, based on the calculated optimal value\r\n",
        "      optimal_actions = [action for action in input_policy[state] if input_policy[state][action] == optimal_value]     \r\n",
        "\r\n",
        "      # Initialize sub dictionary of the action set for the soft policy\r\n",
        "      soft_policy_action_set = {}\r\n",
        "       \r\n",
        "      # Loop through each action possible in the current state\r\n",
        "      for action in input_policy[state]:\r\n",
        "        \r\n",
        "        # The following if statements intend to assign probabilities of taking\r\n",
        "        # actions in given states in accordance with the ε-soft requirements\r\n",
        "        if action in optimal_actions:\r\n",
        "\r\n",
        "          if len(optimal_actions) != act_set_size:\r\n",
        "            # When the optimal value in an action set is > 0\r\n",
        "            # When all actions for a given state are not equiprobable,\r\n",
        "            # Set the probability of taking the optimal action to be = (1/|𝒜'(s)|)(1 - ε) + ε/|𝒜(s)|\r\n",
        "            # Where |𝒜'(s)| represents the number of optimal actions within |𝒜(s)| (i.e. ties)\r\n",
        "            soft_policy_action_set[action] = (1/len(optimal_actions))*(1 - eps) + eps/act_set_size\r\n",
        "\r\n",
        "          else:\r\n",
        "            # Otherwise, if all actions in |𝒜(s)| are equiprobable,\r\n",
        "            # and not equal to 0, they shall remain equiprobable\r\n",
        "            soft_policy_action_set[action] = (1/len(optimal_actions))\r\n",
        "\r\n",
        "        else:\r\n",
        "          # For non optimal actions, set the probability of taking\r\n",
        "          # these actions = ε/|𝒜(s)|\r\n",
        "          soft_policy_action_set[action] = eps/act_set_size\r\n",
        "\r\n",
        "      # Set the newly 𝒜(s) to the current s  \r\n",
        "      soft_policy[state] = soft_policy_action_set\r\n",
        "  \r\n",
        "  # Return the ε-greedy policy\r\n",
        "  return soft_policy"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OucynkHxH9Yc"
      },
      "source": [
        "###ε-Greedy Policy State Update"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1EN-lbi1H7n5"
      },
      "source": [
        "# Updates ε-greedy policy for a specified state (i.e. calculate π(s))\r\n",
        "def eps_greedy_policy_update(input_policy_state, eps = 0.1):\r\n",
        "\r\n",
        "  # Determine the amount of actions that can be taken to go from current\r\n",
        "  # state to another state (|𝒜(s)|)\r\n",
        "  act_set_size = len(input_policy_state)\r\n",
        "\r\n",
        "  # Determine the maximum probability of taking an action for a given state\r\n",
        "  optimal_value = max(input_policy_state.values())\r\n",
        "\r\n",
        "  # Determine which actions out of the state's action set |𝒜(s)|\r\n",
        "  # are optimal, based on the calculated optimal value\r\n",
        "  optimal_actions = [action for action in input_policy_state if input_policy_state[action] == optimal_value]     \r\n",
        "\r\n",
        "  # Initialize sub dictionary of the action set for the soft policy\r\n",
        "  soft_policy_action_set = {}\r\n",
        "    \r\n",
        "  # Loop through each action possible in the current state\r\n",
        "  for action in input_policy_state:\r\n",
        "    \r\n",
        "    # The following if statements intend to assign probabilities of taking\r\n",
        "    # actions in given states in accordance with the ε-soft requirements\r\n",
        "    if action in optimal_actions:\r\n",
        "\r\n",
        "      if len(optimal_actions) != act_set_size:\r\n",
        "        # When the optimal value in an action set is > 0\r\n",
        "        # When all actions for a given state are not equiprobable,\r\n",
        "        # Set the probability of taking the optimal action to be = (1/|𝒜'(s)|)(1 - ε) + ε/|𝒜(s)|\r\n",
        "        # Where |𝒜'(s)| represents the number of optimal actions within |𝒜(s)| (i.e. ties)\r\n",
        "        soft_policy_action_set[action] = (1/len(optimal_actions))*(1 - eps) + eps/act_set_size\r\n",
        "\r\n",
        "      else:\r\n",
        "        # Otherwise, if all actions in |𝒜(s)| are equiprobable,\r\n",
        "        # and not equal to 0, they shall remain equiprobable\r\n",
        "        soft_policy_action_set[action] = (1/len(optimal_actions))\r\n",
        "\r\n",
        "    else:\r\n",
        "      # For non optimal actions, set the probability of taking\r\n",
        "      # these actions = ε/|𝒜(s)|\r\n",
        "      soft_policy_action_set[action] = eps/act_set_size\r\n",
        "\r\n",
        "  # Return the ε policy for a given state\r\n",
        "  return soft_policy_action_set"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0V-RSEEpv-Md"
      },
      "source": [
        "###Argmax at Given State"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iYeL6vSKvkfw"
      },
      "source": [
        " # Determine the argmax of the action value function at a given state\r\n",
        " # For use in determining the agent's optimal policy\r\n",
        " def argmax_action_value(action_value_function, policy, state):\r\n",
        "\r\n",
        "  # For the given state determine the highest value which can be achieved\r\n",
        "  # by taking an action\r\n",
        "  optimal_value = max(action_value_function[state].values())\r\n",
        "\r\n",
        "  # Store the list of actions which satisfy this optimal value requirement\r\n",
        "  # in the 'optimal_actions' list\r\n",
        "  optimal_actions = [action for action in action_value_function[state] if action_value_function[state][action] == optimal_value]\r\n",
        "\r\n",
        "  # Determine the actions which generate the most optimal policy here\r\n",
        "  for action in action_value_function[state]:\r\n",
        "    \r\n",
        "    if action not in optimal_actions:\r\n",
        "      policy[state][action] = 0    \r\n",
        "    else:\r\n",
        "\r\n",
        "      if optimal_value != 0:\r\n",
        "        policy[state][action] = (1/len(optimal_actions))\r\n",
        "\r\n",
        "      else:\r\n",
        "        policy[state][action] = 0  \r\n",
        "\r\n",
        "  # Return the set of revised policy for a given state based on the action value\r\n",
        "  # argmax for a given state\r\n",
        "  return policy[state]"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4dfHgwVqfwpy"
      },
      "source": [
        "###Argmax Policy Generator (Entire State Space)\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ILPsczvY2Ytm"
      },
      "source": [
        " # Determine the argmax of the action value function at a given state\r\n",
        " # For use in determining the agent's greedy policy\r\n",
        " def argmax_policy(action_value_function):\r\n",
        "  \r\n",
        "  # Dictionary to store initial policy\r\n",
        "  policy = {}\r\n",
        "\r\n",
        "  # Store list of possible states in a variable - use the keys of the input\r\n",
        "  # action value function\r\n",
        "  state_list = list(action_value_function.keys())\r\n",
        "\r\n",
        "  # For each state  \r\n",
        "  for state in state_list:\r\n",
        "\r\n",
        "    action_list = list(action_value_function[state].keys())\r\n",
        "    action_set = {}\r\n",
        "    for action in action_list:\r\n",
        "      action_set[action] = 0\r\n",
        "    policy[state] = action_set\r\n",
        "\r\n",
        "    # For the given state determine the highest value which can be achieved\r\n",
        "    # by taking an action\r\n",
        "    optimal_value = max(action_value_function[state].values())\r\n",
        "\r\n",
        "    # Store the list of actions which satisfy this optimal value requirement\r\n",
        "    # in the 'optimal_actions' list\r\n",
        "    optimal_actions = [action for action in action_value_function[state] if action_value_function[state][action] == optimal_value]\r\n",
        "\r\n",
        "    # Determine the actions which generate the most optimal policy here\r\n",
        "    for action in action_value_function[state]:\r\n",
        "      if action not in optimal_actions:\r\n",
        "        policy[state][action] = 0    \r\n",
        "      else:\r\n",
        "        if optimal_value != 0:\r\n",
        "          policy[state][action] = (1/len(optimal_actions))\r\n",
        "        else:\r\n",
        "          policy[state][action] = 0  \r\n",
        "\r\n",
        "  # Return the set of revised policy for a given state based on the action value\r\n",
        "  # argmax for a given state\r\n",
        "  return policy"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fxpfF98F-6oB"
      },
      "source": [
        "###Arbitrary Action Selection Policy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NFyrIG_r-6Ut"
      },
      "source": [
        "# The following function will generate a policy for which all weights for each\r\n",
        "# action that can be taken within a state are equal\r\n",
        "def arbitrary_action_selection_policy(state_list, terminal_state_list, action_count):\r\n",
        "\r\n",
        "  # Initialize empty dictionary into which the arbitrary action seleciton\r\n",
        "  # policy will be stored\r\n",
        "  arbitrary_policy = {}\r\n",
        "\r\n",
        "  # Set action weights for each state in the environment which the agent can \r\n",
        "  # occupy\r\n",
        "  for state in state_list:\r\n",
        "\r\n",
        "    if state not in terminal_state_list:\r\n",
        "      action_set = {}\r\n",
        "\r\n",
        "      for action in range(0, action_count):\r\n",
        "\r\n",
        "        # Set equal weight for each action that can be taken within an \r\n",
        "        # occupiable state\r\n",
        "        action_set[action] = 1/action_count\r\n",
        "    \r\n",
        "      # Assign action weights to a state\r\n",
        "      arbitrary_policy[state] = action_set\r\n",
        "\r\n",
        "  # Return the resulting arbitrary policy\r\n",
        "  return arbitrary_policy"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-dnSPSnrfmap"
      },
      "source": [
        "##**Initialization Functions**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Zfdo8AggEkE"
      },
      "source": [
        "###State List Generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "COsOdkHk2rNx"
      },
      "source": [
        "# Possible states\r\n",
        "# position: 0-10, 11\r\n",
        "# velocity: 0-10, 11\r\n",
        "\r\n",
        "# From the above discretization, we can see that there are 11*11 = 121 states\r\n",
        "\r\n",
        "# This function will be used to generate a list of all possible discretized \r\n",
        "# states our agent can occupy\r\n",
        "def state_generator(position_count, velocity_count, terminal = False):\r\n",
        "\r\n",
        "  # Initialize an empty list of states\r\n",
        "  state_list = []\r\n",
        "\r\n",
        "  # Build a state tuple for every possible discretized state\r\n",
        "  for position in range(0, position_count):\r\n",
        "    for velocity in range(0, velocity_count):\r\n",
        "\r\n",
        "      # If we are looking for the list of non-terminal and terminal states\r\n",
        "      # then we will enter this section of the if statement\r\n",
        "      if terminal == False:\r\n",
        "        # Store each combination of 'position' and  'velocity'\r\n",
        "        # values into a tuple\r\n",
        "        state_tuple = (position, velocity)\r\n",
        "\r\n",
        "        # Append this created tuple to a list which will contain all possible\r\n",
        "        # discretized states\r\n",
        "        state_list.append(state_tuple)\r\n",
        "\r\n",
        "      # If we are only considered with finding terminal states we will enter\r\n",
        "      # this if statement\r\n",
        "      else:\r\n",
        "\r\n",
        "        # if position >= 0.5 and velocity is positive (i.e. discrete state\r\n",
        "        # anywhere from 2 to 4, this is considered a terminal state)\r\n",
        "        if position == 10 and velocity >= 5:\r\n",
        "\r\n",
        "          state_tuple = (position, velocity) \r\n",
        "          state_list.append(state_tuple)\r\n",
        "  \r\n",
        "  # Return our created list of possible states\r\n",
        "  return state_list"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nzK04xO4vJaC"
      },
      "source": [
        "###Action State Mapping Generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aMSPozSAi237"
      },
      "source": [
        "# Initialize a dictionary featuring all state and action pairs, where each\r\n",
        "# initialized value is set 0\r\n",
        "def action_state_initializer(state_list, action_list):\r\n",
        "\r\n",
        "  # Initialize value function as an empty dictionary\r\n",
        "  mapping = {}\r\n",
        "\r\n",
        "  # For each state, assign a value of 0\r\n",
        "  for state in state_list:\r\n",
        "    action_dict = {}\r\n",
        "\r\n",
        "    for action in action_list:\r\n",
        "      \r\n",
        "      action_dict[action] = 0\r\n",
        "    mapping[state] = action_dict\r\n",
        "\r\n",
        "  # Return the created dictionary\r\n",
        "  return mapping"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8Oyol7pfcoE"
      },
      "source": [
        "###Arbitrary Action Value Generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYk7Ivgg1u0N"
      },
      "source": [
        "# This function will generate an arbitrary action value function for each state\r\n",
        "# action pair, such that values range from 0 (inclusive) to 1 (exclusive)\r\n",
        "def arbitrary_action_value(state_list, action_list, terminal_states_list):\r\n",
        "  \r\n",
        "  # Initialize dictionary into which arbitrary action value function will be \r\n",
        "  # stored\r\n",
        "  action_value = {}\r\n",
        "\r\n",
        "  # For each state and action pairing, initialize with a random q(s,a)\r\n",
        "  for state in state_list:\r\n",
        "    action_set = {}\r\n",
        "    \r\n",
        "    for action in action_list:\r\n",
        "      \r\n",
        "      if state not in terminal_states_list:\r\n",
        "        action_set[action] = np.random.uniform(0,1)\r\n",
        "\r\n",
        "      else:\r\n",
        "        action_set[action] = 0\r\n",
        "        \r\n",
        "    action_value[state] = action_set\r\n",
        "\r\n",
        "  # Return the resulting action value\r\n",
        "  return action_value"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kC2pTBx1I95z"
      },
      "source": [
        "##**TD Control Helper Functions**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dHjFH9i1JBKr"
      },
      "source": [
        "###Reward Adjustment Function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHxKbiSKHGxJ"
      },
      "source": [
        "Note that for this implementation, a custom reward structure was employed to improve training progress of the MC. This reward structure encouraged the MC to scale the hill atop which sits the flag by making use of the potential-kinetic energy conversion in descending the leftmost hill from near its top."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lleEzlEUI9PG"
      },
      "source": [
        "def reward_calculation(environment, reward):\r\n",
        "\r\n",
        "  # Reward increment to be applied when the agent's state meets certain criteria\r\n",
        "  reward_increment = 7\r\n",
        "\r\n",
        "  # In the case where the agent is scaling the rightmost hill (and hasn't yet \r\n",
        "  # reached the goal), such that its x position is >= -0.2 (please see \r\n",
        "  # 'position_quantization' helper function to see that such a range corresponds \r\n",
        "  # to a position bin number >= 6), return the below reward, \r\n",
        "  # in place of the default reward\r\n",
        "  if state_discretization((environment.env.state))[0] >= 6 and  state_discretization((environment.env.state))[0] < 10 and state_discretization((environment.env.state))[1] > 5: \r\n",
        "    reward_mod = reward + reward_increment - 4\r\n",
        "\r\n",
        "  # In the case where the agent is scaling the leftmost hill such that its x\r\n",
        "  # position is <= - 1, apply a more generous reward than the previous, as \r\n",
        "  # ultimately occupying states with this criteria are what will encourage \r\n",
        "  # swift movement towards the goal of reaching the goal on the other side \r\n",
        "  # (drastic potential - kinetic energy conversion)\r\n",
        "  elif state_discretization((environment.env.state))[0] <= 2 and state_discretization((environment.env.state))[1] < 5:  \r\n",
        "    reward_mod = reward + reward_increment \r\n",
        "\r\n",
        "  # If the goal is reached, provide the following reward - higher than the\r\n",
        "  # others as ultimately, this is where we want our agent to end up\r\n",
        "  elif state_discretization((environment.env.state))[0] == 10 and state_discretization((environment.env.state))[1] >= 5: \r\n",
        "    reward_mod = reward + reward_increment * 2\r\n",
        "\r\n",
        "  # In all other cases, supply the agent with the default reward\r\n",
        "  else:\r\n",
        "    reward_mod = reward\r\n",
        "\r\n",
        "  # Return the modified reward\r\n",
        "  return reward_mod"
      ],
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHyXVahXJS2L"
      },
      "source": [
        "###TD(0) SARSA Control"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lKo53oLNJF27"
      },
      "source": [
        "def TD0_SARSA(environment, policy, action_value_function, alpha = 0.2, eps = 0.2, gamma = 1):\r\n",
        "\r\n",
        "  # Initialize a reward list which will store rewards attained in an episode\r\n",
        "  reward_list = []\r\n",
        "\r\n",
        "  # Ensure the environment is reset before processing an episode\r\n",
        "  environment.reset()\r\n",
        "\r\n",
        "\r\n",
        "  # Discretize the initial state such that we can look up the set of actions \r\n",
        "  # possible for the given state and choose the best one\r\n",
        "  initial_state = state_discretization(environment.env.state)\r\n",
        "\r\n",
        "  # Choose the first action given the current state\r\n",
        "  initial_action = action_selection(policy, initial_state)\r\n",
        "\r\n",
        "  # The variable done represents the conclusion of an episode - set it to false\r\n",
        "  # to enter the below loop\r\n",
        "  done = False\r\n",
        "\r\n",
        "  # While loop where action value and policy will be updated\r\n",
        "  while done == False:\r\n",
        "\r\n",
        "    # Perform a time step within the environment and unpack the variables \r\n",
        "    # associated with that time step...\r\n",
        "    # obs = next state\r\n",
        "    # reward = reward attained in state transition\r\n",
        "    # done = False if episode is not conclude, True otherwise\r\n",
        "    # info = an unused variable which must be extracted for unpacking to be \r\n",
        "    # successful  \r\n",
        "    obs, reward, done, info = environment.step(initial_action)\r\n",
        "\r\n",
        "    # Calculate reward, modified depending on the state to which the mountain \r\n",
        "    # car has transitioned - inspect the 'reward_calculation' function for greater\r\n",
        "    # detail\r\n",
        "    reward = reward_calculation(environment, reward)\r\n",
        "    \r\n",
        "    # Append to reward list used to summarize results of training when \r\n",
        "    # implementing TD control\r\n",
        "    reward_list.append(reward)\r\n",
        "\r\n",
        "    # Discretize the new state\r\n",
        "    new_state = state_discretization(environment.env.state)\r\n",
        "\r\n",
        "    # Select a 'new_action' for 'new_state' based on the policy\r\n",
        "    new_action = action_selection(policy, new_state)\r\n",
        "\r\n",
        "    # Update the action value function at the original state and action pair\r\n",
        "    # based on the next state and action\r\n",
        "    action_value_function[initial_state][initial_action] += alpha * (reward + gamma * action_value_function[new_state][new_action] - action_value_function[initial_state][initial_action])\r\n",
        "\r\n",
        "    # Update policy based on action_value_function update at 'initial_state'\r\n",
        "    policy[initial_state] = eps_greedy_policy_update(argmax_action_value(action_value_function, policy, initial_state), eps)\r\n",
        "\r\n",
        "    # Set the resulting state and actions to the new state and actions after\r\n",
        "    # moving in accordance with the policy\r\n",
        "    initial_state, initial_action = new_state, new_action\r\n",
        "\r\n",
        "    # If agent is transitioning to the terminal state, the episode has ended\r\n",
        "    if done:\r\n",
        "      break\r\n",
        "\r\n",
        "  # Extract position and velocity from the terminal state\r\n",
        "  position = environment.env.state[0]\r\n",
        "  velocity = environment.env.state[1]\r\n",
        "\r\n",
        "  # If the terminal state meets the requirements for a successful episode, set\r\n",
        "  # 'episode_result' to 1; otherwise, set it to 0\r\n",
        "  if position >= 0.5 and velocity >= 0:\r\n",
        "    episode_result = 1\r\n",
        "\r\n",
        "  else:\r\n",
        "    episode_result = 0\r\n",
        "\r\n",
        "  # Return the final action value function and policy, along with statistics\r\n",
        "  # on episode result and reward attained\r\n",
        "  return policy, action_value_function, reward_list, episode_result"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75VvRlzvJe4S"
      },
      "source": [
        "###TD(0) Expected SARSA Control"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h8b-o0M6JeZ8"
      },
      "source": [
        "def TD0_EXP_SARSA_ON_POLICY(environment, policy, action_value_function, alpha = 0.1, eps = 0.2, gamma = 0.95, episode_count = 10000):\r\n",
        "\r\n",
        "  # Initialize a reward list which will store rewards attained in an episode\r\n",
        "  reward_list = []\r\n",
        "\r\n",
        "  # Ensure the environment is reset before processing an episode\r\n",
        "  environment.reset()\r\n",
        "\r\n",
        "  # Discretize the initial state such that we can look up the set of actions \r\n",
        "  # possible for the given state and choose the best one\r\n",
        "  initial_state = state_discretization(environment.env.state)\r\n",
        "\r\n",
        "  # The variable done represents the conclusion of an episode - set it to false\r\n",
        "  # to enter the below loop\r\n",
        "  done = False\r\n",
        "\r\n",
        "  # While loop where action value and policy will be updated\r\n",
        "  while done == False:\r\n",
        "\r\n",
        "    action = action_selection(policy, initial_state)\r\n",
        "\r\n",
        "    # Perform a time step within the environment and unpack the variables \r\n",
        "    # associated with that time step...\r\n",
        "    # obs = next state\r\n",
        "    # reward = reward attained in state transition\r\n",
        "    # done = False if episode is not conclude, True otherwise\r\n",
        "    # info = an unused variable which must be extracted for unpacking to be \r\n",
        "    # successful  \r\n",
        "    obs, reward, done, info = environment.step(action)\r\n",
        "\r\n",
        "    # Calculate reward, modified depending on the state to which the mountain \r\n",
        "    # car has transitioned - inspect the 'reward_calculation' function for greater\r\n",
        "    # detail\r\n",
        "    reward = reward_calculation(environment, reward)\r\n",
        "\r\n",
        "    # Append to reward list used to summarize results of training when \r\n",
        "    # implementing TD control\r\n",
        "    reward_list.append(reward)\r\n",
        "\r\n",
        "    # Discretize the new state\r\n",
        "    new_state = state_discretization(environment.env.state)\r\n",
        "\r\n",
        "    # Update the action value function at the original state and action pair\r\n",
        "    # based on the next state and action\r\n",
        "    q_exp = 0 \r\n",
        "    \r\n",
        "    for a in range(len(action_value_function[new_state])):\r\n",
        "      q_exp += policy[new_state][a] * action_value_function[new_state][a] \r\n",
        "\r\n",
        "    action_value_function[initial_state][action] += alpha * (reward + gamma * q_exp - action_value_function[initial_state][action])\r\n",
        "    \r\n",
        "    # Update policy based on action_value_function update at 'initial_state'\r\n",
        "\r\n",
        "    policy[initial_state] = eps_greedy_policy_update(argmax_action_value(action_value_function, policy, initial_state), eps)\r\n",
        "\r\n",
        "    # Set the resulting state and actions to the new state and actions after\r\n",
        "    # moving in accordance with the policy\r\n",
        "    initial_state = new_state\r\n",
        "\r\n",
        "    # If agent is transitioning to the terminal state, the episode has ended\r\n",
        "    if done:\r\n",
        "      break\r\n",
        "\r\n",
        "  # Extract position and velocity from the terminal state\r\n",
        "  position = environment.env.state[0]\r\n",
        "  velocity = environment.env.state[1]\r\n",
        "\r\n",
        "  # If the terminal state meets the requirements for a successful episode, set\r\n",
        "  # 'episode_result' to 1; otherwise, set it to 0\r\n",
        "  if position >= 0.5 and velocity >= 0:\r\n",
        "    episode_result = 1\r\n",
        "\r\n",
        "  else:\r\n",
        "    episode_result = 0\r\n",
        "\r\n",
        "  # Return the final action value function and policy, along with statistics\r\n",
        "  # on episode result and reward attained\r\n",
        "  return policy, action_value_function, reward_list, episode_result"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CiCtNdYHJVYR"
      },
      "source": [
        "###TD(0) Expected SARSA (Off-Policy)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VWeijLrJJcyd"
      },
      "source": [
        "def TD0_EXP_SARSA_OFF_POLICY(environment, policy, action_value_function, alpha = 0.4, eps = 0.2, gamma = 0.95, episode_count = 10000):\r\n",
        "\r\n",
        "  # Initialize a reward list which will store rewards attained in an episode\r\n",
        "  reward_list = []\r\n",
        "\r\n",
        "  # Ensure the environment is reset before processing an episode\r\n",
        "  environment.reset()\r\n",
        "\r\n",
        "  # Discretize the initial state such that we can look up the set of actions \r\n",
        "  # possible for the given state and choose the best one\r\n",
        "  initial_state = state_discretization(environment.env.state)\r\n",
        "\r\n",
        "  # The variable done represents the conclusion of an episode - set it to false\r\n",
        "  # to enter the below loop\r\n",
        "  done = False\r\n",
        "\r\n",
        "  # While loop where action value and policy will be updated\r\n",
        "  while done == False:\r\n",
        "\r\n",
        "    action = action_selection(policy, initial_state)\r\n",
        "\r\n",
        "    # Perform a time step within the environment and unpack the variables \r\n",
        "    # associated with that time step...\r\n",
        "    # obs = next state\r\n",
        "    # reward = reward attained in state transition\r\n",
        "    # done = False if episode is not conclude, True otherwise\r\n",
        "    # info = an unused variable which must be extracted for unpacking to be \r\n",
        "    # successful  \r\n",
        "    obs, reward, done, info = environment.step(action)\r\n",
        "\r\n",
        "    # Calculate reward, modified depending on the state to which the mountain \r\n",
        "    # car has transitioned - inspect the 'reward_calculation' function for greater\r\n",
        "    # detail\r\n",
        "    reward = reward_calculation(environment, reward)\r\n",
        "\r\n",
        "    # Append to reward list used to summarize results of training when \r\n",
        "    # implementing TD control\r\n",
        "    reward_list.append(reward)\r\n",
        "\r\n",
        "    # Discretize the new state\r\n",
        "    new_state = state_discretization(environment.env.state)\r\n",
        "\r\n",
        "    # Update the action value function at the original state and action pair\r\n",
        "    # based on the next state and action\r\n",
        "    action_value_function[initial_state][action] += alpha * (reward + gamma * max(action_value_function[new_state].values()) - action_value_function[initial_state][action])\r\n",
        "\r\n",
        "    # Update policy based on action_value_function update at 'initial_state'\r\n",
        "\r\n",
        "    policy[initial_state] = eps_greedy_policy_update(argmax_action_value(action_value_function, policy, initial_state), eps)\r\n",
        "\r\n",
        "    # Set the resulting state and actions to the new state and actions after\r\n",
        "    # moving in accordance with the policy\r\n",
        "    initial_state = new_state\r\n",
        "\r\n",
        "    # If agent is transitioning to the terminal state, the episode has ended\r\n",
        "    if done:\r\n",
        "      break\r\n",
        "\r\n",
        "  policy = argmax_policy(policy)\r\n",
        "\r\n",
        "  # Extract position and velocity from the terminal state\r\n",
        "  position = environment.env.state[0]\r\n",
        "  velocity = environment.env.state[1]\r\n",
        "\r\n",
        "  # If the terminal state meets the requirements for a successful episode, set\r\n",
        "  # 'episode_result' to 1; otherwise, set it to 0\r\n",
        "  if position >= 0.5 and velocity >= 0:\r\n",
        "    episode_result = 1\r\n",
        "\r\n",
        "  else:\r\n",
        "    episode_result = 0\r\n",
        "\r\n",
        "  # Return the final action value function and policy, along with statistics\r\n",
        "  # on episode result and reward attained\r\n",
        "  return policy, action_value_function, reward_list, episode_result"
      ],
      "execution_count": 166,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGrurLd0QaAG"
      },
      "source": [
        "###n-Step SARSA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WY3LjsDSN9Wz"
      },
      "source": [
        "def N_STEP_SARSA(environment, policy, action_value_function, n = 0,  alpha = 0.1, eps = 0.2, gamma = 0.95, episode_count = 5000):\r\n",
        "\r\n",
        "  # Initialize a reward list which will store rewards attained in an episode\r\n",
        "  reward_list = []\r\n",
        "  state_action_reward_list = []\r\n",
        "\r\n",
        "  # Ensure the environment is reset before processing an episode\r\n",
        "  environment.reset()\r\n",
        "\r\n",
        "  # generate initial state and action\r\n",
        "  state = state_discretization(environment.env.state)\r\n",
        "  action = action_selection(policy, state)\r\n",
        "\r\n",
        "  state_action_reward_list = []\r\n",
        "  t = 0\r\n",
        "  T = 10e10\r\n",
        "\r\n",
        "  while True:\r\n",
        "\r\n",
        "    # If current timestep is not the terminal step, enter the if \r\n",
        "    # statement to take the next action\r\n",
        "    if t < T:\r\n",
        "      \r\n",
        "      # Extract variables associated with taking an action\r\n",
        "      obs, reward, done, info = environment.step(action)\r\n",
        "\r\n",
        "      # Calculate reward, modified depending on the state to which the mountain \r\n",
        "      # car has transitioned - inspect the 'reward_calculation' function for greater\r\n",
        "      # detail\r\n",
        "      reward = reward_calculation(environment, reward)\r\n",
        "\r\n",
        "      # Store initial state action and reward\r\n",
        "      state_action_reward_list.append((state, action, reward))\r\n",
        "\r\n",
        "      # Append to reward list used to summarize results of training when \r\n",
        "      # implementing TD control\r\n",
        "      reward_list.append(reward)\r\n",
        "\r\n",
        "      # Update 'state' variable to the state arrived at after taking \r\n",
        "      # 'action'\r\n",
        "      state = state_discretization(environment.env.state)\r\n",
        "\r\n",
        "      # If the next state is terminal, update 'T', which represents the \r\n",
        "      # terminal timestep\r\n",
        "      if done:\r\n",
        "        T = t + 1\r\n",
        "      \r\n",
        "      else:\r\n",
        "        # Select and store next action, as long as this action is not \r\n",
        "        # taken at T (no actions are taken at T)\r\n",
        "        action = action_selection(policy, state)\r\n",
        "\r\n",
        "    # tau is intended to represents timesteps for which action value \r\n",
        "    # updates can be made; when the quantity tau is positive (starting at tau = 0),\r\n",
        "    # the return at timestep tau can be estimated using the n steps taken\r\n",
        "    # by the agent in the environment\r\n",
        "    tau = t - n + 1\r\n",
        "\r\n",
        "    # Enter the loop only when τ (tau) is >= 0 (i.e. when enough \r\n",
        "    # states have been visited to estimate n-step returns)\r\n",
        "    if tau >= 0:\r\n",
        "\r\n",
        "\r\n",
        "      # Initialize the return variable to 0 prior to its calculation\r\n",
        "      G = 0\r\n",
        "\r\n",
        "      # For the return calculation, we need to check that the current \r\n",
        "      # timestep τ + n (number of steps after τ) is equal to or less than \r\n",
        "      # T - cannot calculate returns beyond T\r\n",
        "      end_step = min(tau + n, T)\r\n",
        "\r\n",
        "      # Calculate first component of estimate for return using n \r\n",
        "      # rewards achieved beyond τ\r\n",
        "      for i in range(tau, end_step):\r\n",
        "\r\n",
        "        # Reward achieved when transitioning from state i - 1 to i\r\n",
        "        reward_i = state_action_reward_list[i][2]\r\n",
        "\r\n",
        "        # Update returns computation with these n rewards and γ (gamma)\r\n",
        "        G += (gamma**(i - tau)) * reward_i\r\n",
        "\r\n",
        "      # If the timestep τ + n is not the terminal timestep T, enter\r\n",
        "      # the if statement and update the return to estimate for the \r\n",
        "      # remaining terms of the return calculation\r\n",
        "      if tau + n < T:\r\n",
        "\r\n",
        "        # Extract the state and action stored at timestep τ + n\r\n",
        "        state_tau_n, action_tau_n = state, action\r\n",
        "\r\n",
        "        # Complete our estimate of the return using our estimated\r\n",
        "        # action value function value at the state and action for \r\n",
        "        # timestep τ + n\r\n",
        "        G += (gamma**n) * action_value_function[state_tau_n][action_tau_n]\r\n",
        "\r\n",
        "      # Extract the state and action stored at timestep τ\r\n",
        "      state_tau, action_tau, _ = state_action_reward_list[tau]\r\n",
        "\r\n",
        "      # Store into a variable the current value of the action value\r\n",
        "      # function at the state and action associated with timestep τ\r\n",
        "      q_tau = action_value_function[state_tau][action_tau]\r\n",
        "\r\n",
        "      # Update the action value function for the state and action \r\n",
        "      # associated with timestep τ with our newly calculated \r\n",
        "      # return estimate at timestep τ\r\n",
        "      action_value_function[state_tau][action_tau] = q_tau + alpha * (G - q_tau)\r\n",
        "\r\n",
        "      # Update policy based on action_value_function update at 'initial_state'\r\n",
        "      policy[state_tau] = eps_greedy_policy_update(argmax_action_value(action_value_function, policy, state_tau), eps)\r\n",
        "\r\n",
        "    # If the timestep τ is the step just before reaching\r\n",
        "    # the terminal state, exit the algorithm\r\n",
        "\r\n",
        "    if tau == T - 1:\r\n",
        "\r\n",
        "      break\r\n",
        "\r\n",
        "    # Increment timestep variable t\r\n",
        "    t += 1\r\n",
        "\r\n",
        "\r\n",
        "  # Extract position and velocity from the terminal state\r\n",
        "  position = environment.env.state[0]\r\n",
        "  velocity = environment.env.state[1]\r\n",
        "\r\n",
        "  # If the terminal state meets the requirements for a successful episode, set\r\n",
        "  # 'episode_result' to 1; otherwise, set it to 0\r\n",
        "  if position >= 0.5 and velocity >= 0:\r\n",
        "    episode_result = 1\r\n",
        "\r\n",
        "  else:\r\n",
        "    episode_result = 0\r\n",
        "\r\n",
        "  # Return 𝜋 and q, after updating action value with the algorithm for\r\n",
        "  # a single episode\r\n",
        "  return policy, action_value_function, reward_list, episode_result"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1rbNaYetKAbT"
      },
      "source": [
        "###Tree Backup Control"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Al0O3exC9q0A"
      },
      "source": [
        "def TREE_BACKUP(environment, policy, behaviour_policy, action_value_function, n = 0,  alpha = 0.1, eps = 0.2, gamma = 0.95, episode_count = 5000):\r\n",
        "\r\n",
        "  # Initialize a reward list which will store rewards attained in an episode\r\n",
        "  reward_list = []\r\n",
        "  state_action_reward_list = []\r\n",
        "\r\n",
        "  # Ensure the environment is reset before processing an episode\r\n",
        "  # Reset the environment to start an episodic trial\r\n",
        "  environment.reset()\r\n",
        "\r\n",
        "  # generate initial state and action\r\n",
        "  state = state_discretization(environment.env.state)\r\n",
        "  action = action_selection(behaviour_policy, state)\r\n",
        "\r\n",
        "  # Initialize a list which will store tuples of (state, action, reward)\r\n",
        "  # Note in this implementation that reward at timestep t + 1 will be in \r\n",
        "  # the same tuple as the state and action for timestep t.\r\n",
        "  state_action_reward_list = []\r\n",
        "  t = 0\r\n",
        "  T = 10e10\r\n",
        "\r\n",
        "  # Let this loop run until the break condition at the end\r\n",
        "  while True:\r\n",
        "\r\n",
        "    # If current timestep is not the terminal step, enter the if \r\n",
        "    # statement to take the next action\r\n",
        "    if t < T:\r\n",
        "      \r\n",
        "      # Extract variables associated with taking an action\r\n",
        "      obs, reward, done, info = environment.step(action)\r\n",
        "\r\n",
        "      # Calculate reward, modified depending on the state to which the mountain \r\n",
        "      # car has transitioned - inspect the 'reward_calculation' function for greater\r\n",
        "      # detail\r\n",
        "      reward = reward_calculation(environment, reward)\r\n",
        "\r\n",
        "      # Store initial state action and reward\r\n",
        "      state_action_reward_list.append((state, action, reward))\r\n",
        "\r\n",
        "      # Append to reward list used to summarize results of training when \r\n",
        "      # implementing TD control\r\n",
        "      reward_list.append(reward)\r\n",
        "\r\n",
        "      # Update 'state' variable to the state arrived at after taking \r\n",
        "      # 'action'\r\n",
        "      state = state_discretization(environment.env.state)\r\n",
        "\r\n",
        "      # If the next state is terminal, update 'T', which represents the \r\n",
        "      # terminal timestep\r\n",
        "      if done:\r\n",
        "        T = t + 1\r\n",
        "      \r\n",
        "      else:\r\n",
        "        # Select and store next action, as long as this action is not \r\n",
        "        # taken at T (no actions are taken at T)\r\n",
        "        action = action_selection(behaviour_policy, state)\r\n",
        "\r\n",
        "    # tau is intended to represents timesteps for which action value \r\n",
        "    # updates can be made; when the quantity tau is positive (starting at tau = 0),\r\n",
        "    # the return at timestep tau can be estimated using the n steps taken\r\n",
        "    # by the agent in the environment\r\n",
        "    tau = t - n + 1\r\n",
        "\r\n",
        "    # Enter the loop only when τ (tau) is >= 0 (i.e. when enough \r\n",
        "    # states have been visited to estimate n-step returns)\r\n",
        "    if tau >= 0:\r\n",
        "      \r\n",
        "      # If terminal state has been reached\r\n",
        "      if t + 1 >= T:\r\n",
        "        G = reward\r\n",
        "\r\n",
        "      # Otherwise, return is calculated in the following manner:\r\n",
        "      else:\r\n",
        "        q_exp = 0 \r\n",
        "      \r\n",
        "        for a in range(len(action_value_function[state])):\r\n",
        "          q_exp += policy[state][a] * action_value_function[state][a]\r\n",
        "\r\n",
        "        G = reward + gamma * q_exp\r\n",
        "\r\n",
        "      # For the return calculation, we need to check that the current \r\n",
        "      # timestep τ + n (number of steps after τ) is equal to or less than \r\n",
        "      # T - cannot calculate returns beyond T\r\n",
        "\r\n",
        "\r\n",
        "      # Calculate first component of estimate for return using n \r\n",
        "      # rewards achieved beyond τ\r\n",
        "      for k in range(min(t, T -1), tau + 1, -1):\r\n",
        "\r\n",
        "        # Extract state, action and reward associated with timestep k\r\n",
        "        state_k = state_action_reward_list[k][0]\r\n",
        "        action_k = state_action_reward_list[k][1]\r\n",
        "\r\n",
        "        # reward at timestep k is stored with state and action of timestep\r\n",
        "        # k - 1, hence the difference in first index\r\n",
        "        reward_k = state_action_reward_list[k - 1][2]\r\n",
        "\r\n",
        "        q_exp_k = 0 \r\n",
        "        \r\n",
        "        # Calculate expected value of the action value for all actions except\r\n",
        "        # the action taken at step k\r\n",
        "        for a in range(len(action_value_function[state])):\r\n",
        "          if a != action_k:\r\n",
        "            q_exp_k += policy[state_k][a] * action_value_function[state_k][a]\r\n",
        "        \r\n",
        "        # Update returns computation using the tree backup algorithm\r\n",
        "        G = reward_k + gamma * q_exp_k + gamma * policy[state_k][action_k] * G\r\n",
        "\r\n",
        "      # Extract the state and action stored at timestep τ\r\n",
        "      state_tau, action_tau, _ = state_action_reward_list[tau]\r\n",
        "\r\n",
        "      # Store into a variable the current value of the action value\r\n",
        "      # function at the state and action associated with timestep τ\r\n",
        "      q_tau = action_value_function[state_tau][action_tau]\r\n",
        "\r\n",
        "      # Update the action value function for the state and action \r\n",
        "      # associated with timestep τ with our newly calculated \r\n",
        "      # return estimate at timestep τ\r\n",
        "      action_value_function[state_tau][action_tau] = q_tau + alpha * (G - q_tau)\r\n",
        "\r\n",
        "      # Update the policy in accordance with the updated action value function\r\n",
        "      policy[state_tau] = argmax_action_value(action_value_function, policy, state_tau)\r\n",
        "\r\n",
        "    # If the timestep τ is the step just before reaching\r\n",
        "    # the terminal state, exit the n-step SARSA algorithm\r\n",
        "    if tau == T - 1:\r\n",
        "\r\n",
        "      break\r\n",
        "\r\n",
        "    # Increment timestep variable t\r\n",
        "    t += 1\r\n",
        "\r\n",
        "\r\n",
        "  # Extract position and velocity from the terminal state\r\n",
        "  position = environment.env.state[0]\r\n",
        "  velocity = environment.env.state[1]\r\n",
        "\r\n",
        "  # If the terminal state meets the requirements for a successful episode, set\r\n",
        "  # 'episode_result' to 1; otherwise, set it to 0\r\n",
        "  if position >= 0.5 and velocity >= 0:\r\n",
        "    episode_result = 1\r\n",
        "\r\n",
        "  else:\r\n",
        "    episode_result = 0\r\n",
        "\r\n",
        "  # Return 𝜋 and q, after updating action value with the algorithm for\r\n",
        "  # a single episode\r\n",
        "  return policy, action_value_function, reward_list, episode_result"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ljoCEI6wURHb"
      },
      "source": [
        "###TD Control Function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1zMRxG-8KwFt"
      },
      "source": [
        "This function is built out of convenience. Different TD Control methods can be selected through this function by specifying different inputs for its 'method' parameter."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WJDzoxzHX_tM"
      },
      "source": [
        "# Function which will print an error message in the case an incorrect 'method'\r\n",
        "# input is provided to the 'TD_Control' function below\r\n",
        "def method_input_error():\r\n",
        "  print('Invalid input - please choose from:')\r\n",
        "  print(' If n = 0:')\r\n",
        "  print('  - \\'SARSA\\'')\r\n",
        "  print('  - \\'EXP_SARSA_OFF_POLICY\\'')\r\n",
        "  print('  - \\'EXP_SARSA_ON_POLICY\\'')    \r\n",
        "  print(' \\nIf n > 0:')\r\n",
        "  print('  - \\'SARSA\\'')\r\n",
        "  print('  - \\'TREE_BACKUP\\'')"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-idZlcORC-1"
      },
      "source": [
        "# Cumulative TD Control function for which any of the methods programmed above\r\n",
        "# can be selected\r\n",
        "def TD_CONTROL(environment, initial_policy, initial_action_value_function, method = 'EXP_SARSA', n = 0,  alpha = 0.1, eps = 0.2, gamma = 0.95, episode_count = 5000):\r\n",
        "\r\n",
        "  # Parameter representing the maximum number of episodes possible in an episode\r\n",
        "  # DO NOT CHANGE\r\n",
        "  max_episode_steps = 200\r\n",
        "\r\n",
        "  # Set a batch size for which updates on training will be reported through a \r\n",
        "  # print statement\r\n",
        "  batch_size = 1000\r\n",
        "\r\n",
        "  # Initialize lists that will be used to provide statistics on the training\r\n",
        "  # of the model\r\n",
        "  total_reward = []\r\n",
        "  total_episode_length = []\r\n",
        "  total_episode_result = []\r\n",
        "\r\n",
        "  # Create a deep copy of the greedy policy provided as a parameter to this \r\n",
        "  # function. \r\n",
        "  input_policy = copy.deepcopy(initial_policy)\r\n",
        "\r\n",
        "  # Here, an ε-greedy policy will be constructed out of the greedy policy\r\n",
        "  policy = eps_greedy_policy_generator(input_policy, eps = eps)\r\n",
        "\r\n",
        "  # Create a deep copy of the input q\r\n",
        "  action_value_function = copy.deepcopy(initial_action_value_function)\r\n",
        "\r\n",
        "  # Loop the selected method for each episode\r\n",
        "  for episode in range(0, episode_count):\r\n",
        "\r\n",
        "    # n specifies the number of steps in an episode used to compute rewards\r\n",
        "    # n = 0 methods (SARSA, Off-Policy Expected SARSA, and On-Policy Expected\r\n",
        "    # SARSA are included here)\r\n",
        "    if n == 0:\r\n",
        "      \r\n",
        "      if method == 'SARSA':\r\n",
        "        policy, action_value_function, reward_list, episode_result = TD0_SARSA(environment, policy, action_value_function, alpha = alpha, eps = eps, gamma = gamma)\r\n",
        "\r\n",
        "      elif method == 'EXP_SARSA_OFF_POLICY':\r\n",
        "        policy, action_value_function, reward_list, episode_result = TD0_EXP_SARSA_OFF_POLICY(environment, policy, action_value_function, alpha = alpha, eps = eps, gamma = gamma)\r\n",
        "\r\n",
        "      elif method == 'EXP_SARSA_ON_POLICY':\r\n",
        "        policy, action_value_function, reward_list, episode_result = TD0_EXP_SARSA_ON_POLICY(environment, policy, action_value_function, alpha = alpha, eps = eps, gamma = gamma)\r\n",
        "     \r\n",
        "      # For all other inputs to the 'method' parameter, an error message \r\n",
        "      # requesting a correct input will be displayed\r\n",
        "      else:\r\n",
        "        method_input_error()\r\n",
        "        break\r\n",
        "\r\n",
        "    # The methods for which n > 0 include n-SARSA, and Tree Backup\r\n",
        "    elif n > 0:\r\n",
        "\r\n",
        "      if method == 'SARSA':\r\n",
        "        policy, action_value_function, reward_list, episode_result = N_STEP_SARSA(environment, policy, action_value_function, n = n,  alpha = alpha, eps = eps, gamma = gamma)\r\n",
        "\r\n",
        "      elif method == 'TREE_BACKUP':\r\n",
        "        policy, action_value_function, reward_list, episode_result = TREE_BACKUP(environment, input_policy, policy, action_value_function, n = n,  alpha = alpha, eps = eps, gamma = gamma)\r\n",
        "\r\n",
        "      # For all other inputs to the 'method' parameter, an error message \r\n",
        "      # requesting a correct input will be displayed\r\n",
        "      else:\r\n",
        "        method_input_error()\r\n",
        "        break\r\n",
        "\r\n",
        "    # If n is negative, the following message will be displayed\r\n",
        "    else:\r\n",
        "      print('Invalid input for \\'n\\'')\r\n",
        "\r\n",
        "    # For each episode, sum up the rewards, retrieve the number of episode steps,\r\n",
        "    # and retrieve the episode outcome (success or failure); store these into \r\n",
        "    # lists which will be used to summarize overall training statistics\r\n",
        "    total_reward.append(sum(reward_list))\r\n",
        "    total_episode_length.append(len(reward_list))\r\n",
        "    total_episode_result.append(episode_result)\r\n",
        "\r\n",
        "    # Report training statistics only when a batch is completed\r\n",
        "    if (episode + 1) % batch_size == 0:\r\n",
        "\r\n",
        "      # Print number of episodes completed and the \r\n",
        "      print(episode + 1, ' episodes completed... out of ', episode_count, '*' * 40)\r\n",
        "\r\n",
        "      # Calculate the average reward attained in episodes from the first episode\r\n",
        "      # to the current episode\r\n",
        "      total_avg_reward = sum(total_reward)/len(total_reward)\r\n",
        "\r\n",
        "      # Calculate the average episode length from the first episode\r\n",
        "      # to the current episode\r\n",
        "      total_avg_episode_length = sum(total_episode_length)/len(total_episode_length)\r\n",
        "\r\n",
        "      # Calculate the number of successful episodes from the first episode to\r\n",
        "      # the current episode\r\n",
        "      episode_result_cumulative = sum(total_episode_result)\r\n",
        "\r\n",
        "      # Calculate the three statistics determined above except this time only \r\n",
        "      # compute them for the episodes within the batch just completed\r\n",
        "      batch_avg_reward = sum(total_reward[(episode + 1 - batch_size):(episode + 1)])/batch_size\r\n",
        "      batch_avg_episode_length = sum(total_episode_length[(episode + 1 - batch_size):(episode + 1)])/batch_size\r\n",
        "      episode_result_batch = sum(total_episode_result[(episode + 1 - batch_size):(episode + 1)])\r\n",
        "\r\n",
        "      # Display the results of the statistics calculated above - first the \r\n",
        "      # cumulative statistics, then the results for the batch\r\n",
        "      print(' Cumulative Statistics:')\r\n",
        "\r\n",
        "      # Total average reward attained\r\n",
        "      print('   Cumulative Average Reward = ', total_avg_reward)\r\n",
        "\r\n",
        "      # Average episode length along with its fractional length (out of max\r\n",
        "      # possible episode length)\r\n",
        "      print('   Cumulative Average Episode Length = ', total_avg_episode_length, '/', max_episode_steps, ' (', total_avg_episode_length/max_episode_steps, ')')     \r\n",
        "      \r\n",
        "      # Number of successful episodes (out of all episodes)\r\n",
        "      print('   Cumulative Successful Episode Count = ', episode_result_cumulative, '/', len(total_episode_result), ' (',  episode_result_cumulative/len(total_episode_result), ')')     \r\n",
        "\r\n",
        "      # Similar description of statistics, but this time just pertaining to the\r\n",
        "      # batch\r\n",
        "      print('\\n Batch Statistics (episodes ', (episode + 1) - batch_size + 1, ' to ', episode + 1, '):')\r\n",
        "      print('   Batch Average Reward = ', batch_avg_reward)\r\n",
        "      print('   Batch Average Episode Length = ', batch_avg_episode_length, '/', max_episode_steps, ' (', batch_avg_episode_length/max_episode_steps, ')')           \r\n",
        "      print('   Batch Successful Episode Count = ', episode_result_batch, '/', batch_size, ' (', episode_result_batch/batch_size, ')\\n') \r\n",
        "\r\n",
        "    # If all episodes in the episode_count have been run, training is complete\r\n",
        "    # and method, selected parameters, and final results will be displayed\r\n",
        "    if episode + 1 == episode_count:\r\n",
        "\r\n",
        "      print('-' * 81)\r\n",
        "      print('𝐓𝐑𝐀𝐈𝐍𝐈𝐍𝐆 𝐂𝐎𝐌𝐏𝐋𝐄𝐓𝐄')\r\n",
        "      \r\n",
        "      # Determine how to display chosen TD control method based on user \r\n",
        "      # inputs to the relevant parameters ('n' and 'method') \r\n",
        "      if n == 0:\r\n",
        "        selected_method = method\r\n",
        "      else:  \r\n",
        "        if method == 'SARSA':\r\n",
        "          selected_method = 'n-SARSA (n = ' + str(n) + ')'\r\n",
        "        else:\r\n",
        "          selected_method = 'TREE_BACKUP'\r\n",
        "\r\n",
        "      # Print the selected method\r\n",
        "      print('ᴛᴅ-ᴄᴏɴᴛʀᴏʟ ᴍᴇᴛʜᴏᴅ:')\r\n",
        "      print(' ', selected_method)\r\n",
        "\r\n",
        "      # Print the training hyperparameters used\r\n",
        "      print('\\nᴘᴀʀᴀᴍᴇᴛᴇʀꜱ:')\r\n",
        "      print(' Step-size (𝜶) = ', alpha) \r\n",
        "      print(' Discount Factor (𝜸) = ', gamma)\r\n",
        "      print(' 𝜀-greedy constant (𝜺) = ', eps)\r\n",
        "      \r\n",
        "      # Print cumulative statistics after all episodes have run\r\n",
        "      print('\\nꜱᴜᴍᴍᴀʀʏ ꜱᴛᴀᴛɪꜱᴛɪᴄꜱ:')\r\n",
        "      \r\n",
        "      # Create a list with the episode lengths for the successful episodes only\r\n",
        "      successful_episodes = [total_episode_length[x] for x in range(0, len(total_episode_length)) if total_episode_result[x] == 1]\r\n",
        "      \r\n",
        "      # Calculate the average of episode length for successful episodes only\r\n",
        "      avg_successful_episode_length = sum(successful_episodes)/len(successful_episodes) if len(successful_episodes) !=0 else 0\r\n",
        "      \r\n",
        "      # Display average episode length including failed episodes\r\n",
        "      print(' Cumulative Average Episode Length = ', total_avg_episode_length, '/', max_episode_steps, ' (', total_avg_episode_length/max_episode_steps, ')')\r\n",
        "      \r\n",
        "      # Display average episode length excluding failed episodes\r\n",
        "      print(' Cumulative Average Successful Episode Length: ', avg_successful_episode_length, '/', max_episode_steps, ' (', avg_successful_episode_length/max_episode_steps, ')')\r\n",
        "      \r\n",
        "      # Display the count of successful episodes \r\n",
        "      print(' Cumulative Successful Episode Count = ', episode_result_cumulative, '/', len(total_episode_result), ' (',  episode_result_cumulative/len(total_episode_result), ')')     \r\n",
        "\r\n",
        "  # Return the final policy and action value function after all episodes in the\r\n",
        "  # episode count have run\r\n",
        "  return policy, action_value_function"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z4ZGdW_YUj_m"
      },
      "source": [
        "##**Performance Assessment**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15kjdHiK7WiW"
      },
      "source": [
        "###Performance Assessment Function for MC"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NkGzHU6VUwro"
      },
      "source": [
        "# Function to demonstrate performance of agent\r\n",
        "def MountainCar_TEST(trial_count, episode_count, policy_input, display_trial_readout = True, display_episode_readout = True):\r\n",
        "\r\n",
        "  # QUICK DEFINITION: trial - a group of episodes\r\n",
        "  # 'episode_count': this parameter represents how many episodes are in each \r\n",
        "  # trial\r\n",
        "\r\n",
        "  # Episodes can only last 200 timesteps - do not change this variable\r\n",
        "  max_episode_length = 200\r\n",
        "\r\n",
        "  # Length of timesteps for each trial (trial is a set of episodes)\r\n",
        "  trial_step_count = episode_count * max_episode_length\r\n",
        "\r\n",
        "  # Generate the MC environment\r\n",
        "  env = gym.make(\"MountainCar-v0\")\r\n",
        "\r\n",
        "  # Initialize lists which will store \r\n",
        "  successful_episode_average_list = []\r\n",
        "  successful_episode_count_list = []\r\n",
        "\r\n",
        "  # Loop for each trial\r\n",
        "  for trial in range(0, trial_count):\r\n",
        "    \r\n",
        "    # For each trial initialize a list into which the length of episodes will \r\n",
        "    # be stored\r\n",
        "    episode_steps = []\r\n",
        " \r\n",
        "    # Initialize a list which will indicate whether or not episodes have \r\n",
        "    # succeeded or failed\r\n",
        "    successful_episode_bool = []\r\n",
        "\r\n",
        "    # Loop for each episode within the trial\r\n",
        "    for episode in range(0, episode_count):\r\n",
        "      \r\n",
        "      # Reset the environment before processing an episode \r\n",
        "      env.reset()\r\n",
        "      for i in range(50000):\r\n",
        "\r\n",
        "        # Choose an action and move forward to the next time step\r\n",
        "        action = action_selection(policy_input, state_discretization(env.env.state))\r\n",
        "        obs, reward, done, info = env.step(action)\r\n",
        "\r\n",
        "        # If termination is reached, end the episode\r\n",
        "        if done:\r\n",
        "          break\r\n",
        "\r\n",
        "      # Retrieve the position and velocity when an episode has concluded\r\n",
        "      # These measures will be used to determine if the episode was a \r\n",
        "      # successful one or not\r\n",
        "      position = env.env.state[0]\r\n",
        "      velocity = env.env.state[1]\r\n",
        "\r\n",
        "      # A successful episode is one where the flag is reached (i.e. a position \r\n",
        "      # >= 0.5, and velocity >= 0...\r\n",
        "      if position >= 0.5 and velocity >=0:\r\n",
        "        successful_episode_bool.append(1)\r\n",
        "\r\n",
        "      else:\r\n",
        "        successful_episode_bool.append(0)\r\n",
        "\r\n",
        "      # Append the number of episode steps completed in an episode\r\n",
        "      episode_steps.append(i + 1)\r\n",
        "\r\n",
        "    # Create a list containing only the step counts for episodes that were \r\n",
        "    # successful \r\n",
        "    successful_episode_steps = [steps for success, steps in zip(successful_episode_bool, episode_steps) if success == 1]\r\n",
        "\r\n",
        "    # Determine the average step count of successful episodes\r\n",
        "    successful_episode_average = (sum(successful_episode_steps)/len(successful_episode_steps)) if len(successful_episode_steps) !=0 else 0\r\n",
        "    \r\n",
        "    # Determine the amount of successful episodes\r\n",
        "    successful_episode_count = len(successful_episode_steps)\r\n",
        "\r\n",
        "    # Append the above calculation to a list that will store these measures\r\n",
        "    # in order to average them over all trials\r\n",
        "    successful_episode_average_list.append(successful_episode_average)\r\n",
        "    successful_episode_count_list.append(successful_episode_count)\r\n",
        "\r\n",
        "    # Here we will display the output of our calculations - output can be \r\n",
        "    # seen on an trial-by-trial basis - or, more granularly, an episode-by-\r\n",
        "    # -episode basis. These can also both be turned off with the 'display_' + x\r\n",
        "    # + '_readout' parameters to this function - when they are turned off only\r\n",
        "    # the final cumulative statistics will be seen \r\n",
        "    if display_trial_readout == True or display_episode_readout == True:\r\n",
        "      print('*****Trial ', trial + 1, ' (',episode_count, 'Episodes) *****************')\r\n",
        "\r\n",
        "    if display_episode_readout == True: \r\n",
        "\r\n",
        "        \r\n",
        "      for episode in range(0, episode_count):\r\n",
        "\r\n",
        "        # Print whether episode has failed or succeeded\r\n",
        "        print('\\n Episode ', episode + 1, ':')\r\n",
        "\r\n",
        "        if successful_episode_bool[episode] == 0:\r\n",
        "          print('  FAILED EPISODE...')\r\n",
        "\r\n",
        "        else:\r\n",
        "          print('  SUCCESSFUL EPISODE...')\r\n",
        "\r\n",
        "        # Print the length of the episode\r\n",
        "        print('    Episode Length = ', episode_steps[episode], '/', max_episode_length)\r\n",
        "        print('    Fractional Length = ', episode_steps[episode]/max_episode_length)\r\n",
        "\r\n",
        "    if display_trial_readout == True:\r\n",
        "      # For each trial, display the number of successful episodes and \r\n",
        "      # average successful episode length; they are also displayed in fraction-\r\n",
        "      # al form with the print statements below\r\n",
        "      print('Successful Episodes = ', sum(successful_episode_bool), '/', episode_count)\r\n",
        "      print('Fraction of Successful Episodes = ', sum(successful_episode_bool)/episode_count)\r\n",
        "      print('Average Successful Episode Length = ', successful_episode_average, '/', max_episode_length)\r\n",
        "      print('Fraction Successful Episode Length = ', successful_episode_average/max_episode_length, '\\n')\r\n",
        "\r\n",
        "  # Cumulative results from the trials are calculated here\r\n",
        "  total_successful_episode_count = sum(successful_episode_count_list)\r\n",
        "  total_successful_episode_average = sum(successful_episode_average_list)/len(successful_episode_average_list) if len(successful_episode_average_list) !=0 else 0\r\n",
        "\r\n",
        "  # Display cumulative statistics from the trial runs\r\n",
        "  print('CUMULATIVE RESULTS:')\r\n",
        "  print('Total Number of Successful Episodes = ', total_successful_episode_count, '/', (episode_count * trial_count))\r\n",
        "  print('Fraction of Successful Episodes = ', total_successful_episode_count/(episode_count * trial_count))\r\n",
        "  print('Average of Average Successful Episode Steps per Trial = ', total_successful_episode_average, '/', max_episode_length)\r\n",
        "  print('Average of Average Successful Episode Steps per Trial (Fractional) = ', total_successful_episode_average/max_episode_length)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKGmAfZfzB9e"
      },
      "source": [
        "#**Task 1: TD(0)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfTnIU1p770Z"
      },
      "source": [
        "Develop a TD(0) controller using:\r\n",
        "\r\n",
        "- on-policy SARSA\r\n",
        "- on-policy expected SARSA\r\n",
        "- off-policy expected SARSA with a greedy control policy.\r\n",
        "<br>\r\n",
        "\r\n",
        "Compare the performance of your controllers.\r\n",
        "\r\n",
        "This will be graded."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-KoDpcFZLlGp"
      },
      "source": [
        "##**Initialized Inputs**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "miGCYexBLo5J"
      },
      "source": [
        "###Generate Discrete State Lists"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G9QEYLDqI1GF"
      },
      "source": [
        "# State list\r\n",
        "state_list = state_generator(11, 11)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dIMCxZ-3Lc5l"
      },
      "source": [
        "# Terminal state list\r\n",
        "terminal_state_list = state_generator(11, 11, terminal = True)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78T7QrVmLx5y"
      },
      "source": [
        "###Arbitrary Initial Action Value ($q_i$) Generation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "4wzi8xRuL38z",
        "outputId": "4ba57e36-9156-45a0-d381-8cf520e18df8"
      },
      "source": [
        "# Generate arbitrary action value function\r\n",
        "q_i = arbitrary_action_value(state_list,[0,1,2], terminal_state_list)\r\n",
        "\r\n",
        "# Display action value function\r\n",
        "q_i"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{(0, 0): {0: 0.18612527509967036,\n",
              "  1: 0.9551259472879744,\n",
              "  2: 0.8590289367558246},\n",
              " (0, 1): {0: 0.22152217299892984,\n",
              "  1: 0.1406886623430168,\n",
              "  2: 0.38405619599826357},\n",
              " (0, 2): {0: 0.7148828032686908,\n",
              "  1: 0.9066078667902302,\n",
              "  2: 0.14307067377029603},\n",
              " (0, 3): {0: 0.8876088003306211, 1: 0.8733232555776005, 2: 0.4802353169499928},\n",
              " (0, 4): {0: 0.5278718388836074,\n",
              "  1: 0.016107002170776763,\n",
              "  2: 0.07123127849120492},\n",
              " (0, 5): {0: 0.0011626494086393047,\n",
              "  1: 0.884918327748355,\n",
              "  2: 0.5841875994327754},\n",
              " (0, 6): {0: 0.6417679756488028, 1: 0.680463235404213, 2: 0.06307564535663124},\n",
              " (0, 7): {0: 0.2568464983153592, 1: 0.8771719021314304, 2: 0.8755035160457153},\n",
              " (0, 8): {0: 0.1735013435465339,\n",
              "  1: 0.46633667684122415,\n",
              "  2: 0.37327227123540174},\n",
              " (0, 9): {0: 0.07535018608623278,\n",
              "  1: 0.6441092595464167,\n",
              "  2: 0.3892411661578087},\n",
              " (0, 10): {0: 0.5230459726632052,\n",
              "  1: 0.8790226796154295,\n",
              "  2: 0.9383633500343999},\n",
              " (1, 0): {0: 0.1178357847593623,\n",
              "  1: 0.9743103232306763,\n",
              "  2: 0.35837386625898915},\n",
              " (1, 1): {0: 0.4420944641085268, 1: 0.7812577743517624, 2: 0.5623488180396932},\n",
              " (1, 2): {0: 0.4315113615064621,\n",
              "  1: 0.12107476261387229,\n",
              "  2: 0.2100024372271565},\n",
              " (1, 3): {0: 0.8467201645513109,\n",
              "  1: 0.19640687590168204,\n",
              "  2: 0.4867882394344424},\n",
              " (1, 4): {0: 0.38030783357082776,\n",
              "  1: 0.3687336678498545,\n",
              "  2: 0.5444327369086629},\n",
              " (1, 5): {0: 0.21655068761401663,\n",
              "  1: 0.6979811496710809,\n",
              "  2: 0.8463480401614922},\n",
              " (1, 6): {0: 0.5165778709090993,\n",
              "  1: 0.48898394799168243,\n",
              "  2: 0.7615538396675492},\n",
              " (1, 7): {0: 0.6026141397998408,\n",
              "  1: 0.7751061402523121,\n",
              "  2: 0.04469520833155005},\n",
              " (1, 8): {0: 0.36441965114713604,\n",
              "  1: 0.8522756137070983,\n",
              "  2: 0.5930192419387094},\n",
              " (1, 9): {0: 0.5331057516668576, 1: 0.5459759491102439, 2: 0.771480595005792},\n",
              " (1, 10): {0: 0.5037656021669511,\n",
              "  1: 0.35034671969937103,\n",
              "  2: 0.5237296587766899},\n",
              " (2, 0): {0: 0.5510780946222771, 1: 0.4343462393332568, 2: 0.2599744838466945},\n",
              " (2, 1): {0: 0.4135059017595605, 1: 0.6462721535878169, 2: 0.1393527109504178},\n",
              " (2, 2): {0: 0.022759462142950215,\n",
              "  1: 0.6736569681910013,\n",
              "  2: 0.252746473511403},\n",
              " (2, 3): {0: 0.08585541830486132,\n",
              "  1: 0.16269002936267296,\n",
              "  2: 0.016613536671802098},\n",
              " (2, 4): {0: 0.8466906489805733, 1: 0.4640817251249285, 2: 0.3326218199238926},\n",
              " (2, 5): {0: 0.4681435926590588,\n",
              "  1: 0.16041784380076207,\n",
              "  2: 0.14166570069675588},\n",
              " (2, 6): {0: 0.9402201665193963, 1: 0.1270082111514762, 2: 0.7012513053623531},\n",
              " (2, 7): {0: 0.66987437261755, 1: 0.13990612193091223, 2: 0.1777324445034304},\n",
              " (2, 8): {0: 0.08549698955792295,\n",
              "  1: 0.9060884294271287,\n",
              "  2: 0.6618542821309161},\n",
              " (2, 9): {0: 0.20794959749886277,\n",
              "  1: 0.7862439133318557,\n",
              "  2: 0.22782278286063162},\n",
              " (2, 10): {0: 0.3087639471382442,\n",
              "  1: 0.6091151091128215,\n",
              "  2: 0.14924558467205407},\n",
              " (3, 0): {0: 0.868330394263045, 1: 0.3563190986289747, 2: 0.5314436735570497},\n",
              " (3, 1): {0: 0.3075565269823046,\n",
              "  1: 0.11922322032432675,\n",
              "  2: 0.7160379348292222},\n",
              " (3, 2): {0: 0.4075952976782605, 1: 0.5444484119839258, 2: 0.8164624235542669},\n",
              " (3, 3): {0: 0.4199611851952155,\n",
              "  1: 0.09666052037581963,\n",
              "  2: 0.3476865101898273},\n",
              " (3, 4): {0: 0.6755351731070819, 1: 0.940191407494084, 2: 0.4348953824995824},\n",
              " (3, 5): {0: 0.0989716700530544,\n",
              "  1: 0.1784870978888683,\n",
              "  2: 0.11352090628321132},\n",
              " (3, 6): {0: 0.09194325399793246,\n",
              "  1: 0.060141593789999837,\n",
              "  2: 0.5327527362310488},\n",
              " (3, 7): {0: 0.19573512302268092,\n",
              "  1: 0.14160409234522797,\n",
              "  2: 0.9977632261842933},\n",
              " (3, 8): {0: 0.3910930116202114,\n",
              "  1: 0.6546289316707458,\n",
              "  2: 0.08584031729748642},\n",
              " (3, 9): {0: 0.37664451646137764, 1: 0.6443315997342239, 2: 0.832879471946768},\n",
              " (3, 10): {0: 0.37662430622145926,\n",
              "  1: 0.39130558411533345,\n",
              "  2: 0.6773689610173921},\n",
              " (4, 0): {0: 0.5459555862523296, 1: 0.6065919142345809, 2: 0.633379506603575},\n",
              " (4, 1): {0: 0.9494156418999382,\n",
              "  1: 0.08349538263182643,\n",
              "  2: 0.01075622017454081},\n",
              " (4, 2): {0: 0.6013324530781018,\n",
              "  1: 0.5581055999256643,\n",
              "  2: 0.16322081939652044},\n",
              " (4, 3): {0: 0.23329613318979214,\n",
              "  1: 0.3464309192421454,\n",
              "  2: 0.8107993349407245},\n",
              " (4, 4): {0: 0.28949067210267065,\n",
              "  1: 0.8108613260023586,\n",
              "  2: 0.7162933747481827},\n",
              " (4, 5): {0: 0.7938199714472368, 1: 0.6168709523042931, 2: 0.965052030976108},\n",
              " (4, 6): {0: 0.5205855959977973,\n",
              "  1: 0.04984195903839328,\n",
              "  2: 0.7681849395448442},\n",
              " (4, 7): {0: 0.4371909250439544, 1: 0.447166221390777, 2: 0.09840096614223848},\n",
              " (4, 8): {0: 0.20504897858438742,\n",
              "  1: 0.32261077317858866,\n",
              "  2: 0.43871399197549676},\n",
              " (4, 9): {0: 0.3585118709542082,\n",
              "  1: 0.07960175294189209,\n",
              "  2: 0.9595658555463351},\n",
              " (4, 10): {0: 0.617194915744939, 1: 0.9016390083769997, 2: 0.5493771599045767},\n",
              " (5, 0): {0: 0.5682053461524371, 1: 0.8623177355629125, 2: 0.5708649525941959},\n",
              " (5, 1): {0: 0.05806892100740102,\n",
              "  1: 0.6723047291504763,\n",
              "  2: 0.4070441988085486},\n",
              " (5, 2): {0: 0.1875348825696309, 1: 0.9514664175578991, 2: 0.2245111706296734},\n",
              " (5, 3): {0: 0.6856967771654998, 1: 0.7247680640598126, 2: 0.5741066259139994},\n",
              " (5, 4): {0: 0.030378757818834035,\n",
              "  1: 0.04212243273431837,\n",
              "  2: 0.11447805146056766},\n",
              " (5, 5): {0: 0.5355929701248949,\n",
              "  1: 0.31097849427607305,\n",
              "  2: 0.37298839526547056},\n",
              " (5, 6): {0: 0.42981810144707144, 1: 0.528699917753605, 2: 0.8995480290593229},\n",
              " (5, 7): {0: 0.7842762705008567, 1: 0.5982238309600497, 2: 0.8642046061848825},\n",
              " (5, 8): {0: 0.6275346228722468, 1: 0.9207650167101115, 2: 0.6920780716532434},\n",
              " (5, 9): {0: 0.6151037368224209,\n",
              "  1: 0.02045112260910875,\n",
              "  2: 0.8737454919190336},\n",
              " (5, 10): {0: 0.5045013170186032,\n",
              "  1: 0.8103689439879981,\n",
              "  2: 0.5875112585453302},\n",
              " (6, 0): {0: 0.9782502922757886,\n",
              "  1: 0.7656989018272519,\n",
              "  2: 0.47569085374399467},\n",
              " (6, 1): {0: 0.15743415743486178,\n",
              "  1: 0.5692491346448152,\n",
              "  2: 0.0710852435036412},\n",
              " (6, 2): {0: 0.07086016258788308,\n",
              "  1: 0.9483788163365064,\n",
              "  2: 0.6567608995646631},\n",
              " (6, 3): {0: 0.9923058003076487, 1: 0.1672462872089533, 2: 0.986176550159684},\n",
              " (6, 4): {0: 0.1734731823330029,\n",
              "  1: 0.5076661760652753,\n",
              "  2: 0.01249208991308104},\n",
              " (6, 5): {0: 0.33508712401674234,\n",
              "  1: 0.774764245623859,\n",
              "  2: 0.14952824026983214},\n",
              " (6, 6): {0: 0.29576301290199003,\n",
              "  1: 0.8596413794625535,\n",
              "  2: 0.3960932179138027},\n",
              " (6, 7): {0: 0.5422075139972251,\n",
              "  1: 0.24892933168333953,\n",
              "  2: 0.6305083972789439},\n",
              " (6, 8): {0: 0.1349493064315389,\n",
              "  1: 0.12146870509291952,\n",
              "  2: 0.9170595579200502},\n",
              " (6, 9): {0: 0.2645544402878951,\n",
              "  1: 0.28783144769694335,\n",
              "  2: 0.6220395092802536},\n",
              " (6, 10): {0: 0.1798303316374804,\n",
              "  1: 0.9142878026939473,\n",
              "  2: 0.2200993942493984},\n",
              " (7, 0): {0: 0.8286575371227902, 1: 0.5385997248283843, 2: 0.7580828192075275},\n",
              " (7, 1): {0: 0.057175395460292155,\n",
              "  1: 0.1758283775103644,\n",
              "  2: 0.30259599560711115},\n",
              " (7, 2): {0: 0.9556944879040127,\n",
              "  1: 0.09066464486678172,\n",
              "  2: 0.47151058034724025},\n",
              " (7, 3): {0: 0.7363893639119593, 1: 0.9701933579704131, 2: 0.4542132612679183},\n",
              " (7, 4): {0: 0.9510669253183939,\n",
              "  1: 0.1776527998577162,\n",
              "  2: 0.010218178465323025},\n",
              " (7, 5): {0: 0.91769348782402, 1: 0.5501995223264227, 2: 0.58992125414563},\n",
              " (7, 6): {0: 0.11826806047465943,\n",
              "  1: 0.15435783604091036,\n",
              "  2: 0.08278340435639453},\n",
              " (7, 7): {0: 0.2647233252943513, 1: 0.6662838816577765, 2: 0.9622116575054599},\n",
              " (7, 8): {0: 0.5546440339101029, 1: 0.6963471099130897, 2: 0.7908851577873597},\n",
              " (7, 9): {0: 0.13310872677836783,\n",
              "  1: 0.5012459241149172,\n",
              "  2: 0.23135840713582845},\n",
              " (7, 10): {0: 0.24079217474475, 1: 0.15294560061397833, 2: 0.220865269012273},\n",
              " (8, 0): {0: 0.8319788913873177, 1: 0.6680762341364237, 2: 0.9280964594333768},\n",
              " (8, 1): {0: 0.9104865731488282,\n",
              "  1: 0.3597265332694807,\n",
              "  2: 0.19847776231870884},\n",
              " (8, 2): {0: 0.32081098956629117,\n",
              "  1: 0.8851095202948774,\n",
              "  2: 0.19015488524856272},\n",
              " (8, 3): {0: 0.4154239802140036, 1: 0.8714391418301267, 2: 0.4906469010780129},\n",
              " (8, 4): {0: 0.8789624237328607, 1: 0.7127214606047474, 2: 0.643304568246983},\n",
              " (8, 5): {0: 0.20768891271110523, 1: 0.6677788957530137, 2: 0.0728643298828},\n",
              " (8, 6): {0: 0.23791568691079312,\n",
              "  1: 0.47903271637484257,\n",
              "  2: 0.9127801376683878},\n",
              " (8, 7): {0: 0.7659314559812771, 1: 0.7044634701436271, 2: 0.982698367967752},\n",
              " (8, 8): {0: 0.15461392722416556,\n",
              "  1: 0.14970564516132523,\n",
              "  2: 0.13822156045082779},\n",
              " (8, 9): {0: 0.9110950852242067,\n",
              "  1: 0.49524444425083103,\n",
              "  2: 0.2851488741729843},\n",
              " (8, 10): {0: 0.41416051603687565,\n",
              "  1: 0.6323215055815644,\n",
              "  2: 0.4611660674329461},\n",
              " (9, 0): {0: 0.9793911501689295, 1: 0.5251173402297166, 2: 0.6563516500642295},\n",
              " (9, 1): {0: 0.8076855371746944, 1: 0.777688323471832, 2: 0.5636711180731867},\n",
              " (9, 2): {0: 0.8419686179887982,\n",
              "  1: 0.8278155763625903,\n",
              "  2: 0.19330646757784975},\n",
              " (9, 3): {0: 0.44090711042594777,\n",
              "  1: 0.4681494829113657,\n",
              "  2: 0.10180796871676911},\n",
              " (9, 4): {0: 0.6289591707426473,\n",
              "  1: 0.30132038351654933,\n",
              "  2: 0.5629623716180759},\n",
              " (9, 5): {0: 0.9393992828578326, 1: 0.8334530742748079, 2: 0.5658473980713443},\n",
              " (9, 6): {0: 0.744531902920259,\n",
              "  1: 0.055558431107309736,\n",
              "  2: 0.33919703864871975},\n",
              " (9, 7): {0: 0.7972856914575791, 1: 0.6261384563360576, 2: 0.8710871685433569},\n",
              " (9, 8): {0: 0.01990790360423844,\n",
              "  1: 0.6571494417334866,\n",
              "  2: 0.12695344422537003},\n",
              " (9, 9): {0: 0.19109250384575527,\n",
              "  1: 0.6916257163149612,\n",
              "  2: 0.5890580858455955},\n",
              " (9, 10): {0: 0.41881876948940056,\n",
              "  1: 0.6944013168238292,\n",
              "  2: 0.47791929730105775},\n",
              " (10, 0): {0: 0.954656567690735, 1: 0.5730444074403181, 2: 0.8514737292387016},\n",
              " (10, 1): {0: 0.25241984708443477,\n",
              "  1: 0.20037898280670507,\n",
              "  2: 0.7216149267695144},\n",
              " (10, 2): {0: 0.5829833933454142,\n",
              "  1: 0.13945491125812126,\n",
              "  2: 0.7225532809224158},\n",
              " (10, 3): {0: 0.6428233586734912,\n",
              "  1: 0.23441601355784059,\n",
              "  2: 0.24468462085399956},\n",
              " (10, 4): {0: 0.9651484413041561,\n",
              "  1: 0.39212764136960765,\n",
              "  2: 0.9574606755149122},\n",
              " (10, 5): {0: 0, 1: 0, 2: 0},\n",
              " (10, 6): {0: 0, 1: 0, 2: 0},\n",
              " (10, 7): {0: 0, 1: 0, 2: 0},\n",
              " (10, 8): {0: 0, 1: 0, 2: 0},\n",
              " (10, 9): {0: 0, 1: 0, 2: 0},\n",
              " (10, 10): {0: 0, 1: 0, 2: 0}}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkXZIxggMC-1"
      },
      "source": [
        "###Generating Initial Greedy Policy ($\\pi_{greedy_{i}}$) Derived from Initial Action Value"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LvUAr9gwMB61"
      },
      "source": [
        "# Generate a policy based on argmax of initial action value q_i\r\n",
        "# Note that this policy will be converted to an epsilon greedy policy for use\r\n",
        "# in the implementation of TD control to this problem.\r\n",
        "pi_greedy_i = argmax_policy(q_i)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "alyaskOCeozI"
      },
      "source": [
        "##**Controller Development**#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQwOYBaHOG08"
      },
      "source": [
        "###Develop a TD(0) controller using on-policy SARSA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thzsfl4qVeTk"
      },
      "source": [
        "Please see specification of some parameters in the function argument below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZORMuQGVj3K"
      },
      "source": [
        "**Quick Definition of Parameters:**\r\n",
        "<br>\r\n",
        "`n`: Number of steps which will be used in our TD algorithms for estimating the return received by the agent\r\n",
        "<br>\r\n",
        "`alpha` ($\\alpha$): \"Learning rate\" for TD Control algorithm \r\n",
        "<br>\r\n",
        "`eps` ($ε$): Constant used to determine ε-greedy version of a greedy policy\r\n",
        "<br>\r\n",
        "`gamma` ($\\gamma$): Discounting factor used when computing returns as utilized in our estimation of the action value function for the agent in this environment\r\n",
        "<br>\r\n",
        "\r\n",
        "\r\n",
        "Please be aware that these definitions will hold for the remainder of the TD-Control methods implemented."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "JyNdmMcDdQjh",
        "outputId": "a8593fbb-cf0d-4048-af6f-c3d805417f3b"
      },
      "source": [
        "pi_f_SARSA, q_f_SARSA = TD_CONTROL(env, pi_greedy_i, q_i, method = 'SARSA', n = 0,  alpha = 0.1, eps = 0.1, gamma = 0.95, episode_count = 5000)"
      ],
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1000  episodes completed... out of  5000 ****************************************\n",
            " Cumulative Statistics:\n",
            "   Cumulative Average Reward =  -4.327\n",
            "   Cumulative Average Episode Length =  185.062 / 200  ( 0.9253100000000001 )\n",
            "   Cumulative Successful Episode Count =  388 / 1000  ( 0.388 )\n",
            "\n",
            " Batch Statistics (episodes  1  to  1000 ):\n",
            "   Batch Average Reward =  -4.327\n",
            "   Batch Average Episode Length =  185.062 / 200  ( 0.9253100000000001 )\n",
            "   Batch Successful Episode Count =  388 / 1000  ( 0.388 )\n",
            "\n",
            "2000  episodes completed... out of  5000 ****************************************\n",
            " Cumulative Statistics:\n",
            "   Cumulative Average Reward =  -0.346\n",
            "   Cumulative Average Episode Length =  184.488 / 200  ( 0.92244 )\n",
            "   Cumulative Successful Episode Count =  841 / 2000  ( 0.4205 )\n",
            "\n",
            " Batch Statistics (episodes  1001  to  2000 ):\n",
            "   Batch Average Reward =  3.635\n",
            "   Batch Average Episode Length =  183.914 / 200  ( 0.9195699999999999 )\n",
            "   Batch Successful Episode Count =  453 / 1000  ( 0.453 )\n",
            "\n",
            "3000  episodes completed... out of  5000 ****************************************\n",
            " Cumulative Statistics:\n",
            "   Cumulative Average Reward =  -2.244666666666667\n",
            "   Cumulative Average Episode Length =  184.64433333333332 / 200  ( 0.9232216666666666 )\n",
            "   Cumulative Successful Episode Count =  1241 / 3000  ( 0.4136666666666667 )\n",
            "\n",
            " Batch Statistics (episodes  2001  to  3000 ):\n",
            "   Batch Average Reward =  -6.042\n",
            "   Batch Average Episode Length =  184.957 / 200  ( 0.924785 )\n",
            "   Batch Successful Episode Count =  400 / 1000  ( 0.4 )\n",
            "\n",
            "4000  episodes completed... out of  5000 ****************************************\n",
            " Cumulative Statistics:\n",
            "   Cumulative Average Reward =  -2.391\n",
            "   Cumulative Average Episode Length =  185.466 / 200  ( 0.92733 )\n",
            "   Cumulative Successful Episode Count =  1638 / 4000  ( 0.4095 )\n",
            "\n",
            " Batch Statistics (episodes  3001  to  4000 ):\n",
            "   Batch Average Reward =  -2.83\n",
            "   Batch Average Episode Length =  187.931 / 200  ( 0.939655 )\n",
            "   Batch Successful Episode Count =  397 / 1000  ( 0.397 )\n",
            "\n",
            "5000  episodes completed... out of  5000 ****************************************\n",
            " Cumulative Statistics:\n",
            "   Cumulative Average Reward =  -3.8272\n",
            "   Cumulative Average Episode Length =  186.003 / 200  ( 0.9300149999999999 )\n",
            "   Cumulative Successful Episode Count =  1958 / 5000  ( 0.3916 )\n",
            "\n",
            " Batch Statistics (episodes  4001  to  5000 ):\n",
            "   Batch Average Reward =  -9.572\n",
            "   Batch Average Episode Length =  188.151 / 200  ( 0.940755 )\n",
            "   Batch Successful Episode Count =  320 / 1000  ( 0.32 )\n",
            "\n",
            "---------------------------------------------------------------------------------\n",
            "𝐓𝐑𝐀𝐈𝐍𝐈𝐍𝐆 𝐂𝐎𝐌𝐏𝐋𝐄𝐓𝐄\n",
            "ᴛᴅ-ᴄᴏɴᴛʀᴏʟ ᴍᴇᴛʜᴏᴅ:\n",
            "  SARSA\n",
            "\n",
            "ᴘᴀʀᴀᴍᴇᴛᴇʀꜱ:\n",
            " Step-size (𝜶) =  0.1\n",
            " Discount Factor (𝜸) =  0.95\n",
            " 𝜀-greedy constant (𝜺) =  0.1\n",
            "\n",
            "ꜱᴜᴍᴍᴀʀʏ ꜱᴛᴀᴛɪꜱᴛɪᴄꜱ:\n",
            " Cumulative Average Episode Length =  186.003 / 200  ( 0.9300149999999999 )\n",
            " Cumulative Average Successful Episode Length:  164.25689479060264 / 200  ( 0.8212844739530132 )\n",
            " Cumulative Successful Episode Count =  1958 / 5000  ( 0.3916 )\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_MiX96XVJcX"
      },
      "source": [
        "####Performance Result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3brUQtQkVOyk"
      },
      "source": [
        "As we see, over 1000 test episodes, our derived final policy results in an agent which will successfully reach the flag in 993 of those 1000 episodes! "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "Y5m1IFkwhT1P",
        "outputId": "b54adfb5-9a59-495e-b0c5-cd48363f6e17"
      },
      "source": [
        "MountainCar_TEST(trial_count = 20, episode_count = 50, policy_input = pi_f_SARSA, display_trial_readout = False, display_episode_readout = False)"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CUMULATIVE RESULTS:\n",
            "Total Number of Successful Episodes =  993 / 1000\n",
            "Fraction of Successful Episodes =  0.993\n",
            "Average of Average Successful Episode Steps per Trial =  163.78109013605444 / 200\n",
            "Average of Average Successful Episode Steps per Trial (Fractional) =  0.8189054506802722\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-3Pq5wBPd7_w"
      },
      "source": [
        "###Develop a TD(0) controller using on-policy expected SARSA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "DAswAPVE-RMo",
        "outputId": "f9e59c84-8209-4a75-bb1f-8cd3df9171cf"
      },
      "source": [
        "pi_f_EXP_SARSA_ON, q_f_EXP_SARSA_ON = TD_CONTROL(env, pi_greedy_i, q_i, method = 'EXP_SARSA_ON_POLICY', n = 0,  alpha = 0.1, eps = 0.1, gamma = 0.95, episode_count = 5000)"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1000  episodes completed... out of  5000 ****************************************\n",
            " Cumulative Statistics:\n",
            "   Cumulative Average Reward =  -9.855\n",
            "   Cumulative Average Episode Length =  188.377 / 200  ( 0.9418850000000001 )\n",
            "   Cumulative Successful Episode Count =  326 / 1000  ( 0.326 )\n",
            "\n",
            " Batch Statistics (episodes  1  to  1000 ):\n",
            "   Batch Average Reward =  -9.855\n",
            "   Batch Average Episode Length =  188.377 / 200  ( 0.9418850000000001 )\n",
            "   Batch Successful Episode Count =  326 / 1000  ( 0.326 )\n",
            "\n",
            "2000  episodes completed... out of  5000 ****************************************\n",
            " Cumulative Statistics:\n",
            "   Cumulative Average Reward =  -14.403\n",
            "   Cumulative Average Episode Length =  187.003 / 200  ( 0.9350149999999999 )\n",
            "   Cumulative Successful Episode Count =  704 / 2000  ( 0.352 )\n",
            "\n",
            " Batch Statistics (episodes  1001  to  2000 ):\n",
            "   Batch Average Reward =  -18.951\n",
            "   Batch Average Episode Length =  185.629 / 200  ( 0.928145 )\n",
            "   Batch Successful Episode Count =  378 / 1000  ( 0.378 )\n",
            "\n",
            "3000  episodes completed... out of  5000 ****************************************\n",
            " Cumulative Statistics:\n",
            "   Cumulative Average Reward =  -12.638\n",
            "   Cumulative Average Episode Length =  187.06966666666668 / 200  ( 0.9353483333333333 )\n",
            "   Cumulative Successful Episode Count =  1063 / 3000  ( 0.35433333333333333 )\n",
            "\n",
            " Batch Statistics (episodes  2001  to  3000 ):\n",
            "   Batch Average Reward =  -9.108\n",
            "   Batch Average Episode Length =  187.203 / 200  ( 0.936015 )\n",
            "   Batch Successful Episode Count =  359 / 1000  ( 0.359 )\n",
            "\n",
            "4000  episodes completed... out of  5000 ****************************************\n",
            " Cumulative Statistics:\n",
            "   Cumulative Average Reward =  -11.91275\n",
            "   Cumulative Average Episode Length =  187.46325 / 200  ( 0.93731625 )\n",
            "   Cumulative Successful Episode Count =  1384 / 4000  ( 0.346 )\n",
            "\n",
            " Batch Statistics (episodes  3001  to  4000 ):\n",
            "   Batch Average Reward =  -9.737\n",
            "   Batch Average Episode Length =  188.644 / 200  ( 0.9432200000000001 )\n",
            "   Batch Successful Episode Count =  321 / 1000  ( 0.321 )\n",
            "\n",
            "5000  episodes completed... out of  5000 ****************************************\n",
            " Cumulative Statistics:\n",
            "   Cumulative Average Reward =  -10.762\n",
            "   Cumulative Average Episode Length =  186.9628 / 200  ( 0.9348139999999999 )\n",
            "   Cumulative Successful Episode Count =  1808 / 5000  ( 0.3616 )\n",
            "\n",
            " Batch Statistics (episodes  4001  to  5000 ):\n",
            "   Batch Average Reward =  -6.159\n",
            "   Batch Average Episode Length =  184.961 / 200  ( 0.9248050000000001 )\n",
            "   Batch Successful Episode Count =  424 / 1000  ( 0.424 )\n",
            "\n",
            "---------------------------------------------------------------------------------\n",
            "𝐓𝐑𝐀𝐈𝐍𝐈𝐍𝐆 𝐂𝐎𝐌𝐏𝐋𝐄𝐓𝐄\n",
            "ᴛᴅ-ᴄᴏɴᴛʀᴏʟ ᴍᴇᴛʜᴏᴅ:\n",
            "  EXP_SARSA_ON_POLICY\n",
            "\n",
            "ᴘᴀʀᴀᴍᴇᴛᴇʀꜱ:\n",
            " Step-size (𝜶) =  0.1\n",
            " Discount Factor (𝜸) =  0.95\n",
            " 𝜀-greedy constant (𝜺) =  0.1\n",
            "\n",
            "ꜱᴜᴍᴍᴀʀʏ ꜱᴛᴀᴛɪꜱᴛɪᴄꜱ:\n",
            " Cumulative Average Episode Length =  186.9628 / 200  ( 0.9348139999999999 )\n",
            " Cumulative Average Successful Episode Length:  163.94579646017698 / 200  ( 0.8197289823008849 )\n",
            " Cumulative Successful Episode Count =  1808 / 5000  ( 0.3616 )\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "juUMZcC6a-qI"
      },
      "source": [
        "####Performance Result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KdczDAs4a0t9"
      },
      "source": [
        "As we see, over 1000 test episodes, our derived final policy results in an agent which will successfully reach the flag in 996 of those 1000 episodes! "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "37c1mcDulnDQ",
        "outputId": "baf6d6e0-e4dc-4d5e-ee52-d13e6f6ca614"
      },
      "source": [
        "MountainCar_TEST(trial_count = 20, episode_count = 50, policy_input = pi_f_EXP_SARSA_ON, display_trial_readout = False, display_episode_readout = False)"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CUMULATIVE RESULTS:\n",
            "Total Number of Successful Episodes =  996 / 1000\n",
            "Fraction of Successful Episodes =  0.996\n",
            "Average of Average Successful Episode Steps per Trial =  166.13438775510207 / 200\n",
            "Average of Average Successful Episode Steps per Trial (Fractional) =  0.8306719387755104\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNy-m144dyxZ"
      },
      "source": [
        "###Develop a TD(0) controller using off-policy expected SARSA with a greedy control policy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "YD-sBz7TdTEQ",
        "outputId": "7bc194bc-1c5c-4946-bfe8-bce7a3ad1e35"
      },
      "source": [
        "pi_f_EXP_SARSA_OFF, q_f_EXP_SARSA_OFF = TD_CONTROL(env, pi_greedy_i, q_i, method = 'EXP_SARSA_OFF_POLICY', n = 0,  alpha = 0.07, eps = 0.1, gamma = 0.95, episode_count = 5000)"
      ],
      "execution_count": 169,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1000  episodes completed... out of  5000 ****************************************\n",
            " Cumulative Statistics:\n",
            "   Cumulative Average Reward =  16.649\n",
            "   Cumulative Average Episode Length =  183.445 / 200  ( 0.917225 )\n",
            "   Cumulative Successful Episode Count =  444 / 1000  ( 0.444 )\n",
            "\n",
            " Batch Statistics (episodes  1  to  1000 ):\n",
            "   Batch Average Reward =  16.649\n",
            "   Batch Average Episode Length =  183.445 / 200  ( 0.917225 )\n",
            "   Batch Successful Episode Count =  444 / 1000  ( 0.444 )\n",
            "\n",
            "2000  episodes completed... out of  5000 ****************************************\n",
            " Cumulative Statistics:\n",
            "   Cumulative Average Reward =  6.3045\n",
            "   Cumulative Average Episode Length =  184.831 / 200  ( 0.924155 )\n",
            "   Cumulative Successful Episode Count =  842 / 2000  ( 0.421 )\n",
            "\n",
            " Batch Statistics (episodes  1001  to  2000 ):\n",
            "   Batch Average Reward =  -4.04\n",
            "   Batch Average Episode Length =  186.217 / 200  ( 0.931085 )\n",
            "   Batch Successful Episode Count =  398 / 1000  ( 0.398 )\n",
            "\n",
            "3000  episodes completed... out of  5000 ****************************************\n",
            " Cumulative Statistics:\n",
            "   Cumulative Average Reward =  8.003666666666666\n",
            "   Cumulative Average Episode Length =  184.08866666666665 / 200  ( 0.9204433333333333 )\n",
            "   Cumulative Successful Episode Count =  1324 / 3000  ( 0.44133333333333336 )\n",
            "\n",
            " Batch Statistics (episodes  2001  to  3000 ):\n",
            "   Batch Average Reward =  11.402\n",
            "   Batch Average Episode Length =  182.604 / 200  ( 0.91302 )\n",
            "   Batch Successful Episode Count =  482 / 1000  ( 0.482 )\n",
            "\n",
            "4000  episodes completed... out of  5000 ****************************************\n",
            " Cumulative Statistics:\n",
            "   Cumulative Average Reward =  9.84525\n",
            "   Cumulative Average Episode Length =  183.001 / 200  ( 0.9150050000000001 )\n",
            "   Cumulative Successful Episode Count =  1875 / 4000  ( 0.46875 )\n",
            "\n",
            " Batch Statistics (episodes  3001  to  4000 ):\n",
            "   Batch Average Reward =  15.37\n",
            "   Batch Average Episode Length =  179.738 / 200  ( 0.89869 )\n",
            "   Batch Successful Episode Count =  551 / 1000  ( 0.551 )\n",
            "\n",
            "5000  episodes completed... out of  5000 ****************************************\n",
            " Cumulative Statistics:\n",
            "   Cumulative Average Reward =  10.0106\n",
            "   Cumulative Average Episode Length =  183.4196 / 200  ( 0.917098 )\n",
            "   Cumulative Successful Episode Count =  2311 / 5000  ( 0.4622 )\n",
            "\n",
            " Batch Statistics (episodes  4001  to  5000 ):\n",
            "   Batch Average Reward =  10.672\n",
            "   Batch Average Episode Length =  185.094 / 200  ( 0.92547 )\n",
            "   Batch Successful Episode Count =  436 / 1000  ( 0.436 )\n",
            "\n",
            "---------------------------------------------------------------------------------\n",
            "𝐓𝐑𝐀𝐈𝐍𝐈𝐍𝐆 𝐂𝐎𝐌𝐏𝐋𝐄𝐓𝐄\n",
            "ᴛᴅ-ᴄᴏɴᴛʀᴏʟ ᴍᴇᴛʜᴏᴅ:\n",
            "  EXP_SARSA_OFF_POLICY\n",
            "\n",
            "ᴘᴀʀᴀᴍᴇᴛᴇʀꜱ:\n",
            " Step-size (𝜶) =  0.07\n",
            " Discount Factor (𝜸) =  0.95\n",
            " 𝜀-greedy constant (𝜺) =  0.1\n",
            "\n",
            "ꜱᴜᴍᴍᴀʀʏ ꜱᴛᴀᴛɪꜱᴛɪᴄꜱ:\n",
            " Cumulative Average Episode Length =  183.4196 / 200  ( 0.917098 )\n",
            " Cumulative Average Successful Episode Length:  164.12721765469493 / 200  ( 0.8206360882734747 )\n",
            " Cumulative Successful Episode Count =  2311 / 5000  ( 0.4622 )\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNQaeqoQXhmw"
      },
      "source": [
        "####Performance Result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQnmCzmlaiDi"
      },
      "source": [
        "As we see, over 1000 test episodes, our derived final policy results in an agent which will successfully reach the flag in 1000 of those 1000 episodes! "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "M9LCccRFkWRR",
        "outputId": "b21942e0-ad4f-44b1-b017-ccb7023c45a7"
      },
      "source": [
        "MountainCar_TEST(trial_count = 20, episode_count = 50, policy_input = pi_f_EXP_SARSA_OFF, display_trial_readout = False, display_episode_readout = False)"
      ],
      "execution_count": 170,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CUMULATIVE RESULTS:\n",
            "Total Number of Successful Episodes =  1000 / 1000\n",
            "Fraction of Successful Episodes =  1.0\n",
            "Average of Average Successful Episode Steps per Trial =  159.98700000000002 / 200\n",
            "Average of Average Successful Episode Steps per Trial (Fractional) =  0.7999350000000001\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZxxwV0YbIlQ"
      },
      "source": [
        "##**Summary Table**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A18XrjJAnZg0"
      },
      "source": [
        "The table below summarizes the results above. This table format will be used to summarize the following task results as well.\r\n",
        "<br>\r\n",
        "The columns 'n' through to '𝜸' have already been described. 'Average Successful Episode Length' represents the average number of steps which the MC will take to reach the flag in an episode. This is an average over all the successful episodes within the 1000 that the MC is tested on, once each algorithm runs. 'Number of Successful Episodes' represents how many of the episodes, out of the 1000 the MC was tested on for each algorithm, were successful."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "Hh6G4WxKbKdX",
        "outputId": "3f64d22b-6962-4772-e7e4-ff386402c707"
      },
      "source": [
        "# This cell is used to display the results of our trials with the different\r\n",
        "# TD control methods\r\n",
        "\r\n",
        "# Instantiate table class\r\n",
        "control_table = PrettyTable()\r\n",
        "\r\n",
        "# Provide title to table\r\n",
        "control_table.title = 'TD(0) Results'\r\n",
        "\r\n",
        "# Provide field names to table\r\n",
        "control_table.field_names = ['Method', '# Training Episodes', 'n', '𝜶', '𝜺', '𝜸','Average Successful Episode Length (/200)','Number of Successful Test Episodes (/1000)']\r\n",
        "\r\n",
        "# Populate table rows\r\n",
        "control_table.add_row(['TD(0) SARSA', 5000, 0, 0.1, 0.1, 0.95, 163.8, 993])\r\n",
        "control_table.add_row(['TD(0) On-Policy Expected SARSA', 5000, 0, 0.1, 0.1, 0.95, 166.1, 996])\r\n",
        "control_table.add_row(['TD(0) Off-Policy Expected SARSA', 5000, 0, 0.07, 0.1, 0.95, 160, 1000])\r\n",
        "control_table.border\r\n",
        "print(control_table)"
      ],
      "execution_count": 194,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|                                                                             TD(0) Results                                                                             |\n",
            "+---------------------------------+---------------------+---+------+-----+------+------------------------------------------+--------------------------------------------+\n",
            "|              Method             | # Training Episodes | n |  𝜶   |  𝜺  |  𝜸   | Average Successful Episode Length (/200) | Number of Successful Test Episodes (/1000) |\n",
            "+---------------------------------+---------------------+---+------+-----+------+------------------------------------------+--------------------------------------------+\n",
            "|           TD(0) SARSA           |         5000        | 0 | 0.1  | 0.1 | 0.95 |                  163.8                   |                    993                     |\n",
            "|  TD(0) On-Policy Expected SARSA |         5000        | 0 | 0.1  | 0.1 | 0.95 |                  166.1                   |                    996                     |\n",
            "| TD(0) Off-Policy Expected SARSA |         5000        | 0 | 0.07 | 0.1 | 0.95 |                   160                    |                    1000                    |\n",
            "+---------------------------------+---------------------+---+------+-----+------+------------------------------------------+--------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHB81nCNiOyD"
      },
      "source": [
        "##**Discussion**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0KN6F0qYiRHQ"
      },
      "source": [
        "From the table, we see that TD(0) Off-Policy Expected SARSA performed the best, as all its 1000 test episodes were successful, and the length of successful episodes was shorter than that resulting from the other two methods. However, we can't rule out the possibility that with further tuning of parameters, the ranking of methods could be different. Nevertheless, since the results are so close here, it seems all methods are equally viable options."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2wbiBxceGTG"
      },
      "source": [
        "#**Exercise 2: TD(n)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VLAec2JcfF4-"
      },
      "source": [
        "A useful ungraded exercise. Develop code for TD(n) predication, n-step SARSA (on-policy) control and off-policy Tree Backup control.\r\n",
        "\r\n",
        "Test it out."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66cffOQ8csEN"
      },
      "source": [
        "**Text Response:** n-SARSA on policy and off-policy Tree Backup control were implemented. Please observe results below... The algorithms themselves can be found in the 'Helper Functions' section, under the 'TD Control Helper Functions' subsection."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEjJoeJQfK4K"
      },
      "source": [
        "##**n-step SARSA Testing**\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "4fjH68_SeIs9",
        "outputId": "afd5ad44-4fd4-4426-9083-f84130b815b4"
      },
      "source": [
        "pi_f_N_SARSA_2, q_f_N_SARSA_2 = TD_CONTROL(env, pi_greedy_i, q_i, method = 'SARSA', n = 2,  alpha = 0.1, eps = 0.1, gamma = 0.95, episode_count = 5000)"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1000  episodes completed... out of  5000 ****************************************\n",
            " Cumulative Statistics:\n",
            "   Cumulative Average Reward =  -2.873\n",
            "   Cumulative Average Episode Length =  188.573 / 200  ( 0.9428650000000001 )\n",
            "   Cumulative Successful Episode Count =  332 / 1000  ( 0.332 )\n",
            "\n",
            " Batch Statistics (episodes  1  to  1000 ):\n",
            "   Batch Average Reward =  -2.873\n",
            "   Batch Average Episode Length =  188.573 / 200  ( 0.9428650000000001 )\n",
            "   Batch Successful Episode Count =  332 / 1000  ( 0.332 )\n",
            "\n",
            "2000  episodes completed... out of  5000 ****************************************\n",
            " Cumulative Statistics:\n",
            "   Cumulative Average Reward =  -4.6975\n",
            "   Cumulative Average Episode Length =  189.511 / 200  ( 0.9475549999999999 )\n",
            "   Cumulative Successful Episode Count =  628 / 2000  ( 0.314 )\n",
            "\n",
            " Batch Statistics (episodes  1001  to  2000 ):\n",
            "   Batch Average Reward =  -6.522\n",
            "   Batch Average Episode Length =  190.449 / 200  ( 0.952245 )\n",
            "   Batch Successful Episode Count =  296 / 1000  ( 0.296 )\n",
            "\n",
            "3000  episodes completed... out of  5000 ****************************************\n",
            " Cumulative Statistics:\n",
            "   Cumulative Average Reward =  -5.299\n",
            "   Cumulative Average Episode Length =  190.57833333333335 / 200  ( 0.9528916666666667 )\n",
            "   Cumulative Successful Episode Count =  864 / 3000  ( 0.288 )\n",
            "\n",
            " Batch Statistics (episodes  2001  to  3000 ):\n",
            "   Batch Average Reward =  -6.502\n",
            "   Batch Average Episode Length =  192.713 / 200  ( 0.963565 )\n",
            "   Batch Successful Episode Count =  236 / 1000  ( 0.236 )\n",
            "\n",
            "4000  episodes completed... out of  5000 ****************************************\n",
            " Cumulative Statistics:\n",
            "   Cumulative Average Reward =  -4.776\n",
            "   Cumulative Average Episode Length =  189.7645 / 200  ( 0.9488225 )\n",
            "   Cumulative Successful Episode Count =  1209 / 4000  ( 0.30225 )\n",
            "\n",
            " Batch Statistics (episodes  3001  to  4000 ):\n",
            "   Batch Average Reward =  -3.207\n",
            "   Batch Average Episode Length =  187.323 / 200  ( 0.9366150000000001 )\n",
            "   Batch Successful Episode Count =  345 / 1000  ( 0.345 )\n",
            "\n",
            "5000  episodes completed... out of  5000 ****************************************\n",
            " Cumulative Statistics:\n",
            "   Cumulative Average Reward =  -2.2156\n",
            "   Cumulative Average Episode Length =  189.2388 / 200  ( 0.946194 )\n",
            "   Cumulative Successful Episode Count =  1578 / 5000  ( 0.3156 )\n",
            "\n",
            " Batch Statistics (episodes  4001  to  5000 ):\n",
            "   Batch Average Reward =  8.026\n",
            "   Batch Average Episode Length =  187.136 / 200  ( 0.93568 )\n",
            "   Batch Successful Episode Count =  369 / 1000  ( 0.369 )\n",
            "\n",
            "---------------------------------------------------------------------------------\n",
            "𝐓𝐑𝐀𝐈𝐍𝐈𝐍𝐆 𝐂𝐎𝐌𝐏𝐋𝐄𝐓𝐄\n",
            "ᴛᴅ-ᴄᴏɴᴛʀᴏʟ ᴍᴇᴛʜᴏᴅ:\n",
            "  n-SARSA (n = 2)\n",
            "\n",
            "ᴘᴀʀᴀᴍᴇᴛᴇʀꜱ:\n",
            " Step-size (𝜶) =  0.1\n",
            " Discount Factor (𝜸) =  0.95\n",
            " 𝜀-greedy constant (𝜺) =  0.1\n",
            "\n",
            "ꜱᴜᴍᴍᴀʀʏ ꜱᴛᴀᴛɪꜱᴛɪᴄꜱ:\n",
            " Cumulative Average Episode Length =  189.2388 / 200  ( 0.946194 )\n",
            " Cumulative Average Successful Episode Length:  165.9024081115336 / 200  ( 0.8295120405576679 )\n",
            " Cumulative Successful Episode Count =  1578 / 5000  ( 0.3156 )\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmN-Xu7ZdGEZ"
      },
      "source": [
        "###Performance Result"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "pvbjY3YFxCSr",
        "outputId": "e341fefb-2782-485c-fef4-b6129d8ef2c7"
      },
      "source": [
        "MountainCar_TEST(trial_count = 20, episode_count = 50, policy_input = pi_f_N_SARSA_2, display_trial_readout = False, display_episode_readout = False)"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CUMULATIVE RESULTS:\n",
            "Total Number of Successful Episodes =  964 / 1000\n",
            "Fraction of Successful Episodes =  0.964\n",
            "Average of Average Successful Episode Steps per Trial =  152.09392190715323 / 200\n",
            "Average of Average Successful Episode Steps per Trial (Fractional) =  0.7604696095357661\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iVgOmB90fXaK"
      },
      "source": [
        "##**Off Policy Tree Backup Testing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "W2fzA1ElebCC",
        "outputId": "3cf8aada-43fc-4461-c1a4-a832c2f69a92"
      },
      "source": [
        "pi_f_TREE_BACKUP_2, q_f_TREE_BACKUP_2 = TD_CONTROL(env, pi_greedy_i, q_i, method = 'TREE_BACKUP', n = 2,  alpha = 0.03, eps = 0.1, gamma = 0.95, episode_count = 5000)"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1000  episodes completed... out of  5000 ****************************************\n",
            " Cumulative Statistics:\n",
            "   Cumulative Average Reward =  81.996\n",
            "   Cumulative Average Episode Length =  158.457 / 200  ( 0.792285 )\n",
            "   Cumulative Successful Episode Count =  956 / 1000  ( 0.956 )\n",
            "\n",
            " Batch Statistics (episodes  1  to  1000 ):\n",
            "   Batch Average Reward =  81.996\n",
            "   Batch Average Episode Length =  158.457 / 200  ( 0.792285 )\n",
            "   Batch Successful Episode Count =  956 / 1000  ( 0.956 )\n",
            "\n",
            "2000  episodes completed... out of  5000 ****************************************\n",
            " Cumulative Statistics:\n",
            "   Cumulative Average Reward =  85.221\n",
            "   Cumulative Average Episode Length =  157.4315 / 200  ( 0.7871575 )\n",
            "   Cumulative Successful Episode Count =  1956 / 2000  ( 0.978 )\n",
            "\n",
            " Batch Statistics (episodes  1001  to  2000 ):\n",
            "   Batch Average Reward =  88.446\n",
            "   Batch Average Episode Length =  156.406 / 200  ( 0.78203 )\n",
            "   Batch Successful Episode Count =  1000 / 1000  ( 1.0 )\n",
            "\n",
            "3000  episodes completed... out of  5000 ****************************************\n",
            " Cumulative Statistics:\n",
            "   Cumulative Average Reward =  85.59633333333333\n",
            "   Cumulative Average Episode Length =  156.93933333333334 / 200  ( 0.7846966666666667 )\n",
            "   Cumulative Successful Episode Count =  2956 / 3000  ( 0.9853333333333333 )\n",
            "\n",
            " Batch Statistics (episodes  2001  to  3000 ):\n",
            "   Batch Average Reward =  86.347\n",
            "   Batch Average Episode Length =  155.955 / 200  ( 0.7797750000000001 )\n",
            "   Batch Successful Episode Count =  1000 / 1000  ( 1.0 )\n",
            "\n",
            "4000  episodes completed... out of  5000 ****************************************\n",
            " Cumulative Statistics:\n",
            "   Cumulative Average Reward =  85.6335\n",
            "   Cumulative Average Episode Length =  156.661 / 200  ( 0.783305 )\n",
            "   Cumulative Successful Episode Count =  3956 / 4000  ( 0.989 )\n",
            "\n",
            " Batch Statistics (episodes  3001  to  4000 ):\n",
            "   Batch Average Reward =  85.745\n",
            "   Batch Average Episode Length =  155.826 / 200  ( 0.77913 )\n",
            "   Batch Successful Episode Count =  1000 / 1000  ( 1.0 )\n",
            "\n",
            "5000  episodes completed... out of  5000 ****************************************\n",
            " Cumulative Statistics:\n",
            "   Cumulative Average Reward =  86.0916\n",
            "   Cumulative Average Episode Length =  156.47 / 200  ( 0.78235 )\n",
            "   Cumulative Successful Episode Count =  4956 / 5000  ( 0.9912 )\n",
            "\n",
            " Batch Statistics (episodes  4001  to  5000 ):\n",
            "   Batch Average Reward =  87.924\n",
            "   Batch Average Episode Length =  155.706 / 200  ( 0.7785299999999999 )\n",
            "   Batch Successful Episode Count =  1000 / 1000  ( 1.0 )\n",
            "\n",
            "---------------------------------------------------------------------------------\n",
            "𝐓𝐑𝐀𝐈𝐍𝐈𝐍𝐆 𝐂𝐎𝐌𝐏𝐋𝐄𝐓𝐄\n",
            "ᴛᴅ-ᴄᴏɴᴛʀᴏʟ ᴍᴇᴛʜᴏᴅ:\n",
            "  TREE_BACKUP\n",
            "\n",
            "ᴘᴀʀᴀᴍᴇᴛᴇʀꜱ:\n",
            " Step-size (𝜶) =  0.03\n",
            " Discount Factor (𝜸) =  0.95\n",
            " 𝜀-greedy constant (𝜺) =  0.1\n",
            "\n",
            "ꜱᴜᴍᴍᴀʀʏ ꜱᴛᴀᴛɪꜱᴛɪᴄꜱ:\n",
            " Cumulative Average Episode Length =  156.47 / 200  ( 0.78235 )\n",
            " Cumulative Average Successful Episode Length:  156.08353510895884 / 200  ( 0.7804176755447942 )\n",
            " Cumulative Successful Episode Count =  4956 / 5000  ( 0.9912 )\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wc2iXfUKdVLq"
      },
      "source": [
        "###Performance Result"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "mMzPRWYl9UAi",
        "outputId": "39b877fd-c96c-4e17-9e0d-c214ba65fa20"
      },
      "source": [
        "MountainCar_TEST(trial_count = 20, episode_count = 50, policy_input = pi_f_TREE_BACKUP_2, display_trial_readout = False, display_episode_readout = False)"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CUMULATIVE RESULTS:\n",
            "Total Number of Successful Episodes =  1000 / 1000\n",
            "Fraction of Successful Episodes =  1.0\n",
            "Average of Average Successful Episode Steps per Trial =  155.765 / 200\n",
            "Average of Average Successful Episode Steps per Trial (Fractional) =  0.7788249999999999\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pv5AXUPle22N"
      },
      "source": [
        "#**Task 2: TD(2), TD(3), TD(4)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "doWAwfV7fDA2"
      },
      "source": [
        "Code controllers for TD(2), TD(3), and TD(4) using n-SARSA. Assess performance and compare against TD(0) and each other.\r\n",
        "\r\n",
        "You may choose to use your work from Ex 2 for this (if you did Ex 2); otherwise you can code specific solvers for TD(2/3/4)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jfdcdKmDff0A"
      },
      "source": [
        "##**TD(2) n-SARSA**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ykrgAUle_vM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "061aed8c-63f2-4be9-8ad8-e084aa42c1a1"
      },
      "source": [
        "pi_f_N_SARSA_2, q_f_N_SARSA_2 = TD_CONTROL(env, pi_greedy_i, q_i, method = 'SARSA', n = 2,  alpha = 0.1, eps = 0.1, gamma = 0.95, episode_count = 5000)"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1000  episodes completed... out of  5000 ****************************************\n",
            " Cumulative Statistics:\n",
            "   Cumulative Average Reward =  -1.801\n",
            "   Cumulative Average Episode Length =  188.825 / 200  ( 0.944125 )\n",
            "   Cumulative Successful Episode Count =  295 / 1000  ( 0.295 )\n",
            "\n",
            " Batch Statistics (episodes  1  to  1000 ):\n",
            "   Batch Average Reward =  -1.801\n",
            "   Batch Average Episode Length =  188.825 / 200  ( 0.944125 )\n",
            "   Batch Successful Episode Count =  295 / 1000  ( 0.295 )\n",
            "\n",
            "2000  episodes completed... out of  5000 ****************************************\n",
            " Cumulative Statistics:\n",
            "   Cumulative Average Reward =  -0.165\n",
            "   Cumulative Average Episode Length =  186.368 / 200  ( 0.93184 )\n",
            "   Cumulative Successful Episode Count =  718 / 2000  ( 0.359 )\n",
            "\n",
            " Batch Statistics (episodes  1001  to  2000 ):\n",
            "   Batch Average Reward =  1.471\n",
            "   Batch Average Episode Length =  183.911 / 200  ( 0.919555 )\n",
            "   Batch Successful Episode Count =  423 / 1000  ( 0.423 )\n",
            "\n",
            "3000  episodes completed... out of  5000 ****************************************\n",
            " Cumulative Statistics:\n",
            "   Cumulative Average Reward =  -1.412\n",
            "   Cumulative Average Episode Length =  187.81833333333333 / 200  ( 0.9390916666666667 )\n",
            "   Cumulative Successful Episode Count =  1001 / 3000  ( 0.33366666666666667 )\n",
            "\n",
            " Batch Statistics (episodes  2001  to  3000 ):\n",
            "   Batch Average Reward =  -3.906\n",
            "   Batch Average Episode Length =  190.719 / 200  ( 0.953595 )\n",
            "   Batch Successful Episode Count =  283 / 1000  ( 0.283 )\n",
            "\n",
            "4000  episodes completed... out of  5000 ****************************************\n",
            " Cumulative Statistics:\n",
            "   Cumulative Average Reward =  0.0765\n",
            "   Cumulative Average Episode Length =  187.88675 / 200  ( 0.9394337500000001 )\n",
            "   Cumulative Successful Episode Count =  1367 / 4000  ( 0.34175 )\n",
            "\n",
            " Batch Statistics (episodes  3001  to  4000 ):\n",
            "   Batch Average Reward =  4.542\n",
            "   Batch Average Episode Length =  188.092 / 200  ( 0.9404600000000001 )\n",
            "   Batch Successful Episode Count =  366 / 1000  ( 0.366 )\n",
            "\n",
            "5000  episodes completed... out of  5000 ****************************************\n",
            " Cumulative Statistics:\n",
            "   Cumulative Average Reward =  -3.173\n",
            "   Cumulative Average Episode Length =  188.6974 / 200  ( 0.943487 )\n",
            "   Cumulative Successful Episode Count =  1635 / 5000  ( 0.327 )\n",
            "\n",
            " Batch Statistics (episodes  4001  to  5000 ):\n",
            "   Batch Average Reward =  -16.171\n",
            "   Batch Average Episode Length =  191.94 / 200  ( 0.9597 )\n",
            "   Batch Successful Episode Count =  268 / 1000  ( 0.268 )\n",
            "\n",
            "---------------------------------------------------------------------------------\n",
            "𝐓𝐑𝐀𝐈𝐍𝐈𝐍𝐆 𝐂𝐎𝐌𝐏𝐋𝐄𝐓𝐄\n",
            "ᴛᴅ-ᴄᴏɴᴛʀᴏʟ ᴍᴇᴛʜᴏᴅ:\n",
            "  n-SARSA (n = 2)\n",
            "\n",
            "ᴘᴀʀᴀᴍᴇᴛᴇʀꜱ:\n",
            " Step-size (𝜶) =  0.1\n",
            " Discount Factor (𝜸) =  0.95\n",
            " 𝜀-greedy constant (𝜺) =  0.1\n",
            "\n",
            "ꜱᴜᴍᴍᴀʀʏ ꜱᴛᴀᴛɪꜱᴛɪᴄꜱ:\n",
            " Cumulative Average Episode Length =  188.6974 / 200  ( 0.943487 )\n",
            " Cumulative Average Successful Episode Length:  165.4354740061162 / 200  ( 0.827177370030581 )\n",
            " Cumulative Successful Episode Count =  1635 / 5000  ( 0.327 )\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hMzB76LBdgud"
      },
      "source": [
        "###Performance Result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SwC9g6VJdiqh"
      },
      "source": [
        "965 successful test episodes out of 1000."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "q3MGkBT_9kD2",
        "outputId": "c22a04f6-a2e1-474a-dc9e-2eca6f9ed846"
      },
      "source": [
        "MountainCar_TEST(trial_count = 20, episode_count = 50, policy_input = pi_f_N_SARSA_2, display_trial_readout = False, display_episode_readout = False)"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CUMULATIVE RESULTS:\n",
            "Total Number of Successful Episodes =  965 / 1000\n",
            "Fraction of Successful Episodes =  0.965\n",
            "Average of Average Successful Episode Steps per Trial =  184.5825874607475 / 200\n",
            "Average of Average Successful Episode Steps per Trial (Fractional) =  0.9229129373037375\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SjaBzZjrfnYF"
      },
      "source": [
        "##**TD(3) n-SARSA**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_rhVKnDfdCe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "6aa30ef9-3eda-4c74-a1c1-b8111f8555e6"
      },
      "source": [
        "pi_f_N_SARSA_3, q_f_N_SARSA_3 = TD_CONTROL(env, pi_greedy_i, q_i, method = 'SARSA', n = 3,  alpha = 0.1, eps = 0.1, gamma = 0.95, episode_count = 5000)"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1000  episodes completed... out of  5000 ****************************************\n",
            " Cumulative Statistics:\n",
            "   Cumulative Average Reward =  -2.79\n",
            "   Cumulative Average Episode Length =  189.837 / 200  ( 0.949185 )\n",
            "   Cumulative Successful Episode Count =  338 / 1000  ( 0.338 )\n",
            "\n",
            " Batch Statistics (episodes  1  to  1000 ):\n",
            "   Batch Average Reward =  -2.79\n",
            "   Batch Average Episode Length =  189.837 / 200  ( 0.949185 )\n",
            "   Batch Successful Episode Count =  338 / 1000  ( 0.338 )\n",
            "\n",
            "2000  episodes completed... out of  5000 ****************************************\n",
            " Cumulative Statistics:\n",
            "   Cumulative Average Reward =  -6.0455\n",
            "   Cumulative Average Episode Length =  188.546 / 200  ( 0.94273 )\n",
            "   Cumulative Successful Episode Count =  684 / 2000  ( 0.342 )\n",
            "\n",
            " Batch Statistics (episodes  1001  to  2000 ):\n",
            "   Batch Average Reward =  -9.301\n",
            "   Batch Average Episode Length =  187.255 / 200  ( 0.936275 )\n",
            "   Batch Successful Episode Count =  346 / 1000  ( 0.346 )\n",
            "\n",
            "3000  episodes completed... out of  5000 ****************************************\n",
            " Cumulative Statistics:\n",
            "   Cumulative Average Reward =  -4.509333333333333\n",
            "   Cumulative Average Episode Length =  188.549 / 200  ( 0.942745 )\n",
            "   Cumulative Successful Episode Count =  1013 / 3000  ( 0.33766666666666667 )\n",
            "\n",
            " Batch Statistics (episodes  2001  to  3000 ):\n",
            "   Batch Average Reward =  -1.437\n",
            "   Batch Average Episode Length =  188.555 / 200  ( 0.942775 )\n",
            "   Batch Successful Episode Count =  329 / 1000  ( 0.329 )\n",
            "\n",
            "4000  episodes completed... out of  5000 ****************************************\n",
            " Cumulative Statistics:\n",
            "   Cumulative Average Reward =  -4.74125\n",
            "   Cumulative Average Episode Length =  188.686 / 200  ( 0.94343 )\n",
            "   Cumulative Successful Episode Count =  1332 / 4000  ( 0.333 )\n",
            "\n",
            " Batch Statistics (episodes  3001  to  4000 ):\n",
            "   Batch Average Reward =  -5.437\n",
            "   Batch Average Episode Length =  189.097 / 200  ( 0.945485 )\n",
            "   Batch Successful Episode Count =  319 / 1000  ( 0.319 )\n",
            "\n",
            "5000  episodes completed... out of  5000 ****************************************\n",
            " Cumulative Statistics:\n",
            "   Cumulative Average Reward =  -6.3894\n",
            "   Cumulative Average Episode Length =  189.027 / 200  ( 0.945135 )\n",
            "   Cumulative Successful Episode Count =  1613 / 5000  ( 0.3226 )\n",
            "\n",
            " Batch Statistics (episodes  4001  to  5000 ):\n",
            "   Batch Average Reward =  -12.982\n",
            "   Batch Average Episode Length =  190.391 / 200  ( 0.951955 )\n",
            "   Batch Successful Episode Count =  281 / 1000  ( 0.281 )\n",
            "\n",
            "---------------------------------------------------------------------------------\n",
            "𝐓𝐑𝐀𝐈𝐍𝐈𝐍𝐆 𝐂𝐎𝐌𝐏𝐋𝐄𝐓𝐄\n",
            "ᴛᴅ-ᴄᴏɴᴛʀᴏʟ ᴍᴇᴛʜᴏᴅ:\n",
            "  n-SARSA (n = 3)\n",
            "\n",
            "ᴘᴀʀᴀᴍᴇᴛᴇʀꜱ:\n",
            " Step-size (𝜶) =  0.1\n",
            " Discount Factor (𝜸) =  0.95\n",
            " 𝜀-greedy constant (𝜺) =  0.1\n",
            "\n",
            "ꜱᴜᴍᴍᴀʀʏ ꜱᴛᴀᴛɪꜱᴛɪᴄꜱ:\n",
            " Cumulative Average Episode Length =  189.027 / 200  ( 0.945135 )\n",
            " Cumulative Average Successful Episode Length:  165.98574085554867 / 200  ( 0.8299287042777433 )\n",
            " Cumulative Successful Episode Count =  1613 / 5000  ( 0.3226 )\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V59f2OWAdpXh"
      },
      "source": [
        "###Performance Result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fvHPgxJHdo1X"
      },
      "source": [
        "979 successful test episodes out of 1000."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "daewmYLt9pff",
        "outputId": "6d94ae9d-b8b1-49df-e64f-d685ad364f6e"
      },
      "source": [
        "MountainCar_TEST(trial_count = 20, episode_count = 50, policy_input = pi_f_N_SARSA_3, display_trial_readout = False, display_episode_readout = False)"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CUMULATIVE RESULTS:\n",
            "Total Number of Successful Episodes =  979 / 1000\n",
            "Fraction of Successful Episodes =  0.979\n",
            "Average of Average Successful Episode Steps per Trial =  150.4693976335215 / 200\n",
            "Average of Average Successful Episode Steps per Trial (Fractional) =  0.7523469881676076\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGVKeHzlfpSQ"
      },
      "source": [
        "##**TD(4) n-SARSA**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-T_msscfeaJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "9ec4b744-97c6-4444-989b-a952f0108671"
      },
      "source": [
        "pi_f_N_SARSA_4, q_f_N_SARSA_4 = TD_CONTROL(env, pi_greedy_i, q_i, method = 'SARSA', n = 4,  alpha = 0.06, eps = 0.08, gamma = 0.95, episode_count = 5000)"
      ],
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1000  episodes completed... out of  5000 ****************************************\n",
            " Cumulative Statistics:\n",
            "   Cumulative Average Reward =  31.944\n",
            "   Cumulative Average Episode Length =  179.875 / 200  ( 0.899375 )\n",
            "   Cumulative Successful Episode Count =  506 / 1000  ( 0.506 )\n",
            "\n",
            " Batch Statistics (episodes  1  to  1000 ):\n",
            "   Batch Average Reward =  31.944\n",
            "   Batch Average Episode Length =  179.875 / 200  ( 0.899375 )\n",
            "   Batch Successful Episode Count =  506 / 1000  ( 0.506 )\n",
            "\n",
            "2000  episodes completed... out of  5000 ****************************************\n",
            " Cumulative Statistics:\n",
            "   Cumulative Average Reward =  33.415\n",
            "   Cumulative Average Episode Length =  180.2825 / 200  ( 0.9014125 )\n",
            "   Cumulative Successful Episode Count =  1027 / 2000  ( 0.5135 )\n",
            "\n",
            " Batch Statistics (episodes  1001  to  2000 ):\n",
            "   Batch Average Reward =  34.886\n",
            "   Batch Average Episode Length =  180.69 / 200  ( 0.90345 )\n",
            "   Batch Successful Episode Count =  521 / 1000  ( 0.521 )\n",
            "\n",
            "3000  episodes completed... out of  5000 ****************************************\n",
            " Cumulative Statistics:\n",
            "   Cumulative Average Reward =  36.76433333333333\n",
            "   Cumulative Average Episode Length =  178.17266666666666 / 200  ( 0.8908633333333333 )\n",
            "   Cumulative Successful Episode Count =  1644 / 3000  ( 0.548 )\n",
            "\n",
            " Batch Statistics (episodes  2001  to  3000 ):\n",
            "   Batch Average Reward =  43.463\n",
            "   Batch Average Episode Length =  173.953 / 200  ( 0.869765 )\n",
            "   Batch Successful Episode Count =  617 / 1000  ( 0.617 )\n",
            "\n",
            "4000  episodes completed... out of  5000 ****************************************\n",
            " Cumulative Statistics:\n",
            "   Cumulative Average Reward =  37.74525\n",
            "   Cumulative Average Episode Length =  177.6225 / 200  ( 0.8881125 )\n",
            "   Cumulative Successful Episode Count =  2244 / 4000  ( 0.561 )\n",
            "\n",
            " Batch Statistics (episodes  3001  to  4000 ):\n",
            "   Batch Average Reward =  40.688\n",
            "   Batch Average Episode Length =  175.972 / 200  ( 0.8798600000000001 )\n",
            "   Batch Successful Episode Count =  600 / 1000  ( 0.6 )\n",
            "\n",
            "5000  episodes completed... out of  5000 ****************************************\n",
            " Cumulative Statistics:\n",
            "   Cumulative Average Reward =  37.4968\n",
            "   Cumulative Average Episode Length =  177.0346 / 200  ( 0.8851730000000001 )\n",
            "   Cumulative Successful Episode Count =  2857 / 5000  ( 0.5714 )\n",
            "\n",
            " Batch Statistics (episodes  4001  to  5000 ):\n",
            "   Batch Average Reward =  36.503\n",
            "   Batch Average Episode Length =  174.683 / 200  ( 0.8734149999999999 )\n",
            "   Batch Successful Episode Count =  613 / 1000  ( 0.613 )\n",
            "\n",
            "---------------------------------------------------------------------------------\n",
            "𝐓𝐑𝐀𝐈𝐍𝐈𝐍𝐆 𝐂𝐎𝐌𝐏𝐋𝐄𝐓𝐄\n",
            "ᴛᴅ-ᴄᴏɴᴛʀᴏʟ ᴍᴇᴛʜᴏᴅ:\n",
            "  n-SARSA (n = 4)\n",
            "\n",
            "ᴘᴀʀᴀᴍᴇᴛᴇʀꜱ:\n",
            " Step-size (𝜶) =  0.06\n",
            " Discount Factor (𝜸) =  0.95\n",
            " 𝜀-greedy constant (𝜺) =  0.08\n",
            "\n",
            "ꜱᴜᴍᴍᴀʀʏ ꜱᴛᴀᴛɪꜱᴛɪᴄꜱ:\n",
            " Cumulative Average Episode Length =  177.0346 / 200  ( 0.8851730000000001 )\n",
            " Cumulative Average Successful Episode Length:  159.80854042702134 / 200  ( 0.7990427021351068 )\n",
            " Cumulative Successful Episode Count =  2857 / 5000  ( 0.5714 )\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "klbSpR4LdwZW"
      },
      "source": [
        "###Performance Result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qkuyyxoed33O"
      },
      "source": [
        "998 successful test episodes out of 1000."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "sj9kx8ER9rKb",
        "outputId": "34c71682-2815-4db1-9872-67aea701a614"
      },
      "source": [
        "MountainCar_TEST(trial_count = 20, episode_count = 50, policy_input = pi_f_N_SARSA_4, display_trial_readout = False, display_episode_readout = False)"
      ],
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CUMULATIVE RESULTS:\n",
            "Total Number of Successful Episodes =  998 / 1000\n",
            "Fraction of Successful Episodes =  0.998\n",
            "Average of Average Successful Episode Steps per Trial =  144.13179591836735 / 200\n",
            "Average of Average Successful Episode Steps per Trial (Fractional) =  0.7206589795918368\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_fkAgswd9uy"
      },
      "source": [
        "##**Summary Table**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "UDpI1yqld_uR",
        "outputId": "1b4bd8b4-a902-4f1e-c237-7793e91f1896"
      },
      "source": [
        "# This cell is used to display the results of our trials with the different\r\n",
        "# TD control methods\r\n",
        "\r\n",
        "# Instantiate table class\r\n",
        "control_table = PrettyTable()\r\n",
        "\r\n",
        "# Provide title to table\r\n",
        "control_table.title = 'TD(n) SARSA Results'\r\n",
        "\r\n",
        "# Provide field names to table\r\n",
        "control_table.field_names = ['Method', '# Training Episodes', 'n', '𝜶', '𝜺', '𝜸', 'Average Successful Episode Length (/200)','Number of Successful Test Episodes (/1000)']\r\n",
        "\r\n",
        "# Populate table rows\r\n",
        "control_table.add_row(['TD(2) SARSA', 5000, 2, 0.1, 0.1, 0.95, 184.6, 965])\r\n",
        "control_table.add_row(['TD(3) SARSA', 5000, 3, 0.1, 0.1, 0.95, 150.5, 979])\r\n",
        "control_table.add_row(['TD(4) SARSA', 5000, 4, 0.06, 0.08, 0.95, 144.1, 998])\r\n",
        "control_table.border\r\n",
        "print(control_table)"
      ],
      "execution_count": 191,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|                                                                TD(n) SARSA Results                                                                 |\n",
            "+-------------+---------------------+---+------+------+------+------------------------------------------+--------------------------------------------+\n",
            "|    Method   | # Training Episodes | n |  𝜶   |  𝜺   |  𝜸   | Average Successful Episode Length (/200) | Number of Successful Test Episodes (/1000) |\n",
            "+-------------+---------------------+---+------+------+------+------------------------------------------+--------------------------------------------+\n",
            "| TD(2) SARSA |         5000        | 2 | 0.1  | 0.1  | 0.95 |                  184.6                   |                    965                     |\n",
            "| TD(3) SARSA |         5000        | 3 | 0.1  | 0.1  | 0.95 |                  150.5                   |                    979                     |\n",
            "| TD(4) SARSA |         5000        | 4 | 0.06 | 0.08 | 0.95 |                  144.1                   |                    998                     |\n",
            "+-------------+---------------------+---+------+------+------+------------------------------------------+--------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BzRFXNLDjyys"
      },
      "source": [
        "##**Discussion**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AdPg9f3si_aO"
      },
      "source": [
        "From the table, we see an increase in performance as n is increased - an intuitive result. However we notice that TD(2) SARSA performed worse than TD(0) SARSA in the previous task. This may be a matter of tuning the parameters differently to generate different results. However the results in TD(4) are a great improvement, as we observe the lowest `'Average Successful Episode Length'` yet, and a vary high count of successful episodes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6bfbObEfrfy"
      },
      "source": [
        "#**Bonus Task 3: Tree Backup**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zoOpw0Nnf-k6"
      },
      "source": [
        "This is optional; if you do this, your grade can exceed 100% and will offset other grades.\r\n",
        "\r\n",
        "Assess the performance of Tree Backup for TD(2), TD(3), and TD(4) versus the respective n-SARSA controller."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPam7mSaeWVo"
      },
      "source": [
        "##**TD(2) Tree Backup**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U34HJZELgD1a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "c8f85ba1-c522-4864-8e50-257f81e11468"
      },
      "source": [
        "pi_f_TREE_BACKUP_2, q_f_TREE_BACKUP_2 = TD_CONTROL(env, pi_greedy_i, q_i, method = 'TREE_BACKUP', n = 2,  alpha = 0.03, eps = 0.1, gamma = 0.95, episode_count = 5000)"
      ],
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1000  episodes completed... out of  5000 ****************************************\n",
            " Cumulative Statistics:\n",
            "   Cumulative Average Reward =  81.757\n",
            "   Cumulative Average Episode Length =  160.344 / 200  ( 0.80172 )\n",
            "   Cumulative Successful Episode Count =  954 / 1000  ( 0.954 )\n",
            "\n",
            " Batch Statistics (episodes  1  to  1000 ):\n",
            "   Batch Average Reward =  81.757\n",
            "   Batch Average Episode Length =  160.344 / 200  ( 0.80172 )\n",
            "   Batch Successful Episode Count =  954 / 1000  ( 0.954 )\n",
            "\n",
            "2000  episodes completed... out of  5000 ****************************************\n",
            " Cumulative Statistics:\n",
            "   Cumulative Average Reward =  85.023\n",
            "   Cumulative Average Episode Length =  159.398 / 200  ( 0.79699 )\n",
            "   Cumulative Successful Episode Count =  1954 / 2000  ( 0.977 )\n",
            "\n",
            " Batch Statistics (episodes  1001  to  2000 ):\n",
            "   Batch Average Reward =  88.289\n",
            "   Batch Average Episode Length =  158.452 / 200  ( 0.79226 )\n",
            "   Batch Successful Episode Count =  1000 / 1000  ( 1.0 )\n",
            "\n",
            "3000  episodes completed... out of  5000 ****************************************\n",
            " Cumulative Statistics:\n",
            "   Cumulative Average Reward =  85.49466666666666\n",
            "   Cumulative Average Episode Length =  159.09866666666667 / 200  ( 0.7954933333333334 )\n",
            "   Cumulative Successful Episode Count =  2954 / 3000  ( 0.9846666666666667 )\n",
            "\n",
            " Batch Statistics (episodes  2001  to  3000 ):\n",
            "   Batch Average Reward =  86.438\n",
            "   Batch Average Episode Length =  158.5 / 200  ( 0.7925 )\n",
            "   Batch Successful Episode Count =  1000 / 1000  ( 1.0 )\n",
            "\n",
            "4000  episodes completed... out of  5000 ****************************************\n",
            " Cumulative Statistics:\n",
            "   Cumulative Average Reward =  85.558\n",
            "   Cumulative Average Episode Length =  158.80325 / 200  ( 0.79401625 )\n",
            "   Cumulative Successful Episode Count =  3954 / 4000  ( 0.9885 )\n",
            "\n",
            " Batch Statistics (episodes  3001  to  4000 ):\n",
            "   Batch Average Reward =  85.748\n",
            "   Batch Average Episode Length =  157.917 / 200  ( 0.789585 )\n",
            "   Batch Successful Episode Count =  1000 / 1000  ( 1.0 )\n",
            "\n",
            "5000  episodes completed... out of  5000 ****************************************\n",
            " Cumulative Statistics:\n",
            "   Cumulative Average Reward =  85.2166\n",
            "   Cumulative Average Episode Length =  158.6248 / 200  ( 0.7931239999999999 )\n",
            "   Cumulative Successful Episode Count =  4954 / 5000  ( 0.9908 )\n",
            "\n",
            " Batch Statistics (episodes  4001  to  5000 ):\n",
            "   Batch Average Reward =  83.851\n",
            "   Batch Average Episode Length =  157.911 / 200  ( 0.789555 )\n",
            "   Batch Successful Episode Count =  1000 / 1000  ( 1.0 )\n",
            "\n",
            "---------------------------------------------------------------------------------\n",
            "𝐓𝐑𝐀𝐈𝐍𝐈𝐍𝐆 𝐂𝐎𝐌𝐏𝐋𝐄𝐓𝐄\n",
            "ᴛᴅ-ᴄᴏɴᴛʀᴏʟ ᴍᴇᴛʜᴏᴅ:\n",
            "  TREE_BACKUP\n",
            "\n",
            "ᴘᴀʀᴀᴍᴇᴛᴇʀꜱ:\n",
            " Step-size (𝜶) =  0.03\n",
            " Discount Factor (𝜸) =  0.95\n",
            " 𝜀-greedy constant (𝜺) =  0.1\n",
            "\n",
            "ꜱᴜᴍᴍᴀʀʏ ꜱᴛᴀᴛɪꜱᴛɪᴄꜱ:\n",
            " Cumulative Average Episode Length =  158.6248 / 200  ( 0.7931239999999999 )\n",
            " Cumulative Average Successful Episode Length:  158.24061364553896 / 200  ( 0.7912030682276948 )\n",
            " Cumulative Successful Episode Count =  4954 / 5000  ( 0.9908 )\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2WkaFM2qebhF"
      },
      "source": [
        "###Performance Result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MMAUnEC6edaZ"
      },
      "source": [
        "1000 successful episodes out of 1000 test episodes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "yqvK7UHMFANS",
        "outputId": "e97eb9a0-f0aa-4235-e18d-1d792865f3b7"
      },
      "source": [
        "MountainCar_TEST(trial_count = 20, episode_count = 50, policy_input = pi_f_TREE_BACKUP_2, display_trial_readout = False, display_episode_readout = False)"
      ],
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CUMULATIVE RESULTS:\n",
            "Total Number of Successful Episodes =  1000 / 1000\n",
            "Fraction of Successful Episodes =  1.0\n",
            "Average of Average Successful Episode Steps per Trial =  157.89499999999998 / 200\n",
            "Average of Average Successful Episode Steps per Trial (Fractional) =  0.7894749999999999\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IgQUk2GcesEd"
      },
      "source": [
        "##**TD(3) Tree Backup**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SsAEmKTMgFJN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "8351abf0-d5e4-4fdb-e76e-cd74f7762d25"
      },
      "source": [
        "pi_f_TREE_BACKUP_3, q_f_TREE_BACKUP_3 = TD_CONTROL(env, pi_greedy_i, q_i, method = 'TREE_BACKUP', n = 3,  alpha = 0.05, eps = 0.1, gamma = 0.95, episode_count = 5000)"
      ],
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1000  episodes completed... out of  5000 ****************************************\n",
            " Cumulative Statistics:\n",
            "   Cumulative Average Reward =  74.036\n",
            "   Cumulative Average Episode Length =  155.55 / 200  ( 0.77775 )\n",
            "   Cumulative Successful Episode Count =  990 / 1000  ( 0.99 )\n",
            "\n",
            " Batch Statistics (episodes  1  to  1000 ):\n",
            "   Batch Average Reward =  74.036\n",
            "   Batch Average Episode Length =  155.55 / 200  ( 0.77775 )\n",
            "   Batch Successful Episode Count =  990 / 1000  ( 0.99 )\n",
            "\n",
            "2000  episodes completed... out of  5000 ****************************************\n",
            " Cumulative Statistics:\n",
            "   Cumulative Average Reward =  74.166\n",
            "   Cumulative Average Episode Length =  155.2685 / 200  ( 0.7763424999999999 )\n",
            "   Cumulative Successful Episode Count =  1990 / 2000  ( 0.995 )\n",
            "\n",
            " Batch Statistics (episodes  1001  to  2000 ):\n",
            "   Batch Average Reward =  74.296\n",
            "   Batch Average Episode Length =  154.987 / 200  ( 0.7749349999999999 )\n",
            "   Batch Successful Episode Count =  1000 / 1000  ( 1.0 )\n",
            "\n",
            "3000  episodes completed... out of  5000 ****************************************\n",
            " Cumulative Statistics:\n",
            "   Cumulative Average Reward =  75.19066666666667\n",
            "   Cumulative Average Episode Length =  155.275 / 200  ( 0.776375 )\n",
            "   Cumulative Successful Episode Count =  2990 / 3000  ( 0.9966666666666667 )\n",
            "\n",
            " Batch Statistics (episodes  2001  to  3000 ):\n",
            "   Batch Average Reward =  77.24\n",
            "   Batch Average Episode Length =  155.288 / 200  ( 0.77644 )\n",
            "   Batch Successful Episode Count =  1000 / 1000  ( 1.0 )\n",
            "\n",
            "4000  episodes completed... out of  5000 ****************************************\n",
            " Cumulative Statistics:\n",
            "   Cumulative Average Reward =  74.55125\n",
            "   Cumulative Average Episode Length =  155.07 / 200  ( 0.77535 )\n",
            "   Cumulative Successful Episode Count =  3990 / 4000  ( 0.9975 )\n",
            "\n",
            " Batch Statistics (episodes  3001  to  4000 ):\n",
            "   Batch Average Reward =  72.633\n",
            "   Batch Average Episode Length =  154.455 / 200  ( 0.772275 )\n",
            "   Batch Successful Episode Count =  1000 / 1000  ( 1.0 )\n",
            "\n",
            "5000  episodes completed... out of  5000 ****************************************\n",
            " Cumulative Statistics:\n",
            "   Cumulative Average Reward =  74.2732\n",
            "   Cumulative Average Episode Length =  155.0148 / 200  ( 0.775074 )\n",
            "   Cumulative Successful Episode Count =  4990 / 5000  ( 0.998 )\n",
            "\n",
            " Batch Statistics (episodes  4001  to  5000 ):\n",
            "   Batch Average Reward =  73.161\n",
            "   Batch Average Episode Length =  154.794 / 200  ( 0.77397 )\n",
            "   Batch Successful Episode Count =  1000 / 1000  ( 1.0 )\n",
            "\n",
            "---------------------------------------------------------------------------------\n",
            "𝐓𝐑𝐀𝐈𝐍𝐈𝐍𝐆 𝐂𝐎𝐌𝐏𝐋𝐄𝐓𝐄\n",
            "ᴛᴅ-ᴄᴏɴᴛʀᴏʟ ᴍᴇᴛʜᴏᴅ:\n",
            "  TREE_BACKUP\n",
            "\n",
            "ᴘᴀʀᴀᴍᴇᴛᴇʀꜱ:\n",
            " Step-size (𝜶) =  0.05\n",
            " Discount Factor (𝜸) =  0.95\n",
            " 𝜀-greedy constant (𝜺) =  0.1\n",
            "\n",
            "ꜱᴜᴍᴍᴀʀʏ ꜱᴛᴀᴛɪꜱᴛɪᴄꜱ:\n",
            " Cumulative Average Episode Length =  155.0148 / 200  ( 0.775074 )\n",
            " Cumulative Average Successful Episode Length:  154.9246492985972 / 200  ( 0.774623246492986 )\n",
            " Cumulative Successful Episode Count =  4990 / 5000  ( 0.998 )\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lduvUP61eiDX"
      },
      "source": [
        "###Performance Result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U1G-9T0Hejvs"
      },
      "source": [
        "1000 successful episodes out of 1000 test episodes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "jdJDix2hFD4T",
        "outputId": "e8ad6cda-854d-42fe-e9f7-2b2820a60a52"
      },
      "source": [
        "MountainCar_TEST(trial_count = 20, episode_count = 50, policy_input = pi_f_TREE_BACKUP_3, display_trial_readout = False, display_episode_readout = False)"
      ],
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CUMULATIVE RESULTS:\n",
            "Total Number of Successful Episodes =  1000 / 1000\n",
            "Fraction of Successful Episodes =  1.0\n",
            "Average of Average Successful Episode Steps per Trial =  154.985 / 200\n",
            "Average of Average Successful Episode Steps per Trial (Fractional) =  0.7749250000000001\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X69ElW0we3du"
      },
      "source": [
        "##**TD(4) Tree Backup**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oFXe8u77gGJI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "66446483-701b-4ec3-a4ca-f978a3986087"
      },
      "source": [
        "pi_f_TREE_BACKUP_4, q_f_TREE_BACKUP_4 = TD_CONTROL(env, pi_greedy_i, q_i, method = 'TREE_BACKUP', n = 4,  alpha = 0.07, eps = 0.1, gamma = 0.95, episode_count = 5000)"
      ],
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1000  episodes completed... out of  5000 ****************************************\n",
            " Cumulative Statistics:\n",
            "   Cumulative Average Reward =  73.611\n",
            "   Cumulative Average Episode Length =  155.956 / 200  ( 0.7797799999999999 )\n",
            "   Cumulative Successful Episode Count =  894 / 1000  ( 0.894 )\n",
            "\n",
            " Batch Statistics (episodes  1  to  1000 ):\n",
            "   Batch Average Reward =  73.611\n",
            "   Batch Average Episode Length =  155.956 / 200  ( 0.7797799999999999 )\n",
            "   Batch Successful Episode Count =  894 / 1000  ( 0.894 )\n",
            "\n",
            "2000  episodes completed... out of  5000 ****************************************\n",
            " Cumulative Statistics:\n",
            "   Cumulative Average Reward =  74.732\n",
            "   Cumulative Average Episode Length =  153.034 / 200  ( 0.7651699999999999 )\n",
            "   Cumulative Successful Episode Count =  1894 / 2000  ( 0.947 )\n",
            "\n",
            " Batch Statistics (episodes  1001  to  2000 ):\n",
            "   Batch Average Reward =  75.853\n",
            "   Batch Average Episode Length =  150.112 / 200  ( 0.75056 )\n",
            "   Batch Successful Episode Count =  1000 / 1000  ( 1.0 )\n",
            "\n",
            "3000  episodes completed... out of  5000 ****************************************\n",
            " Cumulative Statistics:\n",
            "   Cumulative Average Reward =  76.452\n",
            "   Cumulative Average Episode Length =  152.07833333333335 / 200  ( 0.7603916666666667 )\n",
            "   Cumulative Successful Episode Count =  2894 / 3000  ( 0.9646666666666667 )\n",
            "\n",
            " Batch Statistics (episodes  2001  to  3000 ):\n",
            "   Batch Average Reward =  79.892\n",
            "   Batch Average Episode Length =  150.167 / 200  ( 0.750835 )\n",
            "   Batch Successful Episode Count =  1000 / 1000  ( 1.0 )\n",
            "\n",
            "4000  episodes completed... out of  5000 ****************************************\n",
            " Cumulative Statistics:\n",
            "   Cumulative Average Reward =  76.55525\n",
            "   Cumulative Average Episode Length =  151.553 / 200  ( 0.757765 )\n",
            "   Cumulative Successful Episode Count =  3894 / 4000  ( 0.9735 )\n",
            "\n",
            " Batch Statistics (episodes  3001  to  4000 ):\n",
            "   Batch Average Reward =  76.865\n",
            "   Batch Average Episode Length =  149.977 / 200  ( 0.749885 )\n",
            "   Batch Successful Episode Count =  1000 / 1000  ( 1.0 )\n",
            "\n",
            "5000  episodes completed... out of  5000 ****************************************\n",
            " Cumulative Statistics:\n",
            "   Cumulative Average Reward =  76.895\n",
            "   Cumulative Average Episode Length =  151.2648 / 200  ( 0.756324 )\n",
            "   Cumulative Successful Episode Count =  4894 / 5000  ( 0.9788 )\n",
            "\n",
            " Batch Statistics (episodes  4001  to  5000 ):\n",
            "   Batch Average Reward =  78.254\n",
            "   Batch Average Episode Length =  150.112 / 200  ( 0.75056 )\n",
            "   Batch Successful Episode Count =  1000 / 1000  ( 1.0 )\n",
            "\n",
            "---------------------------------------------------------------------------------\n",
            "𝐓𝐑𝐀𝐈𝐍𝐈𝐍𝐆 𝐂𝐎𝐌𝐏𝐋𝐄𝐓𝐄\n",
            "ᴛᴅ-ᴄᴏɴᴛʀᴏʟ ᴍᴇᴛʜᴏᴅ:\n",
            "  TREE_BACKUP\n",
            "\n",
            "ᴘᴀʀᴀᴍᴇᴛᴇʀꜱ:\n",
            " Step-size (𝜶) =  0.07\n",
            " Discount Factor (𝜸) =  0.95\n",
            " 𝜀-greedy constant (𝜺) =  0.1\n",
            "\n",
            "ꜱᴜᴍᴍᴀʀʏ ꜱᴛᴀᴛɪꜱᴛɪᴄꜱ:\n",
            " Cumulative Average Episode Length =  151.2648 / 200  ( 0.756324 )\n",
            " Cumulative Average Successful Episode Length:  150.20923579893747 / 200  ( 0.7510461789946874 )\n",
            " Cumulative Successful Episode Count =  4894 / 5000  ( 0.9788 )\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VbMnhoDAe5Sy"
      },
      "source": [
        "###Performance Result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GxWH9vWblRAQ"
      },
      "source": [
        "1000 successful episodes out of 1000 test episodes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "U71PnbRnFFeC",
        "outputId": "fc0a9011-129a-4b35-ba99-d59120595d65"
      },
      "source": [
        "MountainCar_TEST(trial_count = 20, episode_count = 50, policy_input = pi_f_TREE_BACKUP_4, display_trial_readout = False, display_episode_readout = False)"
      ],
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CUMULATIVE RESULTS:\n",
            "Total Number of Successful Episodes =  1000 / 1000\n",
            "Fraction of Successful Episodes =  1.0\n",
            "Average of Average Successful Episode Steps per Trial =  150.10399999999998 / 200\n",
            "Average of Average Successful Episode Steps per Trial (Fractional) =  0.75052\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pHUnWGk8gjmn"
      },
      "source": [
        "##**Summary Table**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "AaEkuCXBglt1",
        "outputId": "27a4444a-ce09-47a9-a87e-7dd948b1ad0a"
      },
      "source": [
        "# This cell is used to display the results of our trials with the different\r\n",
        "# TD control methods\r\n",
        "\r\n",
        "# Instantiate table class\r\n",
        "control_table = PrettyTable()\r\n",
        "\r\n",
        "# Provide title to table\r\n",
        "control_table.title = 'TD(n) Tree-Backup Results'\r\n",
        "\r\n",
        "# Provide field names to table\r\n",
        "control_table.field_names = ['Method', '# Training Episodes', 'n', 'alpha', '𝜺', '𝜸', 'Average Successful Episode Length (/200)','Number of Successful Test Episodes (/1000)']\r\n",
        "\r\n",
        "# Populate table rows\r\n",
        "control_table.add_row(['TD(2) Tree-Backup', 5000, 2, 0.03, 0.1, 0.95, 157.9, 1000])\r\n",
        "control_table.add_row(['TD(3) Tree-Backup', 5000, 3, 0.05, 0.1, 0.95, 155, 1000])\r\n",
        "control_table.add_row(['TD(4) Tree-Backup', 5000, 4, 0.07, 0.1, 0.95, 150.1, 1000])\r\n",
        "control_table.border\r\n",
        "print(control_table)"
      ],
      "execution_count": 192,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|                                                                TD(n) Tree-Backup Results                                                                |\n",
            "+-------------------+---------------------+---+------+-----+------+------------------------------------------+--------------------------------------------+\n",
            "|       Method      | # Training Episodes | n |  𝜶   |  𝜺  |  𝜸   | Average Successful Episode Length (/200) | Number of Successful Test Episodes (/1000) |\n",
            "+-------------------+---------------------+---+------+-----+------+------------------------------------------+--------------------------------------------+\n",
            "| TD(2) Tree-Backup |         5000        | 2 | 0.03 | 0.1 | 0.95 |                  157.9                   |                    1000                    |\n",
            "| TD(3) Tree-Backup |         5000        | 3 | 0.05 | 0.1 | 0.95 |                   155                    |                    1000                    |\n",
            "| TD(4) Tree-Backup |         5000        | 4 | 0.07 | 0.1 | 0.95 |                  150.1                   |                    1000                    |\n",
            "+-------------------+---------------------+---+------+-----+------+------------------------------------------+--------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V8T_n4ZWkwU4"
      },
      "source": [
        "##**Discussion**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N96yorp6kyOe"
      },
      "source": [
        "As we clearly see here, Tree Backup performs astoundingly for all 3 cases of 'n' analyzed. 'Average Successful Episode Length' descreases as 'n' increases, which is both an expected and encouraging result. For all 'n', every test episode is successful."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izhP_9-QpAQW"
      },
      "source": [
        "#**Concluding Remarks**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hi854lfnpEFO"
      },
      "source": [
        "We have seen the performance of a variety of TD-Controllers. From the results, it appears that Tree Backup is the most reliable option (for all n values we've attempted). However, it should be noted that TD(0) Off-Policy Expected SARSA produced the best result out of all control algorithms (lowest number of steps needed to reach the flag, with all episodes being successful). Through these tasks, I've found that hyperparameter selection, particularly the choice of $\\alpha$ is critical to the speed towards which the MC converges to an optimal policy."
      ]
    }
  ]
}